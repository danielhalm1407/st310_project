

---
title: "Untitled"
output:
  html_document: default
  pdf_document: default
date: "2025-03-10"
---

```{r setup, include=FALSE}
library(dplyr)
library(caret)
library(glmnet)
library(ggplot2)
library(tidymodels)
library(ggdag)
library(e1071)
library(recipes)
library(rstudioapi) # to set the correct working directory
```

We begin by loading our dataset, heart.csv, saving it as a dataframe called heart. We then set a seed for random processes, to ensure a level of reproducibility - this ensures that random operations are consistent in nature. 

The sex variable is then recoded for better interpretability, with "0" recoded to "Female" and "1" to "Male". This can be expressed as:

\[
\text{sex_factor} = 
\begin{cases} 
\text{Female} & \text{if } \text{sex} = 0, \\
\text{Male} & \text{if } \text{sex} = 1
\end{cases}
\]

The dataset is then split into training and testing sets using a 70/30 ratio, which partitions the data such that 70% of the observations are allocated to the training set and the remaining 30% are allocated to the testing set.

#### Set the path for the data and predictions
```{r, echo=FALSE}
# Change data_path to where I saved the data
data_path <- paste0(getwd(),"/../data/")
predictions_path <- paste0(getwd(),"/../predictions/")

```

```{r}
# Dataset is loaded and saved as heart
heart <- read.csv(paste0(data_path, "Heart.csv"))

# Sex recoded for greater interpretability 
heart <- heart %>%
  mutate(sex_factor = recode(sex,
                             "0" = "Female",
                             "1" = "Male"))

# Create a 70/30 split between training and testing data set
heart_split <- initial_split(heart, prop = 0.7)
train_data <- heart_split %>% training()
test_data <- heart_split %>% testing()

```

Next, we want to identify the most influential quantitative and qualitative predictors.

For our quantitative predictors such as age, thalach and oldpeak, we seeked to calculate the correlation of the predictors with the target. The correlations are then rearranged from most to least influential - the most influential numeric variable was identified as oldpeak.

For our qualitative predictors such as sex_factor, cp and thal, we decided to look at a logistic regression model to assess the significance of our categorical predictors. The logit model is: 

\[
\log\left(\frac{p}{1-p}\right) = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \dots + \beta_n X_n
\]

such that \(p\) reflects the probability of the target being 1, \(\beta_0\) is the intercept value and \(\beta_1, \beta_2, \dots, \beta_n\) are the coefficients for their respective predictors \(X_1, X_2, \dots, X_n\)

The p-values derived from the model summary are then sorted from most significant to least significant - the most influential categorical variable was identified as cp. These p-values are indicative of statistical significance of each predictor.

```{r}
# Most influential quantitative variable determined via looking at correlations
heart_correlations <- train_data %>%
  rename(y = target) %>%  # Rename target to y
  select(age, thalach, ca, oldpeak, trestbps, chol, y) %>%  # Select columns including y
  cor() %>%
  as.data.frame() %>%
  select(y) %>%
  arrange(desc(abs(y)))

print(heart_correlations) # most highly correlated is oldpeak

# Most influential qualitative variable determined via looking at a logistic model
categorical_data <- train_data %>% 
  rename(y = target) %>%  # Rename target to y
  select(c(sex_factor, cp, fbs, restecg, exang, slope, thal), y)  # Select columns including y

glm_categoricals <- glm(y ~ ., data = categorical_data, family = binomial)

# Plot the coefficients
coef_plot <- summary(glm_categoricals)$coefficients

estimates <- coef_plot[, 1]

standarderror <- coef_plot[, 2] 

barplot <- barplot(coef_plot[, 1], main = "Coefficients of Logistic Model", las = 2, ylim = range(estimates + 2*standarderror, estimates - 2*standarderror), ylab = "Estimate")

arrows(barplot, estimates - 2*standarderror, barplot, estimates + 2*standarderror, angle = 90, code = 3, length = 0.05) # error bars for + or - 2SE

abline(h = 0, lty = 2)

# Most significant qualitative variable
sort(summary(glm_categoricals)$coefficients[, 4]) # most significant is cp
```

Thus, our baseline logistic model is built utilising oldpeak, the numerical variable with the highest correlation to the target, and cp, the most significant cateogrical predictor. Thus, our interpretable logit model is:

\[
\log\left(\frac{p}{1-p}\right) = \beta_0 + \beta_1 \cdot \text{cp} + \beta_2 \cdot \text{oldpeak}
\]

The summary function highlights that both predictors are highly significant when compared to our target.

```{r}
## Task 1: Interpretable model - taking the most highly correlated numerical predictor and the most significant categorical predictor
glm1 <- glm(target ~ cp + oldpeak, data = train_data, family = binomial)
summary(glm1)
```

We then work to identify the best pair of predictors which would minismise the missclassification rate using cross-validation.

We first recode the target variable to ensure smooth running of our logistic model. All predictors are listed and a logistic model is fitted using all these predictors to establish a baseline model. This sets the scene for comparison of predictor pairs to find the optimal pair.

```{r}
## Task 4: For loop to see which two predictors give the best cross-validation misclassification rate
train_data$target <- as.factor(train_data$target) # recoding target to be categorical

predictors <- c("age", "thalach", "ca", "oldpeak", "trestbps", "chol",
                "cp", "slope", "thal", "sex_factor", "exang", "restecg", "fbs") 

glm_full <- glm(target ~ ., data = train_data %>% select(all_of(predictors), target), family = "binomial") # logistic model fit on all predictors
```

Lasso regression is utilised to ensure that the most important predictors are selected. 

Lasso regression is a regression method that introduces an L1 penalty on the coefficients of the predictors. The L1 penalty ensures sparsity in the model by shrinking  coefficients of less important predictors to zero, performing feature selection.

The loss function for Lasso regression in the context of logit is:

\[
\ell_{\text{Lasso}}(\beta) = -\sum_{i=1}^{n} \left[ y_i \log(p(x_i)) + (1 - y_i) \log(1 - p(x_i)) \right] + \lambda \sum_{j=1}^{p} |\beta_j|
\]

such that \[ -\sum_{i=1}^{n} \left[ y_i \log(p(x_i)) + (1 - y_i) \log(1 - p(x_i)) \right] \] is the logistic loss for binary classification with \(p(x_i) = \frac{1}{1 + e^{-x_i \beta}}\) being the     logistic function, \(\lambda\) is the parameter controlling the strength of the L1 penalty and \(\sum_{j=1}^{p} |\beta_j|\) is the L1 penalty term which ensures sparsity in the coefficients

The optimal value of \(\lambda\) is determined using \(k\)-fold cross-validation, namely a 10-fold approach. The selected predictors are those with non-zero coefficients in the Lasso model:

\[
\text{selected_predictors} = \{X_i \mid \beta_i \neq 0\}
\]

The non-zero coefficients are subsequently extracted to help identify the selected predictors and validated to ensure existence within the data set. These remaining coefficients correspond to the predictors that Lasso deems most important. The selected predictors are validated to ensure they exist in the dataset, and all possible pairs of these predictors are generated for further analysis.

```{r}


# Convert target to numeric (0/1) instead of factor
train_data$target_numeric <- as.numeric(train_data$target) - 1

# Standardize predictors and build model matrix
x <- scale(model.matrix(target_numeric ~ age + thalach + ca + oldpeak + trestbps + chol + cp + slope + thal + sex_factor + exang + restecg + fbs, data = train_data))[, -1]
y <- train_data$target_numeric

# Split into train/test
set.seed(42)  # for reproducibility
train <- sample(1:nrow(x), nrow(x) / 2)
test <- setdiff(1:nrow(x), train)
y.test <- y[test]

# Create lambda grid
grid <- 10^seq(10, -4, length = 200)

# Fit lasso model
lasso_model <- glmnet(x[train, ], y[train], alpha = 1, lambda = grid, family = "binomial")
plot(lasso_model)

# Cross-validated lasso
lasso_model_cv <- cv.glmnet(x[train, ], y[train], alpha = 1, lambda = grid, family = "binomial", nfolds = 10)
plot(lasso_model_cv)

# Best lambda
best_lambda <- lasso_model_cv$lambda.min
best_lambda

# Predict (use type = "response" to get probabilities)
lasso_predicted <- predict(lasso_model, s = best_lambda, newx = x[test, ], type = "response")

# Convert probabilities to class 
lasso_class <- ifelse(lasso_predicted > 0.5, 1, 0)

# Compute classification error
mean(lasso_class != y.test)

# Optionally, refit on full data
out <- glmnet(x, y, alpha = 1, lambda = grid, family = "binomial")

```

- Add a legend to the second figure which shows how coefs collapse
- narrow the range of log lambda for the first figure (so thgat we don't have sug a big flat part to the right). display log lambda only for -7 to 0
- fix the numbers on the top of the red (first) figure - add a legend for that


```{r, eval = F}
#OLD CODE - CHECK DIFFERENCES WITH ABOVE
# Use Lasso to select predictors
x <- scale(model.matrix(factor(target) ~ age + thalach + ca + oldpeak + trestbps + chol + cp + slope + thal + sex_factor + exang + restecg + fbs, data = train_data))[, -1]
y <- train_data$target
train <- sample (1: nrow(x), nrow(x) / 2)
test <- (-train)
y.test <- y[test]
grid <- 10^ seq (10, -4, length = 200)
lasso_model <- glmnet(x[train , ], y[train], alpha = 1, lambda = grid, family = "binomial")
lasso_model
plot(lasso_model)


lasso_model_cv <- cv.glmnet(x[train, ], y[train], alpha = 1, nfolds = 100)
plot(lasso_model_cv)
```
```{r}

best_lambda <- lasso_model_cv$lambda.min
best_lambda

lasso_predicted <- predict(lasso_model , s = best_lambda , newx = x[test , ])
mean((lasso_predicted - y.test)^2)

out <- glmnet(x, y, alpha = 1, lambda = grid)
```
```{r}
lasso.coef <- predict(out, type = "coefficients", s = best_lambda)

# Identify selected predictors
selected_predictors <- rownames(lasso.coef)[lasso.coef[, 1] != 0][-1] # checking if predictors exist in dataset
selected_predictors <- intersect(selected_predictors, colnames(train_data)) 
selected_predictors # predictors ranked before the for loop

# Generate all predictor pairs
predictor_pairs <- combn(selected_predictors, 2, simplify = FALSE) # all predictor combinations

# Initialize results data frame
results <- data.frame(Predictor1 = character(),
                      Predictor2 = character(),
                      MisclassificationRate = numeric()) # predictor combos stored

# Set up cross-validation control
cv_control <- trainControl(method = "cv", number = 10) # cross-validation set up
```

We deploy a for loop to go through all potential pairs of predictors. This helps us identify which pair has the lowest missclassification rate. We train the model for each pair using 10-fold cross-validation. 

We calculate the accuracy rate using the calculate_error function which calculates accuracy as follows:

\[
\text{Accuracy} = \frac{1}{n} \sum_{i=1}^{n} \mathbb{I}(\hat{y}_i = y_i)
\]

such that \(\hat{y}_i\) is the predicted class for the \(i\)-th observation, \(y_i\) is the true class for the \(i\)-th observation and \(\mathbb{I}(\cdot)\) is the indicator function

The results for each pair is stored within the results data frame.

The pairs of predictors are sorted from highest to lowest accuracy - the top-performing pair is ca and oldpeak

```{r}
# Initialize an empty data frame to store results
results <- data.frame(Predictor1 = character(),
                      Predictor2 = character(),
                      Accuracy = numeric(),
                      stringsAsFactors = FALSE)

for (pair in predictor_pairs) { # for loop to find 2 most important predictors
  pair <- unlist(pair)
  
  selected_data <- train_data %>% select(all_of(pair), target) # subset to ensure only looking at predictor pair and target
  
  # Validation set approach of logistic model
  model_fit <- glm(target ~ ., data = selected_data, family = binomial)
  probabilities <- predict(model_fit, type = "response")
  predictions <- ifelse(probabilities > 0.5, 1, 0)
  
  # Compute the accuracy using the calculate_error function
  calculate_error <- function(predictions, true_values, classify) {
    if (classify) {
      return(mean(predictions == true_values)) # Calculate accuracy directly
    } else {
      return(mean((predictions - true_values)^2))
    }
  }
  
  accuracy <- calculate_error(predictions, selected_data$target, classify = TRUE)
  
  # Append the results for this pair to the results data frame
  results <- rbind(results, 
                   data.frame(Predictor1 = pair[1], 
                              Predictor2 = pair[2], 
                              Accuracy = accuracy,
                              stringsAsFactors = FALSE))
}

# Sort the results by accuracy (descending order, since higher accuracy is better)
best_pairs <- results[order(-results$Accuracy), ]

# Display the top few rows of the sorted data frame
head(best_pairs)
```





Causality in heart disease can be understood via a Directed Acyclic Graph (DAG) which  maps the relationships between various contributing predictors. In our DAG, we focus on six  variables that we believe most significantly influence the likelihood of developing heart disease.

Ranked in order of relative impact, the most influential predictor is oldpeak, which measures ST depression on an electrocardiogram (Patil, 2025). This reflects how the heart responds under stress compared to rest, with a downward slope indicating potential dysfunction (Patil, 2025). Next is ca, which captures the number of major blood vessels showing signs of blockage (Patil, 2025). The more vessels affected, the higher the cardiovascular risk (Patil, 2025). Trestbps, recording resting blood pressure, follows closely, as high blood pressure at rest is linked to heart disease, stroke and chronic cardiovascular issues (Patil, 2025).

The fourth predictor is restecg, which looks at resting electrocardiogram results (Patil, 2025). Abnormalities here may potentially signal increased strain on the heart or underlying valvular  defects (Patil, 2025). The last two predictors, chol (cholesterol levels) and fbs (fasting blood sugar) are generally considered more manageable through lifestyle changes (Patil, 2025). Diet, exercise and medication can significantly reduce their impact, whereas the other variables often indicate deeper physiological issues that may require surgical intervention (Patil, 2025).

These predictors not only contribute individually but are also interlinked. For instance, higher fasting blood sugar can lower HDL (good cholesterol) and raise LDL (bad cholesterol) worsening lipid profiles (Patil, 2025). Greater cholesterol levels lead to plaque build-up in the arteries, narrowing them and forcing the heart to work harder (Patil, 2025). This can result in increased blood pressure, known as hypertension, which is often detected as ST depression on an ECG (Patil, 2025). Such findings might prompt further investigation through procedures like a fluoroscopy to assess the extent of cardiovascular damage (Patil, 2025).

```{r}
# DAG
heart_dag <- ggdag::dagify(chol ~ fbs,
                           trestbps ~ chol,
                           oldpeak ~ trestbps,
                           restecg ~ oldpeak,
                           ca ~ restecg,
                           target ~ fbs,
                           target ~ chol,
                           target ~ trestbps,
                           target ~ oldpeak, 
                           target ~ restecg,
                           target ~ ca)

ggdag::ggdag(heart_dag) + theme_void()
```

We encountered issues with our dataset sourced from Kaggle, namely related to counterintuitive findings that raised concerns about the accuracy of certain attributes. For instance, the dataset suggests that experiencing chest pain during exercise correlates with a lower likelihood of heart disease, while showing no chest pain increases the risk of having heart disease, an interpretation that contradicts science (Hamada, 2025). This anomaly suggests that the target variable in our Kaggle dataset may have been reversed by the uploader - where a value of 0 actually represents a diseased heart and 1 indicates a healthy heart, rather than the original convention where 0 = healthy and 1 = diseased.

Thus, while the coefficients discussed in our report remain valid in terms of magnitude and relative importance, their directional interpretation should be considered in reverse.

Take the following example, assuming the values of tha target are reversed correctly: 

If we were to include only trestbps (resting blood pressure) as a predictor in a logistic regression model against the target, the estimated effect on the log-odds of heart disease might appear stronger than it actually is. This overestimation occurs because trestbps is positively correlated with cholesterol, a variable omitted in this simplified model. When blood pressure rises, it coincides with an underlying increase in cholesterol levels. As a result, the model mistakenly attributes some of cholesterol's influence on target to trestbps, leading to a higher coefficient. This is an example of Omitted Variable Bias (OVB), where the absence of a relevant predictor, in our case cholestrol, causes the effect of an included variable to be biased in magnitude. Given the interlinked nature of these predictors, as discussed previously, it is important to interpret coefficients within a full model to avoid drawing misleading conclusions from individual variables.

Bibliography

Patil, M. (2025, April 1). Variables impacting presence of heart disease. (A. Patil, Interviewer)
Hamada, M. (2025, 16 April). Heart Disease Dataset - Discussion. Retrieved from Kaggle: https://www.kaggle.com/datasets/johnsmith88/heart-disease-dataset/discussion/171669


[dO THE DAGS HERE AND EXPLAIN THE CAVEAT]
https://www.kaggle.com/datasets/johnsmith88/heart-disease-dataset/discussion/171669
