---
title: "Untitled"
output:
  html_document: default
  pdf_document: default
date: "2025-03-10"
---

```{r setup, include=FALSE}
library(dplyr)
library(glmnet)
library(ggplot2)
library(tidymodels)
library(ggdag) # utilised to help create DAG graph
library(e1071)
library(rstudioapi) # to set the correct working directory
```


```{r, echo=FALSE}
# Change data_path to where I saved the data
data_path <- paste0(getwd(),"/../data/")
predictions_path <- paste0(getwd(),"/../predictions/")
```


## Baseline model

We begin by loading our dataset, heart.csv, saving it as a dataframe called heart. We then set a seed for random processes, to ensure a level of reproducibility - this ensures that random operations are consistent in nature. 

The sex variable is then recoded for better interpretability, with "0" recoded to "Female" and "1" to "Male". This can be expressed as:

\[
\text{sex_factor} = 
\begin{cases} 
\text{Female} & \text{if } \text{sex} = 0, \\
\text{Male} & \text{if } \text{sex} = 1
\end{cases}
\]

The dataset is then split into training and testing sets using a 70/30 ratio, which partitions the data such that 70% of the observations are allocated to the training set and the remaining 30% are allocated to the testing set.

```{r}
# Dataset is loaded and saved as heart
heart <- read.csv(paste0(data_path, "Heart.csv"))

# Sex recoded for greater interpretability 
heart <- heart %>%
  mutate(sex_factor = recode(sex,
                             "0" = "Female",
                             "1" = "Male"))

# Create a 70/30 split between training and testing data set
heart_split <- initial_split(heart, prop = 0.7)
train_data <- heart_split %>% training()
test_data <- heart_split %>% testing()

```

Next, we want to identify the most influential quantitative and qualitative predictors.

For our quantitative predictors such as age, thalach and oldpeak, we seeked to calculate the correlation of the predictors with the target. The correlations are then rearranged from most to least influential - the most influential numeric variable was identified as oldpeak.

For our qualitative predictors such as sex_factor, cp and thal, we decided to look at a logistic regression model to assess the significance of our categorical predictors. The logit model is: 

\[
\log\left(\frac{p}{1-p}\right) = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \dots + \beta_n X_n
\]

such that \(p\) reflects the probability of the target being 1, \(\beta_0\) is the intercept value and \(\beta_1, \beta_2, \dots, \beta_n\) are the coefficients for their respective predictors \(X_1, X_2, \dots, X_n\)

We then visualise the coefficients of the categorical variables, derived from our logistic model, via a plot.

The p-values derived from the model summary are then sorted from most significant to least significant - the most influential categorical variable was identified as cp. These p-values are indicative of statistical significance of each predictor.

```{r}
# Most influential quantitative variable determined via looking at correlations
heart_correlations <- train_data %>%
  rename(y = target) %>%  # Rename target to y
  select(age, thalach, ca, oldpeak, trestbps, chol, y) %>%  # Select columns including y
  cor() %>%
  as.data.frame() %>%
  select(y) %>%
  arrange(desc(abs(y)))

print(heart_correlations) # most highly correlated is oldpeak

# Most influential qualitative variable determined via looking at a logistic model
categorical_data <- train_data %>% 
  rename(y = target) %>%  # Rename target to y
  select(c(sex_factor, cp, fbs, restecg, exang, slope, thal), y)  # Select columns including y

glm_categoricals <- glm(y ~ ., data = categorical_data, family = binomial)

# Plot the coefficients
coef_plot <- summary(glm_categoricals)$coefficients

estimates <- coef_plot[, 1]

standarderror <- coef_plot[, 2] 

barplot <- barplot(coef_plot[, 1], main = "Coefficients of Logistic Model", las = 2, ylim = range(estimates + 2*standarderror, estimates - 2*standarderror), ylab = "Estimate")

arrows(barplot, estimates - 2*standarderror, barplot, estimates + 2*standarderror, angle = 90, code = 3, length = 0.05) # error bars for + or - 2SE

abline(h = 0, lty = 2)

# Most significant qualitative variable
sort(summary(glm_categoricals)$coefficients[, 4]) # most significant is cp
```

Thus, our baseline logistic model is built utilising oldpeak, the numerical variable with the highest correlation to the target, and cp, the most significant cateogrical predictor. Thus, our interpretable logit model is:

\[
\log\left(\frac{p}{1-p}\right) = \beta_0 + \beta_1 \cdot \text{cp} + \beta_2 \cdot \text{oldpeak}
\]

The summary function highlights that both predictors are highly significant when compared to our target.

```{r}
## Task 1: Interpretable model - taking the most highly correlated numerical predictor and the most significant categorical predictor
glm1 <- glm(target ~ cp + oldpeak, data = train_data, family = binomial)
summary(glm1)
```

We then work to identify the best pair of predictors which would minismise the missclassification rate using cross-validation.

We first recode the target variable to ensure smooth running of our logistic model. All predictors are listed and a logistic model is fitted using all these predictors to establish a baseline model. This sets the scene for comparison of predictor pairs to find the optimal pair.

```{r}
## Task 4: For loop to see which two predictors give the best cross-validation misclassification rate
train_data$target <- as.factor(train_data$target) # recoding target to be categorical

predictors <- c("age", "thalach", "ca", "oldpeak", "trestbps", "chol",
                "cp", "slope", "thal", "sex_factor", "exang", "restecg", "fbs") 

glm_full <- glm(target ~ ., data = train_data %>% select(all_of(predictors), target), family = "binomial") # logistic model fit on all predictors
```

Lasso regression is utilised to ensure that the most important predictors are selected. 

Lasso regression is a regression method that introduces an L1 penalty on the coefficients of the predictors. The L1 penalty ensures sparsity in the model by shrinking  coefficients of less important predictors to zero, performing feature selection.

The loss function for Lasso regression in the context of logit is:

\[
\ell_{\text{Lasso}}(\beta) = -\sum_{i=1}^{n} \left[ y_i \log(p(x_i)) + (1 - y_i) \log(1 - p(x_i)) \right] + \lambda \sum_{j=1}^{p} |\beta_j|
\]

such that \[ -\sum_{i=1}^{n} \left[ y_i \log(p(x_i)) + (1 - y_i) \log(1 - p(x_i)) \right] \] is the logistic loss for binary classification with \(p(x_i) = \frac{1}{1 + e^{-x_i \beta}}\) being the     logistic function, \(\lambda\) is the parameter controlling the strength of the L1 penalty and \(\sum_{j=1}^{p} |\beta_j|\) is the L1 penalty term which ensures sparsity in the coefficients

The optimal value of \(\lambda\) is determined using \(k\)-fold cross-validation, namely a 10-fold approach. The selected predictors are those with non-zero coefficients in the Lasso model:

\[
\text{selected_predictors} = \{X_i \mid \beta_i \neq 0\}
\]

The non-zero coefficients are subsequently extracted to help identify the selected predictors and validated to ensure existence within the data set. These remaining coefficients correspond to the predictors that Lasso deems most important. The selected predictors are validated to ensure they exist in the dataset, and all possible pairs of these predictors are generated for further analysis.

```{r}
train_data$target_numeric <- as.numeric(train_data$target) - 1

x <- scale(model.matrix(target_numeric ~ age + thalach + ca + oldpeak + trestbps + chol + cp + slope + thal + sex_factor + exang + restecg + fbs, data = train_data))[, -1]
y <- train_data$target_numeric

train <- sample(1:nrow(x), nrow(x) / 2) # train test split 
test <- setdiff(1:nrow(x), train)
y.test <- y[test]

grid <- 10^seq(0, -10, length = 200) # decreasing lambda grid

# duplicate L1 norms removed 
unique_idx <- !duplicated(round(l1_norm, 5))  # rounding adjusted to remove error
beta_unique <- beta[, unique_idx]
n_nonzero <- colSums(beta_unique != 0)
l1_norm_unique <- l1_norm[unique_idx] 

matplot(l1_norm_unique, t(beta_unique), type = "l" , lwd = 2,
        xlab = "L1 Norm", ylab = "Coefficient Value",
        col = rainbow(nrow(beta_unique)), xlim = range(l1_norm_unique))  + axis(3, at = l1_norm_unique, labels = n_nonzero, line = 0, tick = FALSE) + mtext("Number of Non-Zero Coefficients", side = 3, line = 2.5, cex = 0.9) # manual L1 norm plot


lasso_model_cv <- cv.glmnet(x[train, ], y[train], alpha = 1, lambda = grid, family = "binomial", nfolds = 10) # lasso cross validated 
plot(lasso_model_cv, xlim = c(-7, 0))
mtext("Number of Non-Zero Coefficients", side = 3, line = 2.5, cex = 0.9, font = 1)

best_lambda <- lasso_model_cv$lambda.min 
best_lambda

out <- glmnet(x, y, alpha = 1, lambda = grid, family = "binomial") #glmnet refitted on all data

lasso.coef <- predict(out, type = "coefficients", s = best_lambda) # coefficients extracted from out

selected_predictors <- rownames(lasso.coef)[lasso.coef[, 1] != 0][-1] # checking if predictors exist in dataset
selected_predictors <- intersect(selected_predictors, colnames(train_data)) 
selected_predictors # predictors ranked before the for loop

predictor_pairs <- combn(selected_predictors, 2, simplify = FALSE) # all predictor combinations are generated 

results <- data.frame(Predictor1 = character(),
                      Predictor2 = character(),
                      MisclassificationRate = numeric()) # predictor combos stored
```

We deploy a for loop to go through all potential pairs of predictors. This helps us identify which pair has the lowest missclassification rate. We train the model for each pair using 10-fold cross-validation. 

We calculate the accuracy rate using the calculate_error function which calculates accuracy as follows:

\[
\text{Accuracy} = \frac{1}{n} \sum_{i=1}^{n} \mathbb{I}(\hat{y}_i = y_i)
\]

such that \(\hat{y}_i\) is the predicted class for the \(i\)-th observation, \(y_i\) is the true class for the \(i\)-th observation and \(\mathbb{I}(\cdot)\) is the indicator function

The results for each pair is stored within the results data frame.

The pairs of predictors are sorted from highest to lowest accuracy - the top-performing pair is ca and oldpeak

```{r}

results <- data.frame(Predictor1 = character(),
                      Predictor2 = character(),
                      Accuracy = numeric(),
                      stringsAsFactors = FALSE) # this data frame stores our results

for (pair in predictor_pairs) { # for loop to find 2 most important predictors
  pair <- unlist(pair)
  
  selected_data <- train_data %>% select(all_of(pair), target) # subset to ensure only looking at predictor pair and target
  
  # Validation set approach of logistic model
  
  model_fit <- glm(target ~ ., data = selected_data, family = binomial)
  probabilities <- predict(model_fit, type = "response")
  predictions <- ifelse(probabilities > 0.5, 1, 0)
  
  calculate_error <- function(predictions, true_values, classify) { # accuracy using the calculate_error function
    if (classify) {
      return(mean(predictions == true_values)) # calculate accuracy directly
    } else {
      return(mean((predictions - true_values)^2))
    }
  }
  
  accuracy <- calculate_error(predictions, selected_data$target, classify = TRUE)
  
  results <- rbind(results, 
                   data.frame(Predictor1 = pair[1], 
                              Predictor2 = pair[2], 
                              Accuracy = accuracy,
                              stringsAsFactors = FALSE))
}


best_pairs <- results[order(-results$Accuracy), ] # results sorted from most to least accurate

head(best_pairs)
```

## DAG and data issues

Causality in heart disease can be understood via a Directed Acyclic Graph (DAG) which  maps the relationships between various contributing predictors. In our DAG, we focus on six  variables that we believe most significantly influence the likelihood of developing heart disease.

Ranked in order of relative impact, the most influential predictor is oldpeak, which measures ST depression on an electrocardiogram (Patil, 2025). This reflects how the heart responds under stress compared to rest, with a downward slope indicating potential dysfunction (Patil, 2025). Next is ca, which captures the number of major blood vessels showing signs of blockage (Patil, 2025). The more vessels affected, the higher the cardiovascular risk (Patil, 2025). Trestbps, recording resting blood pressure, follows closely, as high blood pressure at rest is linked to heart disease, stroke and chronic cardiovascular issues (Patil, 2025).

The fourth predictor is restecg, which looks at resting electrocardiogram results (Patil, 2025). Abnormalities here may potentially signal increased strain on the heart or underlying valvular  defects (Patil, 2025). The last two predictors, chol (cholesterol levels) and fbs (fasting blood sugar) are generally considered more manageable through lifestyle changes (Patil, 2025). Diet, exercise and medication can significantly reduce their impact, whereas the other variables often indicate deeper physiological issues that may require surgical intervention (Patil, 2025).

These predictors not only contribute individually but are also interlinked. For instance, higher fasting blood sugar can lower HDL (good cholesterol) and raise LDL (bad cholesterol) worsening lipid profiles (Patil, 2025). Greater cholesterol levels lead to plaque build-up in the arteries, narrowing them and forcing the heart to work harder (Patil, 2025). This can result in increased blood pressure, known as hypertension, which is often detected as ST depression on an ECG (Patil, 2025). Such findings might prompt further investigation through procedures like a fluoroscopy to assess the extent of cardiovascular damage (Patil, 2025).

```{r}
# DAG
heart_dag <- ggdag::dagify(chol ~ fbs,
                           trestbps ~ chol,
                           oldpeak ~ trestbps,
                           restecg ~ oldpeak,
                           ca ~ restecg,
                           target ~ fbs,
                           target ~ chol,
                           target ~ trestbps,
                           target ~ oldpeak, 
                           target ~ restecg,
                           target ~ ca)

ggdag::ggdag(heart_dag) + theme_void()
```

We encountered issues with our dataset sourced from Kaggle, namely related to counterintuitive findings that raised concerns about the accuracy of certain attributes. For instance, the dataset suggests that experiencing chest pain during exercise correlates with a lower likelihood of heart disease, while showing no chest pain increases the risk of having heart disease, an interpretation that contradicts science (Hamada, 2025). This anomaly suggests that the target variable in our Kaggle dataset may have been reversed by the uploader - where a value of 0 actually represents a diseased heart and 1 indicates a healthy heart, rather than the original convention where 0 = healthy and 1 = diseased.

Thus, while the coefficients discussed in our report remain valid in terms of magnitude and relative importance, their directional interpretation should be considered in reverse. 

## Confounding

If we were to include only trestbps (resting blood pressure) as a predictor in a logistic regression model against the target, the estimated effect on the log-odds of heart disease might appear stronger than it actually is. This overestimation occurs because trestbps is positively correlated with cholesterol, a variable omitted in this simplified model. When blood pressure rises, it coincides with an underlying increase in cholesterol levels. As a result, the model mistakenly attributes some of cholesterol's influence on target to trestbps, leading to a higher coefficient. This is an example of Omitted Variable Bias, where the absence of a relevant predictor, in our case cholestrol, causes the effect of an included variable to be biased in magnitude.

```{r}
glm2 <- glm(y ~ trestbps, data = train_data, family = binomial)
summary(glm2)
```

While trestbps eliminates OVB, we believe controlling for confounders will help in correcting bias. 

Therefore, we add both fbs and chol as controls. All predictors are standardised via the scale function to help simplify interpretations.

The rise in one unit of trestbps signifies a 0.18 unit increase in log odds of having heart disease, holding both cholestrol and blood sugar constant. Please note once again, that we consider the reverse of the directional interpretation given the issues from our dataset. 

However, here you can note that cholestrol has the highest impact of the three variables on the probability of developing heart disease, namely a 0.24 unit increase in log odds of having heart disease.

```{r}
# scale predictors
trestbps_scale <- scale(train_data$trestbps)
chol_scale <- scale(train_data$chol)
fbs_scale <- scale(train_data$fbs)

glm3 <- glm(y ~ trestbps_scale + chol_scale + fbs_scale, data = train_data, family = binomial)
summary(glm3)
```

Given that cholestrol has the largest relative impacts on the log odds of having heart disease, this should be controlled. This can be done via ensuring that one has a low fat diet (avoiding fatty foods that contain saturated fats), ensuring they exercise more often, stopping smoking and cutting down on drinking habits (NHS, 2025). This decrease in cholestrol as a results in less narrowing of arteries and thus reduced strain on the heart (Cleveland Clinic, 2025). Therefore, we would recommend such changes to patient to ensure that the likelihood of heart disease development remains as low as possible. 

# Conclusion 

We had two key objectives in our project:

1. To predict the likelihood of an individual developing heart disease - specifically, to identify those at higher risk and develop a model that could be used to assist in diagnosis beyond our initial sample.

2. To identify non-surgical treatment methodologies that can help reduce heart disease risk.

The model we developed can be used in a screening setting to flag individuals who may benefit from lifestyle interventions and further medical attention. Among the modifiable risk factors analysed, diet emerged as one of the most effective and accessible methods of reducing the risk of heart disease. It is a controllable factor that individuals can change relatively easily, making it a highly practical non-surgical intervention.

In particular, dietary changes play a crucial role in lowering cholesterol levels, which in turn can positively influence other important predictors of heart disease, such as blood pressure. A healthy diet can help regulate cholesterol, reduce blood pressure and enhance overall cardiovascular health, highlighting it as a powerful and effective non-surgical approach to prevention.

Bibliography

Patil, M. (2025, April 1). Variables impacting presence of heart disease. (A. Patil, Interviewer)
Hamada, M. (2025, 16 April). Heart Disease Dataset - Discussion. Retrieved from Kaggle: https://www.kaggle.com/datasets/johnsmith88/heart-disease-dataset/discussion/171669
NHS. (2025, April 21). How to lower your cholesterol. Retrieved from NHS: https://www.nhs.uk/conditions/high-cholesterol/how-to-lower-your-cholesterol/
Cleveland Clinic. (2025, April 21). High Cholesterol Diseases. Retrieved from Cleveland Clinic: https://my.clevelandclinic.org/health/articles/11918-cholesterol-high-cholesterol-diseases. 