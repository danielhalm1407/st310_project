---
title: "Gradient Descent Implementation for a Logistic Regression Model"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

```{r include=FALSE}
library(tidyverse)
library(tidymodels)
library(pROC)

Heart <- read.csv("../data/Heart.csv")
```

We implemented a simple gradient descent algorithm to classify if patients have a presence of coronary artery disease or not given $p \in \mathbb{N}$ medical characteristics $\boldsymbol{\theta} = (\theta_{1}, \dots, \theta_{p})^\mathsf{T}$.

Consider a regularized logistic regression model with the parameters $\boldsymbol{\beta}$ being a $(p+1) \times 1$ column vector containing our medical characteristics of interest ($\boldsymbol{\theta}$) and an added intercept coefficient. We are given $N \in \mathbb{Z}$ observations. Let $\boldsymbol{X} = (x_{1}, \dots, x_{N})$ be the design matrix of size $N \times (p+1)$ for the input space where each $\boldsymbol{x_{i}}$ is a row vector of dimension $1 \times (p+1)$ denoting the $i$-th row of observations. Finally, suppose $\boldsymbol{y} = (y_{1}, ..., y_{N})^\mathsf{T}$ be the target classes column vector of dimension $N \times 1$ representing if each observation has CAD or not.

We first define our predictors of interest in a vector which is used throughout our implementation. We then filter the dataset for the requested predictors and the the binary response `target` before passing this subset into the `tidymodel` method `initial_split`, subdividing the data into 70% `training` and 30% `testing`.

```{r}
predictor_names <- c("chol", "thalach", "cp")

data_subset <- Heart %>% select(all_of(predictor_names), "target")
Heart_split <- initial_split(data_subset, prop = 0.7)

training <- Heart_split %>% training()
testing <- Heart_split %>% testing()
```

We seek to find the optimal coefficients $\boldsymbol{\beta} = (\beta_{0}, \dots, \beta_{p})^\mathsf{T}$ that minimizes the negative log-likelihoods with some added positive penalty term $\lambda \in \mathbb{R}$. We decided to go for an L2 (ridge) regularization because [EXPLAIN WHY HERE!!!!!!!!] The negative log-likelihoods, also known as the logistic loss, are given by the following equation

$$
\ell(\beta \; ; \; y  \, \vert \, x) = -\sum_{i=0}^{n} \left [ \, y_{i} \log(\sigma(x_{i})) + (1 - y_{i}) \log(1 - \sigma(x_{i})) \, \right] + \lambda \sum_{j=1}^{p} \beta_{j}^{2},
$$

where $\sigma(\bullet \; ; \; \beta)$ is given by the logistic function (commonly denoted as the sigmoid function)

$$
\sigma(x_{i}) = \frac{1}{1 + e^{-x_{i}\beta}} .
$$

We first need to find the gradient of the loss function by computing derivatives

$$
\frac{\partial}{\partial \beta} \; \ell(\beta \; ; \; y  \, \vert \, x) = -\sum_{i=0}^{n} \left [ \, y_{i} \frac{1}{\sigma(x_{i})} \frac{\partial \sigma(x_{i})}{\partial \beta} - (1 - y_{i}) \frac{1}{1 - \sigma(x_{i})} \frac{\partial (1-\sigma(x_{i}))}{\partial \beta} \, \right] + 2\lambda\beta.
$$

We see that

$$
\frac{\partial}{\partial \beta} \; \sigma(x_{i} \; ; \; \beta) = \frac{\partial}{\partial \beta} \left ( \frac{1}{1 + e^{-x_{i}\beta}} \right) = \frac{x_{i} e^{-x_{i}\beta}}{(1 + e^{-x_{i}\beta})^{2}} = \sigma(x_{i})(1-\sigma(x_{i}))x_{i},
$$

and so substituting back into the gradient function simplifies the equation to

$$
\begin{align}
\frac{\partial}{\partial \beta} \; \ell(\beta \; ; \; y  \, \vert \, x) &= -\sum_{i=0}^{n} \left [ \, y_{i}(1 - \sigma(x_{i}))x_{i} - (1-y_{i})\sigma(x_{i})x_{i} \, \right] + 2\lambda\beta\\
&=\sum_{i=0}^{n} \left [ \, (\sigma(x_i) - y_{i}) x_{i} \, \right ] + 2\lambda\beta.
\end{align}
$$

(Notice we distributed the negative in front of the summation through and factored out the $x_{i}$ found in both terms). We define these functions in R for later in the implementation.

```{r}
# s-shaped sigmoid function
logistic_func <- function(logits) {
	1/(1 + exp(-logits))
}

# implemented using L2 ridge regularization (note: the intercept term does not get penalized)
logistic_loss <- function(betas, X, y, lambda) {
	# note that `y_hats` are the predicted conditional probabilities having applied the sigmoid function
	# on the design matrix and coefficients product NOT the predicted classes of heart disease presence
	y_hats <- logistic_func(X %*% betas)
	penalty <- lambda * sum(betas[-1] * betas[-1])
	
	-sum(y * log(y_hats) + (1 - y) * log(1 - y_hats)) + penalty
}

# this function is not a numeric approximation of the gradient but rather the direct analytical
# functional form as derived in the mathematics above
gradients <- function(betas, X, y, lambda) {
	# note that `y_hats` are the predicted conditional probabilities having applied the sigmoid function
	# on the design matrix and coefficients product NOT the predicted classes of heart disease presence
	y_hats <- logistic_func(X %*% betas)
	
	# separate the logistic and penalty terms to compute gradients easier
	logistic_gradient <- t(X) %*% (y_hats - y)
	penalty_gradient <- 2 * lambda * betas[-1]
	
	# sum the two components of the gradient while keeping the intercept term non-regularized
	logistic_gradient + c(0, penalty_gradient)
}
```

Something else goes here.

```{r}
# format the training dataset by removing the target column, adding an intercept
# column of all 1s, and scaling the inputs before returning it as a matrix
# (necessary for the %*% notation used)
design_matrix <- function(training) {
	training %>%
		select(-c(all_of("target"))) %>%
		mutate(intercept = 1, .before = 1) %>%
		# mutate_at(vars(-intercept), scale) %>%
		as.matrix
}

# returns the optimal step size as computed by the Barzilai-Borwein method given
# the current and previous beta and gradient
update_step_size <- function(iteration, betas, gradients) {
	betas_delta <- betas[iteration - 1,] - betas[iteration - 2,]
	gradients_delta <- gradients[iteration - 1,] - gradients[iteration - 2,]
# I SWEAR TO GOD LOOK INTO THIS TO LEARN ABOUT IT!!!!
	sum(betas_delta %*% betas_delta) / sum(betas_delta %*% gradients_delta)
}

gradient_descent <- function(training, predictor_names, learning_rate = 1e-6, tolerance = 0.0001, lambda = 0.01, max_iterations = 10000) {
	# format the X and y training values to be of use
	X_training <- design_matrix(training)
	y_training <- training$target

	# randomly sample `p` uniformly-distributed values for the initial coefficients
	# since the inputs are scaled in the range [0,1]
	initial_betas <- runif(ncol(X_training))
	
	# each value is an iteration of updated losses to keep track of its history;
	# the same goes for the step sizes
	losses <- numeric(max_iterations)
	step_sizes <- numeric(max_iterations)

	# each row is an iteration of updated coefficients to keep track of its history;
	# the same goes for the gradients
	betas <- matrix(NA, nrow = max_iterations, ncol = ncol(X_training))
	gradients <- matrix(NA, nrow = max_iterations, ncol = ncol(X_training))

	# save the initial coefficients as the randomly selected ones and manually
	# kickstart the descent algorithm
	betas[1,] <- initial_betas
	gradients[1,] <- gradients(betas[1,], X_training, y_training, lambda)
	step_sizes[1] <- learning_rate
	losses[1] <- logistic_loss(betas[1,], X_training, y_training, lambda)

	# another iteration of manually computing the descent algorithm to ensure
	# that it is working before looping through
	betas[2,] <- betas[1,] - step_sizes[1] * gradients[1,]
	gradients[2,] <- gradients(betas[2,], X_training, y_training, lambda)
	step_sizes[2] <- learning_rate
	losses[2] <- logistic_loss(betas[2,], X_training, y_training, lambda)

	for(iteration in 3:max_iterations) {
		previous_beta <- betas[iteration - 1,]
		previous_gradient <- gradients[iteration - 1,]
		previous_step_size <- step_sizes[iteration - 1]

		# compute the optimal step size using the Barzilai-Borwein method and then
		# update the coefficients, gradients, and losses
		step_sizes[iteration] <- update_step_size(iteration, betas, gradients)
		betas[iteration,] <- previous_beta - step_sizes[iteration] * previous_gradient
		gradients[iteration,] <- gradients(betas[iteration,], X_training, y_training, lambda)
		losses[iteration] <- logistic_loss(betas[iteration,], X_training, y_training, lambda)
		
		# we compute the norm, or size of, the gradient vector using the L2 norm
		# (Euclidean norm) and check if it is reasonably small enough
		if (norm(gradients[iteration,], type = "2") < tolerance) {
			# trim the betas, gradients, losses, and step sizes matrices/vectors
			# as to not have a bunch of trailing zeros or NULL values and then exit
			# the loop
			betas <- betas[1:iteration,]
			gradients <- gradients[1:iteration,]
			losses <- losses[1:iteration]
			step_sizes <- step_sizes[1:iteration]
			break
		}
	}

	# return all of the matrices and vectors used in a R list which will be
	# convenient by using the `$.` notation on the model output
	list(
		coefficients = betas,
		gradients = gradients,
		losses = losses,
		step_sizes = step_sizes,
		predictors = c("(Intercept)", predictor_names)
	)
}
```

Note that we scale every column in the input space (besides the intercept and target columns) in the `design_matrix` function. When we ran this gradient descent algorithm on unscaled values the logistic loss broke down after the first few iterations. A deeper examination revealed that the scale of the input values could range from 126â€“564 mg/dL (in the case of cholesterol) whereas our response is only binary. This size difference meant that $\boldsymbol{X\beta}$ would either produce extremely large or extremely small values which would become numerically unstable once the sigmoid function was applied $\sigma(\boldsymbol{X\beta})$.

Consider if $\sigma(\boldsymbol{X\beta}) = 0$. Then, $\log(\sigma(\boldsymbol{X\beta})) = \log(0)$ which is undefined. Similarly, if $\sigma(\boldsymbol{X\beta}) = 1$ then, $\log(1-\sigma(\boldsymbol{X\beta})) = \log(0)$ which is also undefined. The output of sigmoid being exactly 0 or exactly 1 happens when the result of $\boldsymbol{X\beta}$ is numerically unstable. The `logistic_loss` method would save the current iteration's loss as `NaN` (not a number) in the losses history every iteration. The `coefficients` and `gradients` contained large vectors meaning the descent jumped around a lot and never converged on the local minima, only stopping due to the `max_iterations` being met.

We scaled down the input space $\boldsymbol{X}$ (besides the intercept and target columns) to combat the numerical instability and our implementation works as expected.

Before we run our logistic regression gradient descent implementation on our project data we want to assure that it works on simulated data.

```{r}
simulate_data <- function(n_observations = 5000, p_predictors = 10) {
	set.seed(1)

	sparsity <- p_predictors / 2

	nonzero_betas <- runif(sparsity, min = -1, max = 1)
	true_coefficients <- c(.5, nonzero_betas, rep(0, sparsity))

	# we again generate random values from a uniform distribution to match the scale
	# of the inputs of the target dataset
	raw_data <- matrix(
		runif(n_observations * p_predictors),
		ncol = p_predictors
	)

	X_simulated <- cbind(
		rep(1, n_observations),
		raw_data
	)
	y_simulated <- rbinom(
		n = n_observations,
		size = 1,
		prob = logistic_func(X_simulated %*% true_coefficients)
	)

	# create the simulated dataset in the correct format as expected by the gradient
	# descent method; additionally create dummy predictor names using R's default V{1,...,p}
	# undefined column name notation
	simulated_dataset <- cbind(X_simulated[,-1], target = y_simulated) %>% as.data.frame
	predictor_names <- paste0("V", 1:p_predictors)

	set.seed(NULL)

	list(
		data = simulated_dataset,
		true_coefficients = true_coefficients,
		predictors = predictor_names
	)
}

p_predictors <- 6

simulated_training <- simulate_data(n_observations = 1025, p_predictors = p_predictors)
simulated_training$data %>% head
simulated_training$true_coefficients %>% print
```

We also will define the simulated testing dataset which will be used later to evaluate the naive model later. It is generated in a similar manner to its training counterpart.

```{r}
simulated_testing <- simulate_data(n_observations = 500, p_predictors = p_predictors)
simulated_testing$data %>% head
simulated_testing$true_coefficients %>% print
```

We then now define our naive model by calling our `gradient_descent` implementation on the `simulated_training` object so that we can later visualize how the model performs on the testing stage and, eventually, how it performs on unseen testing data.

```{r}
naive_model <- gradient_descent(
	simulated_training$data,
	simulated_training$predictors,
	learning_rate = 1e-6,
	tolerance = 1e-3,
	lambda = 0.5,
	max_iterations = 10000
)
```

We additionally define some methods for understanding the output of our model including a table of coefficients, a plot of losses over iterations, a plot of step sizes over iterations, and a plot of coefficients over iterations.

We can now analyze what the naive model converged on for its coefficients versus the true coefficients by using the `compare_coefficients` method. This function generates a table of true and expected coefficients with a third column being their difference to more easily view how big the difference is.

```{r}
compare_coefficients <- function(modelA_coefficients, modelB_coefficients, predictors, column_names) {
	# compute the difference in coefficients to more easily see how off our implementation estimates are
	# either in the case of our model versus R's `glm` version or our naive model versus the true
	# coefficients
	difference <- modelA_coefficients - modelB_coefficients

	# assemble the table and make it more readable with meaningful column and row names
	coefficients_comparison <- cbind(modelA_coefficients, modelB_coefficients, difference)
	colnames(coefficients_comparison) <- c(column_names, "difference")
	rownames(coefficients_comparison) <- predictors

	coefficients_comparison
}

naive_coefficients <- naive_model$coefficients[nrow(naive_model$coefficients),]
compare_coefficients(
	modelA_coefficients = naive_coefficients,
	modelB_coefficients = simulated_training$true_coefficients,
	predictors = c("(Intercept)", simulated_training$predictors),
	column_names = c("estimated coef.", "true coef.")
)
```

We can also observe the logistic loss history saved in the `naive_model` object and view it on a plot via the `plot_losses` function. Note that this method accepts a `model_type` parameter which is placed into the plot's title for clarity so that this function can be used by both the naive and (later) the final models.

```{r}
plot_losses <- function(model, model_type = "normal") {
	n_iterations <- length(model$losses)
	min_loss <- min(model$losses)

	ggplot(data.frame(iteration = 1:n_iterations, loss = model$losses)) +
		# connect each loss value per iteration with a solid blue line
		geom_line(aes(x = iteration, y = loss), color = "blue", linewidth = 1) +
		# visibly show where the minimum loss achieved is and add text with the number
		geom_hline(yintercept = min_loss, linetype = "dotted", color = "gray", linewidth = 0.75) +
		annotate("text", x = n_iterations * 0.9, y = min_loss, label = paste("minimum loss:", round(min_loss, 2)), vjust = -0.5, color = "darkgray") +
		scale_x_continuous(
			limits = c(1, n_iterations),
			breaks = c(1:n_iterations)
		) +
		# we use the parameter `model_type` since we use this same method for both our actual model
		# and the naive model
		labs(
			title = paste("Losses of binary cross-entropy of", model_type, "model over iterations"),
			subtitle = paste("Fitted on the predictor(s)", toString(model$predictors)),
			caption = "Data from the 1988 UCI Heart Disease study"
		) +
		theme_classic() +
		theme(legend.position = "none") +
		xlab("Iteration") +
		ylab("Loss")
}

plot_losses(model = naive_model, model_type = "naive")
```

We can additionally observe the step sizes history saved in the `naive_model` object and view it on a plot via the `plot_steps` function. It is nearly identical to `plot_losses` but with different labels and verbiage.

```{r}
plot_steps <- function(model, model_type = "normal") {
	n_iterations <- length(model$step_sizes)
	min_step <- min(model$step_sizes)

	ggplot(data.frame(iteration = 1:n_iterations, step = model$step_sizes)) +
		# connect each step size value per iteration with a solid blue line
		geom_line(aes(x = iteration, y = step), color = "blue", linewidth = 1) +
		# visibly show where the minimum step size achieved is and add text with the number
		geom_hline(yintercept = min_step, linetype = "dotted", color = "gray", linewidth = 0.75) +
		annotate("text", x = n_iterations * 0.9, y = min_step, label = paste("minimum step:", round(min_step, 2)), vjust = -0.5, color = "darkgray") +
		scale_x_continuous(
			limits = c(1, n_iterations),
			breaks = c(1:n_iterations)
		) +
		labs(
			title = paste("Barzilai-Borwein step sizes of", model_type, "model over iterations"),
			subtitle = paste("Fitted on the predictor(s)", toString(model$predictors)),
			caption = "Data from the 1988 UCI Heart Disease study"
		) +
		theme_classic() +
		theme(legend.position = "none") +
		xlab("Iteration") +
		ylab("Step size")
}

plot_steps(model = naive_model, model_type = "naive")
```

We lastly observe the coefficients history saved in the `naive_model` object and view it on a plot via the `plot_steps` function. This allows readers to see, first off, a snapshot of all the coefficients across the iterations and, secondly, see how each coefficient changes throughout the descent.

```{r}
plot_betas <- function(model, model_type = "normal") {
	n_iterations <- nrow(model$coefficients)

	to_plot <- data.frame(
		iteration = c(1:n_iterations),
		coefficient = as.data.frame(model$coefficients)
	)
	colnames(to_plot) <- c("iteration", model$predictors)

	# we re-shape the data so that we can connect each coefficient across iterations AND
	# show all coefficients at each snapshot via pivoting the dataframe longwise
	to_plot <- to_plot %>%
		pivot_longer(cols = -iteration, names_to = "predictor", values_to = "coefficient")

	ggplot(to_plot, aes(x = iteration, y = coefficient, color = predictor)) +
		geom_point(size = 3, stroke = 1.2) +
		geom_line() +
		scale_x_continuous(
			limits = c(1, n_iterations),
			breaks = c(1:n_iterations)
		) +
		labs(x = "Iteration", y = "Coefficient") +
		labs(
			title = paste("Esimated coefficients of the", model_type, "model over iterations"),
			subtitle = paste("Fitted on the predictor(s)", toString(model$predictors)),
			caption = "Data from the 1988 UCI Heart Disease study"
		) +
		theme_minimal()
}

plot_betas(model = naive_model, model_type = "naive")
```

Having analyzed the naive model coefficients and visualized the losses and step sizes, we now seek to evaluate it by applying the `testing` subset and seeing how accurate its predictions are. We do this with the `prediction_metrics` method.

```{r include=FALSE}
as_grade <- function(raw_score) {
	# convert scores of the form 0.xxxxxxx to xx.xx for easier readability of model accuracy
	# as a percentage
	round(raw_score * 100, 2)
}

prediction_metrics <- function(model, training, testing, model_type = "normal", is_glm = FALSE) {
	# save the vectors of true classes from the training and testing subsets, respectively
	true_classes_training <- training$target
	true_classes_testing <- testing$target
	
	# if using the `glm` model, the inputs need to be scaled to match the inputs passed into the
	# naive and normal models AND the prediction step is different (built-in R function)
	if(is_glm) {
		r_training <- training %>% mutate_at(vars(-target), scale)
		r_testing <- testing %>% mutate_at(vars(-target), scale)
		
		predictions_training <- predict(model, newdata = r_training, type = "response")
		predictions_testing <- predict(model, newdata = r_testing, type = "response")
	} else {
		X_training <- design_matrix(training)
		X_testing <- design_matrix(testing)
		
		final_betas <- model$coefficients[nrow(model$coefficients),]
		
		predictions_training <- logistic_func(X_training %*% final_betas)
		predictions_testing <- logistic_func(X_testing %*% final_betas)
	}
	
	# convert the probabilities in `predictions_training` and `predictions_testing` into definitive
	# classifications of 0 or 1 depending on if the calculated probability p >= 0.5 or not
	prediction_classes_training <- as.vector(ifelse(predictions_training >= 0.5, 1, 0))
	prediction_classes_testing <- as.vector(ifelse(predictions_testing >= 0.5, 1, 0))

	# we use mean(...) since these are vectors of 0 and 1 so it effectively computes the accuracy;
	# we use the roc and auc methods from the pROC library
	training_accuracy <- mean(prediction_classes_training == true_classes_training)
	training_auc <- auc(roc(response = true_classes_training, predictor = prediction_classes_training))
	
	testing_accuracy <- mean(prediction_classes_testing == true_classes_testing)
	testing_auc <- auc(roc(response = true_classes_testing, predictor = prediction_classes_testing))
	
	# return all of these vectors and matrices for visualization and later analysis
	list(
		true_classes_training = true_classes_training,
		true_classes_testing = true_classes_testing,
		prediction_classes_training = prediction_classes_training,
		prediction_classes_testing = prediction_classes_testing,
		training_accuracy = as_grade(training_accuracy),
		training_auc = as_grade(training_auc),
		testing_accuracy = as_grade(testing_accuracy),
		testing_auc = as_grade(testing_auc),
		model_type = model_type
	)
}

naive_metrics <- prediction_metrics(
	model = naive_model,
	training = simulated_training$data,
	testing = simulated_testing$data,
	model_type = "naive",
	is_glm = FALSE
)
```

We will analyze the metrics of the naive model once we do the same with the normal model and R's `glm` model. Now we create these model objects and examine the output of the normal model output.

```{r}
log_model <- gradient_descent(
	training = training,
	predictor_names = predictor_names,
	learning_rate = 1e-2,
	tolerance = 1e-5,
	lambda = 1.7,
	max_iterations = 10000
)

r_training <- training %>% mutate_at(vars(-target), scale)
r_model <- glm(target ~ ., family = binomial, data = r_training)
```

Because we do not know the "true coefficients" of our actual heart disease dataset we instead compare our model's coefficients against R's `glm` model's coefficients to see how similar the values are. As seen in the table blow, the difference between our model and R's coefficients are on the scale of $10^{-3}$ and smaller.

```{r}
log_coefficients <- log_model$coefficients[nrow(log_model$coefficients),]
compare_coefficients(
	modelA_coefficients = log_coefficients,
	modelB_coefficients = r_model$coefficients,
	predictors = log_model$predictors,
	column_names = c("model coef.", "R glm coef.")
)
```

```{r}
summary(r_model)
```

Examining the $p$-values for these coefficients we see that [EXAMINE THE TRUE PREDICTORS USED ONCE HAVING DONE STEPWISE SELECTION!!!!]

```{r}
plot_losses(model = log_model, model_type = "normal")
plot_steps(model = log_model, model_type = "normal")
plot_betas(model = log_model, model_type = "normal")
```

We see that the logistic loss decreases sharply and lands on a minimum loss of 384.18 at around 13 iterations. The step size history is more random but regardless the movement of the gradient vector is extremely small as shown by the scale of the $y$-axis. Finally, ... [FIX ALL OF THIS WITH THE ACTUAL PREDICTORS IN USE].

We can now analyze the prediction metrics of our model and R's `glm` model to evaluate their effectiveness as well as compare to our naive model.

```{r include=FALSE}
log_metrics <- prediction_metrics(
	model = log_model,
	training = training,
	testing = testing,
	model_type = "normal",
	is_glm = FALSE
)

r_metrics <- prediction_metrics(
	model = r_model,
	training = training,
	testing = testing,
	model_type = "glm",
	is_glm = TRUE
)
```

We now look at the accuracy and AUC metrics across all three models as summarized in the following table.

```{r}
all_metrics <- data.frame(
	"training accuracy" = c(
		naive_metrics$training_accuracy,
		log_metrics$training_accuracy,
		r_metrics$training_accuracy
	),
	"training auc" = c(
		naive_metrics$training_auc,
		log_metrics$training_auc,
		r_metrics$training_auc
	),
	"testing accuracy" = c(
		naive_metrics$testing_accuracy,
		log_metrics$testing_accuracy,
		r_metrics$testing_accuracy
	),
	"testing auc" = c(
		naive_metrics$testing_auc,
		log_metrics$testing_auc,
		r_metrics$testing_auc
	)
)

rownames(all_metrics) <- c("naive model", "normal model", "glm model")
colnames(all_metrics) <- c("training accuracy", "training auc", "testing accuracy", "testing auc")

all_metrics
```

We want to also view the Receiver Operating Characteristic (ROC) curve available via the `pROC` library in our wrapper function of `plot_ROC` to compare the true and false positive rates. Note that this is plotted on the `testing` data as another visualization of model accuracy. As expected, the naive model is slightly over 50% which indicates that our gradient descent implementation picks up some slight noise in the data but is overall not much better than flipping a coin to determine heart disease presence based on [INSERT CHOSEN PREDICTORS HERE].

```{r}
plot_ROC <- function(metrics, curve_color, y_height, add_to = FALSE) {
	# `y_height` is the height where the AUC percentage label is printed so that they do not
	# overlap; `add_to` is needed to overlap the naive, normal, and glm model ROC curves
	roc_object <- roc(
		response = metrics$true_classes_testing,
		predictor = metrics$prediction_classes_testing,
		plot = TRUE,
		percent = TRUE,
		xlab = "False positive percentage",
		ylab = "True positive percentage",
		col = curve_color,
		lwd = 3,
		print.auc = TRUE,
		print.auc.y = y_height,
		add = add_to,
		quiet = TRUE
	)
}

plot_ROC(metrics = naive_metrics, curve_color = "red", y_height = 45, add_to = FALSE)
plot_ROC(metrics = log_metrics, curve_color = "blue", y_height = 35, add_to = TRUE)
plot_ROC(metrics = r_metrics, curve_color = "lightgreen", y_height = 25, add_to = TRUE)

models <- c("naive model", "normal model", "glm model")
colors <- c("red", "blue", "lightgreen")

legend("bottomright", legend = models, col = colors, lwd = 3)
```
