---
title: "Gradient Descent Implementation for a Logistic Regression Model"
output:
  word_document: default
  pdf_document: default
---

# Gradient Descent Implementation for a Logistic Regression Model

```{r include=FALSE}
library(tidyverse)
library(tidymodels)
library(pROC)
```

```{r}
Heart <- read.csv("../data/Heart.csv")
```

We implemented a simple gradient descent algorithm to classify if patients have a presence of coronary artery disease or not given some medical characteristics $\theta = (\theta_{0}, \dots, \theta_{p})$ for $p \in \mathbb{N}$.

Consider a logistic regression model with $p+1$ (including the constant coefficient term) parameters. Let $X = (x_{1}, \dots, x_{n})$ represent input space where each $x_{i}$ is a vector of length $p+1$ denoting $i$-th row of $X$. Similarly, let $y = (y_{1}, \dots, y_{n})$ be a vector of length $n$ such that each $y_{i} \in \{0,1\}$.

We first divide our `Hearts` dataset into training and testing subsets by defining the following method. The `split_data` function neatly returns training and testing subsets (for our model and R's built-in `glm`) given a vector of column names from the dataset to act as predictors in the model. We also can set the proportion of data allocated to the training and testing subsets with a default of 70/30, respectively.

```{r}
split_data <- function(predictors, training_proportion = 0.7, data) {
	response = c("target")

	raw_inputs <- data %>%
		select(all_of(predictors), "target") %>%
		mutate_at(vars(-response), scale)

	n_training = floor(nrow(data) * training_proportion)

	observation_indices <- c(1:nrow(data))
	sampled_indices <- sample(observation_indices, n_training)

	training_subset <- raw_inputs %>%
		slice(sampled_indices)
	testing_subset <- raw_inputs %>%
		slice(-sampled_indices)

	X_training <- cbind(1, training_subset %>% select(-all_of(response)))
	y_training <- training_subset %>% select(all_of(response))

	X_testing <- cbind(1, testing_subset %>% select(-all_of(response)))
	y_testing <- testing_subset %>% select(all_of(response))

	list(
		X_training = as.matrix(X_training),
		y_training = y_training$target,
		X_testing = as.matrix(X_testing),
		y_testing = y_testing$target,
		r_training = training_subset,
		r_testing = testing_subset %>% select(-all_of(response))
	)
}
```

**Note:** We scaled down the inputs (excluding the column of 1s representing the constant coefficient and the `target` response column). We did this because the conditional in our `gradient_descent` method (shown later) kept failing. Upon examination we noticed that applying the logistic function on $X\beta$ would sometimes produce extremely small values that, once passed into the logistic loss function, would practically become $\log(0)$ which was undefined. This `NaN` value would be saved in the `losses` history and then the conditional errored out since the distance was undefined. The medical characteristics acting as our inputs were usually high values like cholesterol which ranges from 126 to 564 mm/dL; these high ranges are far from the response range which likely contributed to the underflow into $\log(0)$.

We seek to find the optimal coefficients $\beta = (\beta_{0}, \dots, \beta_{p})$ that minimizes the negative log-likelihoods. The negative log-likelihoods, also known as the logistic loss, are given by the following equation

$$
\ell(\beta \; ; \; y  \, \vert \, x) = -\sum_{i=0}^{n} \left [ \, y_{i} \log(p(x_{i})) + (1 - y_{i}) \log(1 - p(x_{i})) \, \right] ,
$$

where $p(\bullet \; ; \; \beta)$ is given by the logistic function

$$
p(x_{i}) = \frac{1}{1 + e^{-x_{i}\beta}} .
$$

We first need to find the gradient of the loss function by computing derivatives

$$
\frac{\partial}{\partial \beta} \; \ell(\beta \; ; \; y  \, \vert \, x) = -\sum_{i=0}^{n} \left [ \, y_{i} \frac{1}{p(x_{i})} \frac{\partial p(x_{i})}{\partial \beta} - (1 - y_{i}) \frac{1}{1 - p(x_{i})} \frac{\partial (1-p(x_{i}))}{\partial \beta} \, \right] .
$$

We see that

$$
\frac{\partial}{\partial \beta} \; p(x_{i} \; ; \; \beta) = \frac{\partial}{\partial \beta} \left ( \frac{1}{1 + e^{-x_{i}\beta}} \right) = \frac{x_{i} e^{-x_{i}\beta}}{(1 + e^{-x_{i}\beta})^{2}} = p(x_{i})(1-p(x_{i}))x_{i},
$$

and so substituting back into the gradient function simplifies the equation to

$$
\begin{align}
\frac{\partial}{\partial \beta} \; \ell(\beta \; ; \; y  \, \vert \, x) &= -\sum_{i=0}^{n} \left [ \, y_{i}(1 - p(x_{i}))x_{i} - (1-y_{i})p(x_{i})x_{i} \, \right]\\
&=\sum_{i=0}^{n} \left [ \, (p(x_i) - y_{i}) x_{i} \, \right ] .
\end{align}
$$

(Notice we distributed the negative in front of the summation through and factored out the $x_{i}$ found in both terms). We define these functions in R for later in the implementation.

```{r}
logistic_func <- function(logits) {
	1/(1 + exp(-logits))
}

logistic_loss <- function(betas, X, y) {
	y_hats <- logistic_func(X %*% betas)
	-sum(y * log(y_hats) + (1 - y) * log(1 - y_hats))
}

gradients <- function(betas, X, y) {
	y_hats <- logistic_func(X %*% betas)
	t(X) %*% (y_hats - y)
}
```

We then define the gradient descent algorithm. At the start we set the `initial_betas` (including the constant term) to random values following from the uniform distribution on $[0,1]$. We defined a `betas` matrix to keep track of the history with size $N \times p+1$ where $N$ denotes the set maximum number of iterations. We similarly set a `losses` vector of size $N$ to keep track of its history.

To start off the descent, we manually run the first two iterations of updating the `betas` and recording the `losses`. From there, this function loops through the remaining $N-2$ iterations where it saves the new `betas`. The loop also contains conditional logic where it computes the `loss_delta` (distance between the current and previous loss value) and breaks out of the loop if it detects that this distance is less than or equal to some set `tolerance`. Before breaking out the `betas` matrix and `losses` vector are trimmed down for easier manipulation later.

```{r}
gradient_descent <- function(X_training,
														 y_training,
														 predictor_names,
														 learning_rate = 0.01,
														 tolerance = 0.0001,
														 max_iterations = 10000) {
	initial_betas <- runif(ncol(X_training))
	losses <- numeric(max_iterations)
	
	# each row is an iteration of updated betas
	betas <- matrix(NA, nrow = max_iterations, ncol = ncol(X_training))
	betas[1,] <- initial_betas
	losses[1] <- logistic_loss(betas[1,],
														 X_training,
														 y_training)
	
	# update the betas once manually to kickstart descent algorithm
	betas[2,] <- betas[1,] - learning_rate * gradients(betas[1,],
																										 X_training,
																										 y_training)
	losses[2] <- logistic_loss(betas[2,],
														 X_training,
														 y_training)
	
	for(iteration in 3:max_iterations) {
		previous_beta <- betas[iteration - 1,]
		betas[iteration,] <- previous_beta - learning_rate * gradients(previous_beta,
																																	 X_training,
																																	 y_training)
		losses[iteration] <- logistic_loss(betas[iteration,],
																			 X_training,
																			 y_training)
		
		loss_delta <- abs(losses[iteration] - losses[iteration - 1])
		
		if(loss_delta <= tolerance) {
			losses <- losses[1:iteration]
			betas <- betas[1:iteration,]
			break
		}
	}
	
	list(coefficients = betas,
			 losses = losses,
			 predictors = predictor_names)
}
```

With all of these methods defined we can fit both our own logistic regression model as well as R's built-in `glm`. Note that if we change `predictor_names` and re-run `split_data` that R's `glm` will contain the same data as our model without needing to manually update its inputs.

```{r}
predictor_names <- c("chol", "thalach", "cp")

splitted <- split_data(predictors = predictor_names,
											 training_proportion = 0.7,
											 data = Heart)

r_model <- glm(target ~ ., family = binomial, data = splitted$r_training)

log_model <- gradient_descent(splitted$X_training,
															splitted$y_training,
															predictor_names,
															learning_rate = 0.01,
															tolerance = 0.01,
															max_iterations = 10000)
```

We also define some more methods for analyzing each model's performance. The `compare_coefficients` function extracts the coefficients from our model (the last row in the `coefficients` matrix) and the `glm`. This extraction is neat since the gradient descent function returns the `coefficients` matrix, `losses` vector, and `predictors` vector as a list that can be access via the `$` notation. The function returns a table of both model's coefficients side-by-side. The row names are set to the same as `log_model$predictors` so it will automatically update.

```{r}
compare_coefficients <- function(model, r_model) {
	final_iteration <- nrow(model$coefficients)
	predicted_coefficients <- matrix(model$coefficients[final_iteration,])
	r_coefficients <- matrix(r_model$coefficients)

	deltas <- r_coefficients - predicted_coefficients

	coefficients_comparison <- cbind(predicted_coefficients, r_coefficients, deltas)
	colnames(coefficients_comparison) <- c("Estimates", "R estimates", "Deltas")
	rownames(coefficients_comparison) <- c("(Intercept)", model$predictors)
	coefficients_comparison
}
```

To see the loss history we defined the following wrapper method around a `ggplot` using `geom_line`. The subtitle dynamically updates with the parameters used in our model so that readers can remember which predictors were fitted.

```{r}
plot_losses <- function(model) {
	n_iterations <- length(model$losses)

	ggplot(data.frame(iteration = 1:n_iterations, loss = model$losses)) +
		geom_line(aes(x = iteration, y = loss, color = "blue"), size = 2) +
		labs(title = "Losses of binary cross-entropy over iterations",
				 subtitle = paste("Fitted on the predictor(s)", toString(model$predictors)),
				 caption = "Data from the 1988 UCI Heart Disease study") +
		theme_classic() +
		theme(legend.position = "none") +
		xlab("Iteration") +
		ylab("Loss")
}
```

```{r}
compare_coefficients(log_model, r_model)
plot_losses(log_model)
```

We see that our coefficients are very similar to the `glm` coefficients suggesting that our gradient descent model is behaving optimally. The helpful `deltas` column allows readers to quickly identify the distances of each predicted coefficient between the two models which are minuscule.

The losses history graph similar shows a decrease from a high starting loss to a lower ending loss. Our implementation did not need many iterations to converge.

We finally wish to test both models and evaluate their prediction accuracies. We easily find both our training and testing predictions by applying $X\beta^{(N)}$ on the testing subset where $\beta^{(N)}$ are the betas found from the final iteration of the gradient descent. Since these vectors are probabilities we transform them into binary predictions by setting values above a given threshold to `1` and values under the threshold to `0`. We follow a similar procedure for the `glm` predictions. Now the variables `?_classes` are binary vectors just like `y_training` and `y_testing`.

For the training, testing, and `glm` accuracies we compare each respective `?_classes` with the true `y` values. This will return a binary vector with `1` in indices where the model predicted correctly and `0` in indices where the model predicted incorrectly. Taking the mean of this vector finds the respective accuracy.

We also seek to find the area under the curve (AUC) of the Receiver Operating Characteristic (ROC). **EXPLAIN THIS PORTION BETTER LATER.**

```{r}
compare_predictions <- function(splitted, model, r_model, threshold = 0.5) {
	final_betas <- model$coefficients[nrow(model$coefficients),]

	predictions_training <- logistic_func(splitted$X_training %*% final_betas)
	predictions_testing <- logistic_func(splitted$X_testing %*% final_betas)

	training_classes <- ifelse(predictions_training >= threshold, 1, 0)
	testing_classes <- ifelse(predictions_testing >= threshold, 1, 0)

	r_predictions <- predict(r_model, newdata = splitted$r_testing, type = "response")
	r_classes <- ifelse(r_predictions >= threshold, 1, 0)

	list(
		training_accuracy = mean(training_classes == splitted$y_training),
		training_auc = auc(roc(splitted$y_training, as.vector(training_classes), quiet = TRUE)),
		testing_accuracy = mean(testing_classes == splitted$y_testing),
		testing_auc = auc(roc(splitted$y_testing, as.vector(testing_classes), quiet = TRUE)),
		r_accuracy = mean(r_classes == splitted$y_testing),
		r_auc = auc(roc(splitted$y_testing, as.vector(r_classes), quiet = TRUE))
	)
}

display_metrics <- function(metrics) {
	neat_metrics <- data.frame(
		Training = c(metrics$training_accuracy, metrics$training_auc),
		Testing = c(metrics$testing_accuracy, metrics$testing_auc),
		R = c(metrics$r_accuracy, metrics$r_auc)
	)

	row.names(neat_metrics) <- c("accuracy", "AUC")

	neat_metrics
}
```

```{r}
metrics <- compare_predictions(splitted, log_model, r_model, threshold = 0.5)
display_metrics(metrics)
```

The area under the curve for all models are **EXPLAIN THIS PART BETTER LATER.**
