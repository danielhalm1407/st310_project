---
title: "Untitled"
output:
  html_document: default
  pdf_document: default
date: "2025-03-10"
---

```{r setup, include=FALSE}
library(dplyr)
library(caret)
library(glmnet)
library(ggplot2)
library(tidymodels)
library(ggdag)
library(e1071)
library(recipes)
```

We begin by loading our dataset, heart.csv, saving it as a dataframe called heart. We then set a seed for random processes, to ensure a level of reproducibility - this ensures that random operations are consistent in nature. 

The sex variable is then recoded for better interpretability, with "0" recoded to "Female" and "1" to "Male". This can be expressed as:

\[
\text{sex_factor} = 
\begin{cases} 
\text{Female} & \text{if } \text{sex} = 0, \\
\text{Male} & \text{if } \text{sex} = 1
\end{cases}
\]

The dataset is then split into training and testing sets using a 70/30 ratio, which partitions the data such that 70% of the observations are allocated to the training set and the remaining 30% are allocated to the testing set.

```{r}
# Dataset is loaded and saved as heart
heart <- read.csv("heart.csv")
head(heart)

# Set seed for reproduciability 
set.seed(200)

# Sex recoded for greater interpretability 
heart <- heart %>%
  mutate(sex_factor = recode(sex,
                             "0" = "Female",
                             "1" = "Male"))
heart
# Create a 70/30 split between training and testing data set
heart_split <- initial_split(heart, prop = 0.7)
train_data <- heart_split %>% training()
test_data <- heart_split %>% testing()

glm5 <- glm(factor(target) ~ oldpeak + trestbps + fbs, data = train_data, family = binomial)
summary(glm5)
```

Next, we want to identify the most influential quantitative and qualitative predictors.

For our quantitative predictors such as age, thalach and oldpeak, we seeked to calculate the correlation of the predictors with the target. The correlations are then rearranged from most to least influential - the most influential numeric variable was identified as oldpeak.

For our qualitative predictors such as sex_factor, cp and thal, we decided to look at a logistic regression model to assess the significance of our categorical predictors. The logit model is: 

\[
\log\left(\frac{p}{1-p}\right) = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \dots + \beta_n X_n
\]

such that \(p\) reflects the probability of the target being 1, \(\beta_0\) is the intercept value and \(\beta_1, \beta_2, \dots, \beta_n\) are the coefficients for their respective predictors \(X_1, X_2, \dots, X_n\)

The p-values derived from the model summary are then sorted from most significant to least significant - the most influential categorical variable was identified as cp. These p-values are indicative of statistical significance of each predictor.

```{r}
# Most influential quantitative variable determined via looking at correlations
heart_correlations <- train_data %>%
  rename(y = target) %>%  # Rename target to y
  select(age, thalach, ca, oldpeak, trestbps, chol, y) %>%  # Select columns including y
  cor() %>%
  as.data.frame() %>%
  select(y) %>%
  arrange(desc(abs(y)))

print(heart_correlations) # most highly correlated is oldpeak

# Most influential qualitative variable determined via looking at a logistic model
categorical_data <- train_data %>% 
  rename(y = target) %>%  # Rename target to y
  select(c(sex_factor, cp, fbs, restecg, exang, slope, thal), y)  # Select columns including y

glm_categoricals <- glm(y ~ ., data = categorical_data, family = binomial)

# Plot the coefficients
coef_plot <- summary(glm_categoricals)$coefficients

estimates <- coef_plot[, 1]

standarderror <- coef_plot[, 2] 

barplot <- barplot(coef_plot[, 1], main = "Coefficients of Logistic Model", las = 2, ylim = range(estimates + 2*standarderror, estimates - 2*standarderror), ylab = "Estimate")

arrows(barplot, estimates - 2*standarderror, barplot, estimates + 2*standarderror, angle = 90, code = 3, length = 0.05) # error bars for + or - 2SE

abline(h = 0, lty = 2)

# Most significant qualitative variable
sort(summary(glm_categoricals)$coefficients[, 4]) # most significant is cp
```

Thus, our baseline logistic model is built utilising oldpeak, the numerical variable with the highest correlation to the target, and cp, the most significant cateogrical predictor. Thus, our interpretable logit model is:

\[
\log\left(\frac{p}{1-p}\right) = \beta_0 + \beta_1 \cdot \text{cp} + \beta_2 \cdot \text{oldpeak}
\]

The summary function highlights that both predictors are highly significant when compared to our target.

```{r}
## Task 1: Interpretable model - taking the most highly correlated numerical predictor and the most significant categorical predictor
glm1 <- glm(target ~ cp + oldpeak, data = train_data, family = binomial)
summary(glm1)
```

We then work to identify the best pair of predictors which would minismise the missclassification rate using cross-validation.

We first recode the target variable to ensure smooth running of our logistic model. All predictors are listed and a logistic model is fitted using all these predictors to establish a baseline model. This sets the scene for comparison of predictor pairs to find the optimal pair.

```{r}
## Task 4: For loop to see which two predictors give the best cross-validation misclassification rate
train_data$target <- as.factor(train_data$target) # recoding target to be categorical

predictors <- c("age", "thalach", "ca", "oldpeak", "trestbps", "chol",
                "cp", "slope", "thal", "sex_factor", "exang", "restecg", "fbs") 

glm_full <- glm(target ~ ., data = train_data %>% select(all_of(predictors), target), family = "binomial") # logistic model fit on all predictors
```

Lasso regression is utilised to ensure that the most important predictors are selected. 

Lasso regression is a regression method that introduces an L1 penalty on the coefficients of the predictors. The L1 penalty ensures sparsity in the model by shrinking  coefficients of less important predictors to zero, performing feature selection.

The loss function for Lasso regression in the context of logit is:

\[
\ell_{\text{Lasso}}(\beta) = -\sum_{i=1}^{n} \left[ y_i \log(p(x_i)) + (1 - y_i) \log(1 - p(x_i)) \right] + \lambda \sum_{j=1}^{p} |\beta_j|
\]

such that \[ -\sum_{i=1}^{n} \left[ y_i \log(p(x_i)) + (1 - y_i) \log(1 - p(x_i)) \right] \] is the logistic loss for binary classification with \(p(x_i) = \frac{1}{1 + e^{-x_i \beta}}\) being the     logistic function, \(\lambda\) is the parameter controlling the strength of the L1 penalty and \(\sum_{j=1}^{p} |\beta_j|\) is the L1 penalty term which ensures sparsity in the coefficients

The optimal value of \(\lambda\) is determined using \(k\)-fold cross-validation, namely a 10-fold approach. The selected predictors are those with non-zero coefficients in the Lasso model:

\[
\text{selected_predictors} = \{X_i \mid \beta_i \neq 0\}
\]

The non-zero coefficients are subsequently extracted to help identify the selected predictors and validated to ensure existence within the data set. These remaining coefficients correspond to the predictors that Lasso deems most important. The selected predictors are validated to ensure they exist in the dataset, and all possible pairs of these predictors are generated for further analysis.

```{r}
# Use Lasso to select predictors
x <- scale(model.matrix(factor(target) ~ age + thalach + ca + oldpeak + trestbps + chol + cp + slope + thal + sex_factor + exang + restecg + fbs, data = train_data))[, -1]
y <- train_data$target
train <- sample (1: nrow(x), nrow(x) / 2)
test <- (-train)
y.test <- y[test]
grid <- 10^ seq (10, -4, length = 200)
lasso_model <- glmnet(x[train , ], y[train], alpha = 1, lambda = grid, family = "binomial")
lasso_model
plot(lasso_model)


lasso_model_cv <- cv.glmnet(x[train, ], y[train], alpha = 1, nfolds = 100)
plot(lasso_model_cv)

best_lambda <- lasso_model_cv$lambda.min
best_lambda

lasso_predicted <- predict(lasso_model , s = best_lambda , newx = x[test , ])
mean((lasso_predicted - y.test)^2)

out <- glmnet(x, y, alpha = 1, lambda = grid)

lasso.coef <- predict(out, type = "coefficients", s = best_lambda)

# Identify selected predictors
selected_predictors <- rownames(lasso.coef)[lasso.coef[, 1] != 0][-1] # checking if predictors exist in dataset
selected_predictors <- intersect(selected_predictors, colnames(train_data)) 
selected_predictors # predictors ranked before the for loop

# Generate all predictor pairs
predictor_pairs <- combn(selected_predictors, 2, simplify = FALSE) # all predictor combinations

# Initialize results data frame
results <- data.frame(Predictor1 = character(),
                      Predictor2 = character(),
                      MisclassificationRate = numeric()) # predictor combos stored

# Set up cross-validation control
cv_control <- trainControl(method = "cv", number = 10) # cross-validation set up
```

We deploy a for loop to go through all potential pairs of predictors. This helps us identify which pair has the lowest missclassification rate. We train the model for each pair using 10-fold cross-validation. 

We calculate the accuracy rate using the calculate_error function which calculates accuracy as follows:

\[
\text{Accuracy} = \frac{1}{n} \sum_{i=1}^{n} \mathbb{I}(\hat{y}_i = y_i)
\]

such that \(\hat{y}_i\) is the predicted class for the \(i\)-th observation, \(y_i\) is the true class for the \(i\)-th observation and \(\mathbb{I}(\cdot)\) is the indicator function

The results for each pair is stored within the results data frame.

The pairs of predictors are sorted from highest to lowest accuracy - the top-performing pair is ca and oldpeak

```{r}
# Initialize an empty data frame to store results
results <- data.frame(Predictor1 = character(),
                      Predictor2 = character(),
                      Accuracy = numeric(),
                      stringsAsFactors = FALSE)

for (pair in predictor_pairs) { # for loop to find 2 most important predictors
  pair <- unlist(pair)
  
  selected_data <- train_data %>% select(all_of(pair), target) # subset to ensure only looking at predictor pair and target
  
  # Validation set approach of logistic model
  model_fit <- glm(target ~ ., data = selected_data, family = binomial)
  probabilities <- predict(model_fit, type = "response")
  predictions <- ifelse(probabilities > 0.5, 1, 0)
  
  # Compute the accuracy using the calculate_error function
  calculate_error <- function(predictions, true_values, classify) {
    if (classify) {
      return(mean(predictions == true_values)) # Calculate accuracy directly
    } else {
      return(mean((predictions - true_values)^2))
    }
  }
  
  accuracy <- calculate_error(predictions, selected_data$target, classify = TRUE)
  
  # Append the results for this pair to the results data frame
  results <- rbind(results, 
                   data.frame(Predictor1 = pair[1], 
                              Predictor2 = pair[2], 
                              Accuracy = accuracy,
                              stringsAsFactors = FALSE))
}

# Sort the results by accuracy (descending order, since higher accuracy is better)
best_pairs <- results[order(-results$Accuracy), ]

# Display the top few rows of the sorted data frame
head(best_pairs)
```
