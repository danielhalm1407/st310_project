---
output: html_document
---

# ST 310 Group Project: Predicting Presence of Heart Disease in Patients

**Candidate Numbers:**



## Outline/Contents




## Introduction

### The Dataset
Where from, vars used etc etc

### What We Aim to Achieve
Motivations and objectives

## Set Up

### Packages
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
# General
library(dplyr) # for filtering and sorting to be easier.
library(rstudioapi) # to set the correct working directory



# Fitting Models
#library(tidyverse)
library(ISLR) # to get all the data and models from ISLR to experiment
library(gam) # to fit gams
library(tree) # for tree fitting to guage the functional form of the data
library(randomForest) # for random forest fitting
library(ranger) # for random forest fitting
library(MASS) # for QDA and LDA
library(e1071) # to fit a naive bayes GMC.

# Plotting/ Visualisation
library(ggplot2) # for maintainable plots with a lot of interactability
library(yardstick) # for plotting an ROC curve
library(iml) # To get PDPs from a random forest
library(GGally) # to make various two-way plots

# Easier Analysis
library(tidymodels) # Allows easy model fitting and multi-parameter tuning
library(purrr) # So that I can iterate over custom thresholds to assign class
library(rsample) # to allow parameter tuning using tidymodels

# Resolve package conflicts (basically avoid having to us dplyr::select all the time)
library(conflicted)

conflict_prefer("select", "dplyr")
conflicts_prefer(dplyr::filter)
conflicts_prefer(yardstick::accuracy)
conflicts_prefer(base::as.matrix)
conflicts_prefer(tree::tree)
conflicts_prefer(plyr::id)
conflicts_prefer(dplyr::rename)
conflicts_prefer(e1071::tune)
conflicts_prefer(dplyr::mutate)
conflicts_prefer(dplyr::count)
```

### Directories/Paths
#### Set the current directory to the working directory
```{r}
setPath <- dirname(getSourceEditorContext()$path)

setwd(setPath)
getwd()
```

#### Set the path for the data and predictions
```{r, echo=FALSE}
# Change data_path to where I saved the data
data_path <- paste0(getwd(),"/../data/")
predictions_path <- paste0(getwd(),"/../predictions/")

```

### Load Data
```{r}
Heart <- read.csv(paste0(data_path, "Heart.csv"))
# Rename the outcome column to y
Heart <- Heart %>% rename(y = target)

# Move the last column to the front so that the cross-validation works
Heart <- Heart[, c(ncol(Heart), 1:(ncol(Heart) - 1))]

# Change y to be factor so that  it is recognised as classification
Heart_non_fctr <- Heart
Heart$y <- factor(Heart$y)

# Get number of predictors/features
n_preds <- sum(names(Heart) != "y")

head(Heart)
names(Heart)
```
### Cleaning
Any cleaning we have to do
### Exploratory Data Analysis (EDA)
Any eda and domain-specific research

#### Look at joint distribution of predictors against each other and the outcome

```{r}
Heart[,1:5] |>
  ggpairs(progress = F)
```

```{r}
for (i in seq(2,n_preds,3)) {
  plot <- ggpairs(
    Heart[, c(i:(min(n_preds,i + 2)), 1)], 
    progress = FALSE
  ) + 
    theme_minimal()
  
  print(plot)
}
```

Note in the above that the distribution of most of the integer variables appear to have a discrete gaussian/bell-curve type distribution within each class (with the exception of trestbps (resting blood pressure)). 

Moreover, the distributions for both integer and factor variables can be distinct conditional on each class.Where the distributions are conditionally gaussian, this implies that both the mean and covariance for each class is different for each class, possibly motivating a QDA.


However, this is not precisely a normal (conditional on y), limiting the possible usefulness of QDA as a model fitting procedure.

Naive bayes also appears a dud, as we often observe high correlations between the features/predictors across both classes, and naturally this is likely to extend to high correlations within class as well. Therefore, we cannot make the necessary argument that the predictors are independent within a given class.

## Model Fitting

### Set Up

#### Look at the Class Balance in Our Data


```{r}
# We need the training set to be balanced, yet the validation set to have 68% balance.
outcome_counts_fold <- Heart |> count(y)
outcome_counts_fold$proportion <- outcome_counts_fold$n / sum(outcome_counts_fold$n)
const_yhat_success <- outcome_counts_fold$proportion[[2]]
outcome_counts_fold
```

Nice, it seems rather balanced!

```{r}
const_yhat_success
```


#### Set up separate training and validation sets

##### Define a function to create training and validation sets

The below function creates training and validation sets or 'folds' in dataframe and matrix form, as well as returning the indices of the observations corresponding to these.

Optionally, we can have the class balance in the validation set (called test_bal) be different to that in the training set. We set this to be the same in both training and validation sets, and equal to the positive class proportion in the original data, given that each set is sampled independently from the original data.


```{r}

make_sets <- function(df, frac = 0.6, classify = F, test_bal = const_yhat_success) {
  train_indices <- sample(1:nrow(df), size = frac * nrow(df), replace = F)
  
  y_train <- df[, 1]
  X_train <- as.matrix(df[, -1])
  
  if (frac>0) {
    valid_indices <- -train_indices
  }
  else {
    valid_indices = rownames(df)
  }
  train_fold <- df[train_indices,]
  valid_fold <- df[valid_indices,]
  
  const_yhat_success <- NA
  outcome_counts_pre <- NA
  outcome_counts_valid <- NA
  if (classify==T) {
    outcome_counts_pre <- valid_fold |> count(y)
    outcome_counts_pre$proportion <- outcome_counts_pre$n / sum(outcome_counts_pre$n)
    rare_class_size <- round(((1-test_bal) * outcome_counts_pre[outcome_counts_pre$y==1,"n"] * (1/test_bal)),0)
    rare_class_indices <- which(valid_fold$y==0)
    common_class_indices <- which(valid_fold$y==1)
    sampled_indices <- sample(rare_class_indices, size = rare_class_size, replace = FALSE)
    valid_indices <- c(sampled_indices,common_class_indices)
    valid_fold <- valid_fold[valid_indices,]
    outcome_counts_valid <- valid_fold |> count(y)
    outcome_counts_valid$proportion <- outcome_counts_valid$n / sum(outcome_counts_valid$n)
    const_yhat_success <- outcome_counts_valid$proportion[[2]]
  }
  #train_fold <- train_fold[order(rownames(train_fold)),]
  #valid_fold <- valid_fold[order(rownames(valid_fold)),]
  
  y_train_fold <- train_fold[, 1]
  X_train_fold <- as.matrix(train_fold[, -1])
  
  y_valid_fold <- valid_fold[, 1]
  X_valid_fold <- as.matrix(valid_fold[, -1])
  
  return(list(y_train = y_train, X_train=X_train, train_fold = train_fold, valid_fold = valid_fold, y_train_fold = y_train_fold, X_train_fold = X_train_fold, y_valid_fold = y_valid_fold, X_valid_fold=X_valid_fold, const_yhat_success= const_yhat_success, outcome_counts_pre = outcome_counts_pre, outcome_counts_valid = outcome_counts_valid, train_indices=train_indices, valid_indices = valid_indices))
}
```

##### Call the function that creates training and validation sets

```{r}


# Make training and validation sets for the whole data
make_sets_output_1 <- make_sets(df=Heart )
Heart_train_fold <- make_sets_output_1$train_fold
Heart_valid_fold <- make_sets_output_1$valid_fold

# Get matrix versions of the full data
X <- make_sets_output_1$X_train
Y <- make_sets_output_1$y_train

# Make matrix versions of each of these
Heart_X_train_fold <- make_sets_output_1$X_train_fold
Heart_X_valid_fold <- make_sets_output_1$X_valid_fold

Heart_y_train_fold <- make_sets_output_1$y_train_fold
Heart_y_valid_fold <- make_sets_output_1$y_valid_fold

# Extract the indices used for training and validation for future use
train_indices <- make_sets_output_1$train_indices
valid_indices <- make_sets_output_1$valid_indices
```

##### Alternately, Use the Tidyverse package
```{r}
Heart_split <- initial_split(Heart, prop = 0.7)
Heart_train <- Heart_split %>% training()
Heart_valid <- Heart_split %>% testing()

```
**Inspect the training data**

```{r, eval = FALSE}
Heart_train
```
**Inspect the validation data**
```{r, eval= F}
Heart_valid
```


#### Create k folds for cross-validation 

If we want to use the same folds for cross validation for all of the model fitting procedures and all specifications for all model fitting procedures.


```{r}


k <- 5
folds <- sample(rep(1:k, length.out = nrow(Heart)))

train_input <- list(train_df = Heart, X_train = X, y_train = Y, train_indices=train_indices, valid_indices = valid_indices, folds=folds)

```



#### Create A Dataframe to Store All of Our Model Types and the 'Best' Specifications for Each.

For each model fitting procedure, we create a grid of tuning parameters and model inputs (including a choice of predictors and functional forms where applicable).

We then iteratively fit all of these specifications and calculate the validation/cross-validation set accuracy, choosing the 'Best' model usually as that with the highest accuracy, or the simplest specification with a high enough accuracy.

In our classification setting, this is the misclassification rate.

We then store this specification (all the inputs we need to run this fit) and its accuracy in this aggregated table, repeating this for all model fitting procedures.

In other words, each row corresponds to a model fitting procedure (e.g. GAM, tree, OLS) with each column giving some parameter or information about the specification that achieved the best accuracy given that model procedure.

##### Create a function to make this dataframe

This allows us to create the table above to store any number of tuning parameters and use any form of accuracy measure, in this case, we use the misclassification (misclass) rate,
and 3 tuning parameters.

In this table, sub-models and functional forms and combinations of predictors are also counted as tuning parameters, just for maintainability, although this is not strictly true.


```{r}
gen_models_df <- function(len=10, accuracy_measures = c("min_mse")) {
  df = data.frame(
    model_type = character(len),
    model_name = character(len)
  )
  for (measure in accuracy_measures) {
    df[[measure]] <- numeric(len)
  }
  #tp stands for tuning parameter
  df$tp1_name = character(len)
  df$tp2_name = character(len)
  df$tp3_name = character(len)
  df$tp1_value = numeric(len)
  df$tp2_value = numeric(len)
  df$tp3_value = numeric(len)

  return(df)
}
Heart_models <- gen_models_df(accuracy_measures = c("misclass_rate"))

Heart_models

```


#### Define a set of functions that help to validate or cross-validate a grid of tuning parameters


##### Define a Function that Generates Predictions

Given any model fitting procedure (e.g. tree), a model fitted to that procedure (e.g. tree.fit fit to some training data), and any validation set, this function calculates the predicted outcome.

Calculating predictions varies not only on:

1. Whether we are using classification (in which case we get the conditional probabilities first, and then assign a class based on our thresholds), but also; 

2. what model fitting procedure we are using, since the packages in R use different syntax. For example, some require an X matrix input only (without the outcome), and others have the conditional probabilities in different formats.

```{r}
get_predictions <- function(model, X_valid_fold, y_valid_fold, classify, formula, model_f, matrix, thresh) {
  
  # Handle the matrix vs non-matrix input
  if (matrix) {
    valid_fold <- X_valid_fold
  } else {
    valid_fold <- data.frame(y = y_valid_fold, X_valid_fold)
  }
  
  # Classification predictions
  if (classify) {
    if (model_f == "tree") {
      # For tree models (e.g., decision tree), with 2 indexing the prob of positive class
      prob_predictions <- predict(model, valid_fold)[, 2]
    }
    else if (model_f == "randomForest") {
      # For random forest models (e.g., decision tree), assuming it's a binary classification
      prob_predictions <- predict(model, valid_fold, type = "prob")[,2]
    }
    else if (model_f == "naiveBayes") {
      # For Naive Bayes, type="raw" returns probabilities
      prob_predictions <- predict(model, newdata = valid_fold, type = "raw")[, 2]  # Checking column 2 for second class
    }
    else if (model_f == "qda" | model_f == "lda") {
      # For QDA and LDA, we extract posterior probabilities for the second class (usually "1" or "2")
      prob_predictions <- predict(model, newdata = valid_fold, type = "response")$posterior[, 2]  # Assuming the second column is for class "1"
    } 
    else {
      # Default case for other models (e.g., logistic regression)
      prob_predictions <- predict(model, valid_fold, type = "response")
    }
    
    # Apply threshold and convert to factor (binary classification)
    predictions <- as.factor(as.numeric(prob_predictions > thresh))
  } 
  else {
    # If regression, just return predicted values
    predictions <- predict(model, valid_fold)
  }
  
  return(predictions)
}

```

##### Define a Function that Finds Prediction Accuracy

This takes predictions as inputs and calculates the validation set mse or misclass rate accordingly.

```{r}

calculate_error <- function(predictions, true_values, classify) {
  if (classify) {
    return(mean(predictions != (as.numeric(true_values) - 1)))
  } else {
    return(mean((predictions - true_values)^2))
  }
}

```


##### Define a Function that Cross-Validates a Specification

This function takes a model fitting procedure and specification as an input, and then fits the model k times on k folds, using the previously defined functions to generate predictions and prediction accuracy for each fold.

In turn, it stores the accuracy of these folds and calculates the mean of this accuracy across all k folds and stores it as its main output, corresponding to the k-validation-fold accuracy.

```{r}

cross_validate <- function(thresh, model_f, X, Y, k, classify, train_df, folds, model_input, matrix, xname, yname) {
  e_folds <- c()  # Store error metrics for each fold
  fits <- list()   # Store model fits for each fold
  preds <- list()  # Store predictions for each fold
  valids <- list() # Store true y-values for each fold
  
  for (fold in 1:k) {
    # Train/validation split for the fold
    X_train_fold <- X[folds != fold, ]
    y_train_fold <- Y[folds != fold]
    X_valid_fold <- X[folds == fold, ]
    y_valid_fold <- Y[folds == fold]
    
    # Update model_input for this fold
    if (!matrix) {
      model_input$data <- train_df[folds != fold, ]
    } else {
      model_input[[xname]] <- X_train_fold
      model_input[[yname]] <- y_train_fold
    }

    
    # Train model
    model <- do.call(model_f, model_input)
    
    # Get validation predictions
    predictions <- get_predictions(model, X_valid_fold, y_valid_fold, classify, model_input$formula, model_f, matrix, thresh)
    
    # Compute error for this fold
    fold_error <- calculate_error(predictions, y_valid_fold, classify)

    
    # Store results
    fits[[fold]] <- model
    preds[[fold]] <- predictions
    valids[[fold]] <- y_valid_fold
    e_folds <- c(e_folds, fold_error)
  }
  
  # Compute overall CV error
  avg_error <- mean(e_folds)
  
  # Return models, predictions, validation targets, and error
  return(list(fits = fits, preds = preds, valids = valids, error = avg_error))
}


```


##### Define a Function that Cross-Validates a Specification

This function takes a model fitting procedure and specification as an input, and then fits the model once on a single validation set, using the previously defined functions to generate predictions and prediction accuracy on this set.

In turn, it stores this accuracy as its main output, corresponding to the validation set accuracy.

```{r}

validate <- function(thresh, model_f, X, Y, train_indices, valid_indices, classify, train_df, model_input, matrix, xname, yname) {
  
  # Update model_input for this fold
  if (!matrix) {
    model_input$data <- train_df[train_indices,]
  } else {
      model_input[[xname]] <- X[train_indices, ]
      model_input[[yname]] <- y[train_indices]
  }
  
  # Train model using pre-set model_input
  model <- do.call(model_f, model_input)
  # Get validation predictions
  predictions <- get_predictions(model, X[valid_indices, ], Y[valid_indices], classify, model_input$formula, model_f, matrix, thresh)
  
  # Compute validation error
  fold_error <- calculate_error(predictions, Y[valid_indices], classify)

  
  # Return the model, predictions, validation targets, and error
  return(list(fits = model, preds = predictions, valids = Y[valid_indices], error = fold_error))
}

```

##### Define a Function that Validates or Cross Validates a Grid of Specifications

This function takes a model fitting procedure and a grid where each row is a specification as an input, and then finds the validation set or k-fold-validation accuracy, using the previous functions to fit the model, calculate predictions and in turn their accuracy.

In turn, it stores this accuracy for each specification under a new column of this input grid as its main output.

From the model output is also possible to retrieve the model fit, the predictions and the validation sets for each time a model (model fitting procedure and specification/tuning parameters combination) is fit, in case we want to go back to our results.

Depending on interpretability and time, one can choose whether to validate or cross-validate.

It is possible to input the validation set or validation folds from the start, or reshuffle (create these again) each time the function is called.



```{r}



grid_validate <- function(train_input_list = train_input, method = "cv", model_f = "tree", grid_df, k = 5, frac = 0.6, matrix = FALSE, classify = FALSE, test_bal = const_yhat_success, reshuffle = F) {
  train_df <- train_input_list$train_df
  if (reshuffle) {
    sets_output <- make_sets(df = train_df, frac = frac, classify = classify, test_bal = test_bal)
    X <- sets_output$X_train
    Y <- sets_output$y_train
    train_indices <- sets_output$train_indices
    valid_indices <- sets_output$valid_indices
    folds <- sample(rep(1:k, length.out = nrow(train_df))) # makes sure that the same folds are used, just like we make sure that the same validation set is used.
  }
  else {
    X <- train_input_list$X_train
    Y <- train_input_list$y_train
    train_indices <- train_input_list$train_indices
    valid_indices <- train_input_list$valid_indices
    folds <- train_input_list$folds
  }
  # Initialize common variables
  fits <- list()
  preds <- list()
  valids <- list()
  
  # Iterate over each row in grid_df
  for (i in 1:nrow(grid_df)) {
    # Set up model_input
    model_input <- list()
    xname <- ifelse(model_f == "gbart", "x.train", "x")
    yname <- ifelse(model_f == "gbart", "y.train", "y")
    
    if (!matrix) {
      model_input$formula <- as.formula("y ~ .")
    } 
    
    # Apply grid parameters to model_input
    current_model_f <- model_f  # Default model function
    for (tp in names(grid_df)) {
      if (!all(is.na(grid_df[[tp]]))) {
        if (tp == "mindev") {
          model_input$control <- tree.control(nobs = length(train_df$y), mindev = grid_df[i, tp])
        } else if (tp == "preds_used") {
          model_input$formula <- as.formula(paste("y ~", grid_df[i, tp]))
        } else if (tp == "sub_model") {
          current_model_f <- grid_df[i, tp]  # Override model function if 'submodel' is specified
        } else if (tp != "thresh") {
          model_input[[tp]] <- grid_df[i, tp]
        }
      }
    }
    # Get the threshold param if we have it specified and we are classifying. We don't pass this directly into the model fitting function, hence why it is not in the loop above
    thresh <- if ("thresh" %in% names(grid_df) && !is.na(grid_df[i, "thresh"])) grid_df[i, "thresh"] else 0.5
    
    # Call validation function and store results
    if (method == "cv") {
      result <- cross_validate(thresh, current_model_f, X, Y, k, classify, train_df, folds, model_input, matrix, xname, yname)
      error_col <- paste0("cv_", ifelse(classify, "misclass_rate", "mse"))
    } else if (method == "v") {
      result <- validate(thresh, current_model_f, X, Y, train_indices, valid_indices, classify, train_df, model_input, matrix, xname, yname)
      error_col <- paste0("v_", ifelse(classify, "misclass_rate", "mse"))
    } else {
      stop("Invalid method. Choose 'cv' or 'v'.")
    }
    
    # Store results in grid_df and lists
    grid_df[[error_col]][i] <- result$error
    fits[[i]] <- result$fits
    preds[[i]] <- result$preds
    valids[[i]] <- result$valids
  }
  
  return(list(grid_df = grid_df, fits = fits, preds = preds, valids = valids))
}


```

#### Create a Function that can plot how our validation or cross-validation error measures compare based on Combinations of tuning parameters.

This takes a specification grid output from grid_validate() above as an input, plotting how validation accuracy/error changes based on up to two dimensions.

```{r}
plot_grid <- function(grid, val_measure = "v_mse", tp1 = "n_preds_used", tp2 = NA, logx = FALSE) {
  best_model <- grid[which.min(grid[[val_measure]]), ]
  
  plot <- grid |>
    ggplot(aes(x = .data[[tp1]])) +
    geom_point(aes(y = .data[[val_measure]], color = if (!is.na(tp2)) as.factor(.data[[tp2]]) else NULL), size = 2, alpha = 0.5) +
    geom_point(data = best_model, aes(x = .data[[tp1]], y = .data[[val_measure]]), 
               color = "purple", shape = 16, size = 3) +
    labs(
      title = paste(val_measure, "vs.", tp1),
      x = tp1,
      y = val_measure
    ) +
    expand_limits(y = 0.9 * min(grid[[val_measure]])) +
    theme_minimal()
  
  if (!is.na(tp2)) {
    plot <- plot + scale_color_discrete(name = tp2)
  }
  if (logx) {
    plot <- plot + scale_x_log10(breaks = scales::trans_breaks("log10", function(x) 10^x))
  } 
  print(plot)
}
```


#### Create a function that retrieves the best specification 

In other words, this retreives the corresponding sub-model, regressors, functional forms and tuning parameters that give us the best validation set accuracy from a grid (like that output from grid_validate()) where each row is some different specification.

```{r}
get_best_spec <- function(grid_df, grid_validation) {
  e_column <- grep("misclass|mse", colnames(tree_df), value = TRUE)
  best_idx <- which.min(grid_df[[e_column]])
  best_e <- grid_df[best_idx, e_column]
  best_row <- grid_df[best_idx,]
  best_preds <- grid_validation$preds[[best_idx]]
  best_valids <- grid_validation$valids[[best_idx]]
  best_fits <- grid_validation$fits[[best_idx]]
  return(list(preds=best_preds, valids=best_valids, fits = best_fits, error = best_e, row =best_row))
}


```

#### Create a function that visually compares predicted values to validation values

This is set up to work both for continuous outcomes (preds against valids scatter plot) and a categorical outcome (confusion matrix)

```{r}
graph_preds <- function(preds, valids, cm=T, scatter=F, classify=T) {
  predictions_df <- data.frame(y = valids, yhat=preds)
  if (cm) {
    confusion_matrix <-
    predictions_df |> 
    conf_mat(truth = y, estimate = yhat)
    print(confusion_matrix |> autoplot(type = "heatmap"))
  }
  if (scatter == T) {
    if (classify) {
      predictions_df$yhat <- as.numeric(predictions_df$yhat) - 1
      predictions_df$y <- as.numeric(predictions_df$y) - 1
    }
    print(plot(predictions_df$yhat, predictions_df$y))
    abline(0, 1)
  }
  error_col <- paste0(ifelse(classify, "misclass_rate", "mse"))
  print(paste(error_col, ":", calculate_error(preds, valids, classify)))
}



```



### Baseline Model: Logistic (Few Predictors)
Explain why we used this

#### Validate/Cross-Validate Best Tuning Parameters
Run some code
#### Analyse Results
Look at results from best Model



### Gradient-Descent-Based Model: Logistic (Few Predictors)
Take a decreasing size step, and use a loss function without a constant
Explain why we used this
#### Explain how this works (Stickiest Part of Project)

#### Validate/Cross-Validate Best Tuning Parameters
Run some code
#### Analyse Results
Look at results from best Model


### Relatively Interpretable Model: Logistic (Multiple Predictors and Interactions)

#### Validate/Cross-Validate Best Tuning Parameters
Run some code
#### Analyse Results
Look at results from best Model



### High-Dimensional Model: Regularised Logistic (Few Predictors)

#### Validate/Cross-Validate Best Tuning Parameters
Run some code
#### Analyse Results
Look at results from best Model



### Predictively Accurate Model: Classification Random Forest?




### Classification Tree

#### Validate/Cross-Validate Best Tuning Parameters

##### Create a Grid of Tuning Parameters
```{r}
mindev_values <- c(5,4,3,2,1)*rep(10^seq(-2, -4, by = -1), each=5) # i've done too much matrix stuff in my finance module so this dodgy syntax is now all too natural.

tree_grid <- expand.grid(mindev = mindev_values, thresh = seq(0.4,0.6,0.02))
#grid_results$tree_size <- NA
#tree_grid <- data.frame(dummy = rep(NA,4))
tree_grid
```

##### Compare Prediction accuracy for each of the models
```{r} 
tree_df_output <- grid_validate(grid_df = tree_grid,  method = "v", classify=T)
tree_df <- tree_df_output$grid_df
```
**Look at the Grid**
```{r}
tree_df
```

**Graph the Grid**

```{r}
plot_grid(tree_df, val_measure = "v_misclass_rate", tp1 = "mindev", tp2 = "thresh")
```


#### Analyse the best specification's predictions against the validation set



```{r}
best_spec_output <- get_best_spec(grid_df = tree_df, grid_validation = tree_df_output)
graph_preds(best_spec_output$preds, best_spec_output$valids, cm=T)
```
#### Validate/Cross-Validate Best Tuning Parameters Using In-Built Features

```{r}
# Fit a tree
tree_fit <- tree(y ~ ., Heart_train_fold)
# Fit a glm
heart_2 <- glm(y~ ., Heart_non_fctr, family = "gaussian")
# Store the Probability predictions (the second row corresponds to the postive class probability)
tree_fit_prob_preds <- predict(tree_fit, Heart_valid_fold)[,2]

#GHet the standard bayesian classifier class predictions
preds <- predict(tree_fit, Heart_valid_fold, type = "class")
plot(preds, as.numeric(Heart_valid_fold$y)-1)

```

##### Cross-Validate the Best Pruning Coefficient, K (Penalty for number of terminal nodes) and overall tree size.
```{r}
cv.tree_fit <- cv.tree(tree_fit, FUN = prune.misclass)
par(mfrow = c(1, 2))
plot(cv.tree_fit$size, cv.tree_fit$dev, type = "b")
plot(cv.tree_fit$k, cv.tree_fit$dev, type = "b")
```
#### Analyse the Accuracy


##### Compare confusion matrices for the tree model.
```{r}

tree_fit_predictions <-data.frame(
  y = Heart_valid_fold$y,
  .fitted = tree_fit_prob_preds
) |> 
  mutate(yhat = factor(as.numeric(.fitted > const_yhat_success))) |>
  mutate(misclass = (y!=yhat))
paste("Class Balance:")
tree_fit_predictions |> pull(.fitted) |> mean() # same as mean(tree_fit_predictions$.fitted)
paste("Misclass Rate:")
sum((tree_fit_predictions$y!=tree_fit_predictions$yhat)) / nrow(tree_fit_predictions)
```
##### Look at ROC Curve to guage Trade-Off
```{r, eval=F}
tree_fit_predictions |>
  roc_curve(truth = y, .fitted,
            event_level = "second") |>
  autoplot()
# the 'truth' is just the training y here, not the testing y, which we do not have, nor a validation set.
```
#### Classify at different thresholds.

```{r, eval=F}
higher_cutoff <- const_yhat_success + .15
confusion_matrix_higher <-
  tree_fit_predictions |>
  mutate(yhat = factor(as.numeric(.fitted > higher_cutoff))) |> # this line predicts y = 1 ONLY if the predicted conditional probability is above the higher cutoff
  conf_mat(truth = y, estimate = yhat)
```
```{r, eval=F}
confusion_matrix_medium <-
  tree_fit_predictions |>
  mutate(yhat = factor(as.numeric(.fitted > const_yhat_success))) |> 
  conf_mat(truth = y, estimate = yhat)
```


```{r, eval=F}
lower_cutoff <- const_yhat_success - 0.15
confusion_matrix_lower <-
  tree_fit_predictions |>
  mutate(yhat = factor(as.numeric(.fitted > lower_cutoff))) |>
  conf_mat(truth = y, estimate = yhat)
```

##### 1. Balanced Confusion Matrix
```{r, eval=F}
confusion_matrix_medium |> autoplot(type = "heatmap")
```
##### 3. Low Cut-Off Confusion Matrix
```{r, eval=F}
confusion_matrix_lower |> autoplot(type = "heatmap")
```
##### 3. High Cut-Off Confusion Matrix
```{r, eval=F}
confusion_matrix_higher |> autoplot(type = "heatmap")
```
##### Comparing key sum stats:

```{r, eval=F}
higher_summary <- summary(confusion_matrix_higher) |>
  mutate(higher = .estimate) |>
  dplyr::select(.metric, higher)
medium_summary <- summary(confusion_matrix_medium) |>
  mutate(medium = .estimate) |>
  dplyr::select(medium)
lower_summary <- summary(confusion_matrix_lower) |>
  mutate(lower = .estimate) |>
  dplyr::select(lower)
cbind(higher_summary, medium_summary, lower_summary) |>
  knitr::kable()


```

As seen from the confusion matrices earlier, the threshold does not really seem to matter.

### Generative Models for Classification (GMCs: LDA, QDA and Naive Bayes)**


##### Create a Grid of Tuning Parameters

```{r}

gmc_grid <- expand.grid(sub_model = c("lda","qda", "naiveBayes"), thresh = seq(0.4,0.6,0.02))
gmc_grid$sub_model <- as.character(gmc_grid$sub_model)
gmc_grid
```

##### Compare Prediction accuracy for each of the models

**Look at the Grid**
```{r}
gmc_output <- grid_validate(grid_df = gmc_grid,  method = "v", classify=T)
gmc_df <- gmc_output$grid_df
gmc_df
```
**Graph the Grid**

```{r}
plot_grid(gmc_df, val_measure = "v_misclass_rate", tp1 = "thresh", tp2 = "sub_model")
```




#### Analyse Results

```{r}
best_spec_output <- get_best_spec(grid_df = gmc_df, grid_validation = gmc_output)
graph_preds(best_spec_output$preds, best_spec_output$valids, cm=T)
```

### Random Forest

#### Validate/Cross-Validate Best Tuning Parameters Using In-Built Features

```{r}


thresh = 0.51  # Classification threshold

# Fit a Random Forest model
rf_fit <- randomForest(y ~ ., data = Heart_train_fold, ntree = 500, mtry = 3, importance = TRUE)

# Store the probability predictions (the second column corresponds to the positive class probability)
rf_fit_prob_preds <- predict(rf_fit, Heart_valid_fold, type = "prob")[,2]

# Convert probabilities to class predictions using the threshold
preds <- as.factor(as.numeric(rf_fit_prob_preds > thresh))
valids <- Heart_valid_fold$y

# Get the standard Bayesian classifier class predictions
graph_preds(preds, valids, cm = TRUE)




```
#### Validate/Cross-Validate Best Tuning Parameters

##### Create a Grid of Tuning Parameters
```{r}
#mtry_choice <- c(seq(1,ncol(Heart)-1,1))
mtry_choice = c(1,2,3,4,5)
grid_rf <- expand.grid(ntree = c(500), mtry = mtry_choice, thresh = seq(0.4,0.6,0.05), importance=F)
grid_rf
```

##### Compare Prediction accuracy for each of the models
```{r} 
rf_df_output <- grid_validate(grid_df = grid_rf,  model_f = "randomForest", method = "v", classify=T)
rf_df <- rf_df_output$grid_df
```
**Look at the Grid**
```{r}
rf_df
```

**Graph the Grid**

```{r}
plot_grid(rf_df, val_measure = "v_misclass_rate", tp1 = "mtry", tp2 = "thresh")
```


#### Analyse the best specification's predictions against the validation set



```{r}
best_spec_output <- get_best_spec(grid_df = rf_df, grid_validation = rf_df_output)
graph_preds(best_spec_output$preds, best_spec_output$valids, cm=T)
```
#### Use Best Model To Learn About Data

##### Find Variable Importance
```{r}
best_spec_output$row

rf_fit <- randomForest(y ~ ., data = Heart_train_fold, ntree = 500, mtry = best_spec_output$row$mtry, importance = TRUE)


varImpPlot(rf_fit)
varImpPlot(best_spec_output$fits)
```
```{r}

```


##### Guage Functional Form of Conditional Probability using PDPs
```{r}
pred_rf <- Predictor$new(best_spec_output$fits)

pdp_rf <- FeatureEffects$new(pred_rf, features = c("cp","ca", "chol", "trestbps"), method = "pdp+ice")

plot(pdp_rf) 
```

```{r}
Heart
```

#### Tidymodels Exploration

```{r}
ranger_fit <- rand_forest(trees = 100, mode = "classification") %>%
  set_engine("ranger") %>%
  fit(y ~ ., data = Heart_train)
rf_fit <- rand_forest(trees = 100, mode = "classification") %>%
  set_engine("randomForest") %>%
  fit(y ~ ., data = Heart_train)
```
**Check ranger accuracy**
```{r}
ranger_fit %>%
  predict(Heart_valid) %>%
  bind_cols(Heart_valid) %>%
  metrics(truth = y, estimate = .pred_class)
```
**Check randomForest accuracy**

```{r}
rf_fit %>%
  predict(Heart_valid) %>%
  bind_cols(Heart_valid) %>%
  metrics(truth = y, estimate = .pred_class)
```

##### Compare choice probabilities

```{r}
Heart_probs <- ranger_fit %>%
  predict(Heart_valid, type = "prob") %>% 
  bind_cols(Heart_valid)
Heart_probs
```
```{r}
Heart_probs%>%
  gain_curve(y, .pred_0) %>%
  autoplot()
```

```{r}
Heart_probs%>%
  roc_curve(y, .pred_0) %>%
  autoplot()
```

#### All in One Go
```{r}

predict(ranger_fit, Heart_valid, type = "prob") %>% # add the predicted probs
  bind_cols(predict(ranger_fit, Heart_valid)) %>% # add the predicted class
  bind_cols(select(Heart_valid, y)) %>% # add the class in the validation set
  metrics(y, .pred_1, estimate = .pred_class) # calculate accuracy (correct classification rate of the +ve class)

```

#### Validate/Cross-Validate Best Tuning Parameters using Tidymodels

##### Cross Validate


```{r}
# Set up the folds for 5 fold Cross-validation
cv_folds <- vfold_cv(Heart_valid, v = 5, strata = y)

# Define a tunable model (mtry is tuned, trees is fixed at 500)
ranger_spec <- rand_forest(mtry = tune(), trees = tune(), mode = "classification") %>%
  set_engine("ranger", probability = TRUE)

# Create a workflow
ranger_wf <- workflow() %>%
  add_model(ranger_spec) %>%
  add_formula(y ~ .)

# Create a tuning grid
tuning_grid <- expand.grid(
  mtry = 1:6,
  trees = c(500)
)

# Run grid search with predictions saved
tune_results <- tune_grid(
  ranger_wf,
  resamples = cv_folds,
  grid = tuning_grid,
  metrics = metric_set(accuracy, roc_auc),
  control = control_grid(save_pred = TRUE)  # <-- Save predictions!
)


```
```{r}
# Extract probability predictions
predictions <- collect_predictions(tune_results)

# View first few rows
predictions
```






##### Compare Prediction accuracy for each of the models
```{r}

# Get grid of accuracy depending on choice of mtry
param_grid <- tune_results %>%
  collect_metrics() %>%  # Extract all metrics
  filter(.metric == "accuracy") %>%  # Focus only on accuracy
  mutate(cv_misclass_rate = 1 - mean) %>% # Create new column for misclass rate
  mutate(ntree = 500)


```
**Look at the Grid**
```{r}
param_grid
```
**Look at the Plot**
```{r}
plot_grid(param_grid, val_measure = "cv_misclass_rate", tp1 = "mtry", tp2 = "trees")
```



##### Use Validation Set Approach to get best tuning Parameters

```{r}
predictors <- names(Heart)[-1]
# Set up a set of formulae (subsets of predictors) that we can use
selected_predictors = c(paste(as.character(predictors[1]), collapse = " + "),
                                           paste(as.character(predictors[1:4]), collapse = " + "),
                                           paste(as.character(predictors[1:8]), collapse = " + "),
                                           paste(as.character(predictors[1:12]), collapse = " + "))
formulas <- paste("y ~", selected_predictors)




# Set up a new tuning grid with a thresh value that we can change
tuning_grid2 <- expand.grid(
  mtry = 1:6,
  trees = c(500),
  thresh = c(0.4, 0.5),
  formula = as.character(formulas[1])
)
tuning_grid2$formula = as.character(tuning_grid2$formula)
tuning_grid2
#as.formula(tuning_grid2$formula[2])
```

##### Define a function that calculatevalidation set accuracy for a grid of tuning parameters.

Uses tidymodels syntax efficiency

Uses a helper function to get the model specification (including the fitting procedure and the engine) into tidyverse's desired format first.

```{r}
# Function to get model specification dynamically
get_model_spec <- function(model_fitting_procedure, engine, tuning_params) {
  if (model_fitting_procedure == "tree") {
    model_spec_updated <- decision_tree(mode = "classification") %>%
      set_engine(engine)  # Default engine: "tree"
    
  } else if (model_fitting_procedure == "random_forest") {
    model_spec_updated <- rand_forest(mtry = tuning_params$mtry, 
                                      trees = tuning_params$trees, 
                                      mode = "classification") %>%
      set_engine(engine, probability = TRUE)  # Default engine: "randomForest"
    
  } else if (model_fitting_procedure == "boosting") {
    model_spec_updated <- boost_tree(mode = "classification") %>%
      set_engine(engine)  # Default engine: "gbm"
    
  } else {
    stop("Invalid model fitting procedure. Choose from 'tree', 'random_forest', or 'boosting'.")
  }
  
  return(model_spec_updated)
}
```

```{r}



grid_validate_tidy <- function(train_data, valid_data, tuning_grid, model_fitting_procedure, engine) {
  # Initialize empty data frames to store results and predictions
  accuracy_df <- tuning_grid
  all_preds_df <- data.frame()
  
  # Iterate over each combination of hyperparameters in the tuning grid
  for(i in 1:nrow(tuning_grid)) {
    # Extract current tuning parameters
    tuning_params <- tuning_grid[i, ]
    
    # Get the model specification dynamically
    model_spec_updated <- get_model_spec(model_fitting_procedure, engine, tuning_params)
    
    # Create a workflow
    current_wf <- workflow() %>%
      add_model(model_spec_updated) %>%
      add_formula(as.formula(tuning_params$formula))
    
    # Fit the model on the training data
    model_fit <- fit(current_wf, data = train_data)
    
    # Predict probabilities on the validation set
    prob_predictions <- predict(model_fit, valid_data, type = "prob")$.pred_1
    
    # Apply threshold to classify predictions
    predictions <- as.factor(as.numeric(prob_predictions > tuning_params$thresh))
    
    # Calculate misclassification rate
    error <- mean(predictions != valid_data$y)
    
    # Store results
    accuracy_df$misclass_rate[i] = error
                                  
    
    # Store predictions
    preds_df <- data.frame(preds = predictions) %>%
      bind_cols(valid_data %>% select(y)) %>%
      mutate(spec_no = i,
             mtry = tuning_params$mtry, 
             trees = tuning_params$trees)
    
    all_preds_df <- rbind(all_preds_df, preds_df)
  }
  
  # Return both results and predictions
  return(list(
    accuracy_df = accuracy_df,
    preds = all_preds_df
  ))
}

rf_output <- grid_validate_tidy(Heart_train, Heart_valid, tuning_grid2, "random_forest", "randomForest")

```
##### Compare the Prediction Accuracy of Multiple Models

**Look at the Grid**
```{r}
rf_output$accuracy_df
```

**Look at the Plot**
```{r}
plot_grid(rf_output$accuracy_df, val_measure = "misclass_rate", tp1 = "mtry", tp2 = "thresh")
```
```{r}
# Check the predictions of the model with the lowest misclass rate
check_preds <- rf_output$preds %>%
  filter(spec_no == which.min(rf_output$results$misclass_rate))
check_preds
```

```{r}
graph_preds(check_preds$preds, check_preds$y, cm=T)
```

#### Create a function that conducts one forward or one backward stepwise iteration
```{r}
fwd_bckwd_step <- function(train_data, valid_data, current_predictors_formula, tuning_params, model_fitting_procedure, engine, direction = "fwd") {
  

  
  # Get all predictor names except the target variable "y"
  predictors <- setdiff(names(train_data), "y")
  
  # Split current_predictors string into individual variable names, if any
  if (current_predictors_formula == "") {
    current_predictors = " "
  }
  else {
    current_predictors <- unlist(strsplit(current_predictors_formula, " \\+ "))
  }
  
  # Find remaining predictors not in the current predictor set
  predictors_left <- setdiff(predictors, current_predictors)
  
  # Track the number of predictors currently in the model
  n <- length(current_predictors) # Current number of predictors in the model
  
  # Generate formulas based on direction
  if (direction == "fwd") {
    # Forward step: Generate new formulas by adding one predictor at a time
    predictor_combos <- expand.grid(current = paste(current_predictors, collapse = " + "), new = predictors_left, stringsAsFactors = FALSE)
    if (current_predictors_formula == "") {
      predictor_combos$formula <- paste("y ~", predictor_combos$new)
    }
    else {
      predictor_combos$formula <- paste("y ~", predictor_combos$current, "+", predictor_combos$new)
    }
    
  } else if (direction == "bkwd") {
    # Backward step: Generate new formulas by removing one predictor at a time
    predictor_combos <- expand.grid(current = paste(current_predictors, collapse = " + "), remove = current_predictors, stringsAsFactors = FALSE)
    predictor_combos$formula <- apply(predictor_combos, 1, function(row) {
      remaining_vars <- setdiff(current_predictors, row["remove"])
      paste("y ~", paste(remaining_vars, collapse = " + "))
    })
  }
  
  # Expand tuning_params to match the number of new formulas
  tuning_params <- tuning_params %>% select(-formula)
  fwd_tuning_grid <- tuning_params[rep(1:nrow(tuning_params), each = nrow(predictor_combos)), ]
  
  # Assign new formulas to the expanded tuning grid
  fwd_tuning_grid$formula <- as.character(predictor_combos$formula)
  # Calculate the accuracy for each specification in the grid
  accuracy_df <- grid_validate_tidy(train_data, valid_data, fwd_tuning_grid, model_fitting_procedure, engine)$accuracy_df
  
  # Extract the best model (the one with the lowest misclassification rate)
  best_model <- accuracy_df %>% filter(misclass_rate == min(misclass_rate))
  best_model <- best_model[1,]
  
  # Get the final list of predictors from the best model (right-hand side of the formula)
  new_predictors <- unlist(strsplit(best_model$formula, " ~ "))[2]
  
  # Output the best model and the new set of predictors
  return(list(accuracy_df = accuracy_df, best_model = best_model, new_predictors = new_predictors, n = n))
  return(fwd_tuning_grid)
}

fwd_step_output <- fwd_bckwd_step(Heart_train, Heart_valid, selected_predictors[1]
, tuning_grid2[1,], "random_forest", "randomForest", direction = "fwd")

fwd_step_output$accuracy_df
fwd_step_output$best_model
fwd_step_output$new_predictors

```






#### Create a function that conducts mixed stepwise subset selection

This will be based on a rule of conducting b steps backward after every a steps forward, until we get to using the maximum amount of predictors.

This then produces a grid of the best model for each semi iteration, and then in turn selects the best n predictor model out of those.


```{r}
stepwise_selection <- function(train_data, valid_data, tuning_params, model_fitting_procedure, engine, fwd_steps = 3, bkwd_steps = 2) {
  # Get all predictor names except the target variable "y"
  predictors <- setdiff(names(train_data), "y")
  
  # Get the Number of Predictors
  n_predictors <- length(predictors)
  
  # find out how many iterations m  it is before we reach the total number of predictors
  m = 0 # start with 0 iterations
  n = 0 # start with 0 predictors
  
  fwd = c()
  bkwd = c()
  n_list = c()
  total_fwd = 0
  total_bkwd = 0
  while (n <= n_predictors-fwd_steps) {
    if (total_fwd/fwd_steps != total_bkwd/bkwd_steps) {
      # if we have not done m iterations of a forward and b backward steps, then we must take another b backward steps so that they have the same number of iterations
      total_bkwd = total_bkwd + bkwd_steps
      fwd = c(fwd, total_fwd)
      bkwd = c(bkwd, total_bkwd)
      n = n - bkwd_steps
      n_list = c(n_list, n)
    }
    total_fwd = total_fwd + fwd_steps 
    fwd = c(fwd, total_fwd)
    bkwd = c(bkwd, total_bkwd)
    n = n + fwd_steps
    n_list = c(n_list, n)
  }
  
  accuracy_df = data.frame(total_fwd = fwd, total_bkwd = bkwd, n = n_list)
  
  return(list(accuracy_df = accuracy_df))
}

stepwise_selection_output <- stepwise_selection(Heart_train, Heart_valid, tuning_grid2[2,], "random_forest", "randomForest")

stepwise_selection_output$accuracy_df
```


```{r}
stepwise_selection <- function(train_data, valid_data, tuning_params, model_fitting_procedure, engine, fwd_steps = 3, bkwd_steps = 2) {
  # Get all predictor names except the target variable "y"
  predictors <- setdiff(names(train_data), "y")
  
  # Get the Number of Predictors
  n_predictors <- 4
  
  # Initialize counters for forward and backward steps
  total_fwd = 0
  total_bkwd = 0
  n = 0  # start with 0 predictors in the model
  
  fwd = c()
  bkwd = c()
  n_list = c()
  accuracy = c()
  
  best_models <- list()  # To store the best models for each step
  selected_predictors <- "cp"  # Start with "cp" as the first predictor
  
  # Perform major iterations (each major iteration consists of fwd_steps forward and bkwd_steps backward)
  while (n < n_predictors) {
    # Forward steps (for each major iteration, take fwd_steps forward)
    for (i in 1:fwd_steps) {
      if (n + 1 <= n_predictors) {
        total_fwd = total_fwd + 1
        n = n + 1
        
        # Perform a forward selection step using fwd_bckwd_step
        fwd_step_output <- fwd_bckwd_step(train_data, valid_data, selected_predictors, tuning_params, model_fitting_procedure, engine, direction = "fwd")
        
        # Store the best model for this forward step
        best_model <- fwd_step_output$best_model
        best_models[[paste0("fwd_", total_fwd)]] <- best_model
        
        # Update selected_predictors to the new predictors from the best model
        selected_predictors <- fwd_step_output$new_predictors
        
        fwd = c(fwd, total_fwd)
        bkwd = c(bkwd, total_bkwd)
        n_list = c(n_list, n)
        accuracy = c(accuracy, best_model$misclass_rate)
        #print(best_model)
      }
    }
    
    # Backward steps (after forward steps, perform bkwd_steps backward)
    for (i in 1:bkwd_steps) {
      if (n - 1 >= 0) {
        total_bkwd = total_bkwd + 1
        n = n - 1
        
        # Perform a backward selection step using fwd_bckwd_step
        bkwd_step_output <- fwd_bckwd_step(train_data, valid_data, selected_predictors, tuning_params, model_fitting_procedure, engine, direction = "bkwd")
        
        # Store the best model for this backward step
        best_model <- bkwd_step_output$best_model
        best_models[[paste0("bkwd_", total_bkwd)]] <- best_model
        
        
        # Update selected_predictors to the new predictors from the best model
        selected_predictors <- bkwd_step_output$new_predictors
        
        fwd = c(fwd, total_fwd)
        bkwd = c(bkwd, total_bkwd)
        n_list = c(n_list, n)
        accuracy = c(accuracy, best_model$misclass_rate)
        #print(best_model)
      }
    }
    
    # If we can still take more forward steps and backward steps, continue another major iteration
    if (n + fwd_steps > n_predictors) {
      break  # Stop if adding fwd_steps would exceed the total number of predictors
    }
  }
  
  # Create a dataframe for the accuracy results
  accuracy_df = data.frame(total_fwd = fwd, total_bkwd = bkwd, n = n_list, misclass_rate = accuracy)
  
  return(list(accuracy_df = accuracy_df, best_models = best_models))
}

# Example usage:
stepwise_selection_output <- stepwise_selection(Heart_train, Heart_valid, tuning_grid2[2,], "random_forest", "randomForest")
stepwise_selection_output$accuracy_df  # Table with steps
best_models <- stepwise_selection_output$best_models  # List of best models after each step
#best_models

```
```{r}
rbind(best_models$fwd_1,best_models$fwd_2)
```



## Conclusion

Did we achieve our objective? Why/Why not?

## Bibliography


## Annex
Only if you get disgusting enough to go really technical. OR, if we tried something first, and it didn't wuite work, we can show that here.