---
output: html_document
---

# ST 310 Group Project: Predicting Presence of Heart Disease in Patients

**Candidate Numbers:**



## Outline/Contents




## Introduction

### The Dataset
Where from, vars used etc etc

### What We Aim to Achieve
Motivations and objectives

## Set Up

### Packages
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
# General
library(dplyr) # for filtering and sorting to be easier.
library(rstudioapi) # to set the correct working directory


# Fitting Models
#library(tidyverse)
library(ISLR) # to get all the data and models from ISLR to experiment
library(gam) # to fit gams
library(tree) # for tree fitting to guage the functional form of the data
library(MASS) # for QDA and LDA
library(e1071) # to fit a naive bayes GMC.

# Plotting/ Visualisation
library(ggplot2) # for maintainable plots with a lot of interactability
library(yardstick) # for plotting an ROC curve


```

### Directories/Paths
#### Set the current directory to the working directory
```{r}
setPath <- dirname(getSourceEditorContext()$path)

setwd(setPath)
getwd()
```

#### Set the path for the data and predictions
```{r, echo=FALSE}
# Change data_path to where I saved the data
data_path <- paste0(getwd(),"/../data/")
predictions_path <- paste0(getwd(),"/../predictions/")

```

### Load Data
```{r}
Heart <- read.csv(paste0(data_path, "Heart.csv"))
# Rename the outcome column to y
Heart <- Heart %>% rename(y = target)

# Move the last column to the front so that the cross-validation works
Heart <- Heart[, c(ncol(Heart), 1:(ncol(Heart) - 1))]

# Change y to be factor so that  it is recognised as classification
Heart_non_fctr <- Heart
Heart$y <- factor(Heart$y)

head(Heart)
names(Heart)
```
### Cleaning
Any cleaning we have to do
### Exploratory Data Analysis (EDA)
Any eda and domain-specific research




## Model Fitting

### Set Up

#### Look at the Class Balance in Our Data


```{r}
# We need the training set to be balanced, yet the validation set to have 68% balance.
outcome_counts_fold <- Heart |> count(y)
outcome_counts_fold$proportion <- outcome_counts_fold$n / sum(outcome_counts_fold$n)
const_yhat_success <- outcome_counts_fold$proportion[[2]]
outcome_counts_fold
```

Nice, it seems rather balanced!

```{r}
const_yhat_success
```


#### Set up separate training and validation sets

##### Define a function to create training and validation sets

The below function creates training and validation sets or 'folds' in dataframe and matrix form, as well as returning the indices of the observations corresponding to these.

Optionally, we can have the class balance in the validation set (called test_bal) be different to that in the training set. We set this to be the same in both training and validation sets, and equal to the positive class proportion in the original data, given that each set is sampled independently from the original data.


```{r}

make_sets <- function(df, frac = 0.6, classify = F, test_bal = const_yhat_success) {
  train_indices <- sample(1:nrow(df), size = frac * nrow(df), replace = F)
  
  y_train <- df[, 1]
  X_train <- as.matrix(df[, -1])
  
  if (frac>0) {
    valid_indices <- -train_indices
  }
  else {
    valid_indices = rownames(df)
  }
  train_fold <- df[train_indices,]
  valid_fold <- df[valid_indices,]
  
  const_yhat_success <- NA
  outcome_counts_pre <- NA
  outcome_counts_valid <- NA
  if (classify==T) {
    outcome_counts_pre <- valid_fold |> count(y)
    outcome_counts_pre$proportion <- outcome_counts_pre$n / sum(outcome_counts_pre$n)
    rare_class_size <- round(((1-test_bal) * outcome_counts_pre[outcome_counts_pre$y==1,"n"] * (1/test_bal)),0)
    rare_class_indices <- which(valid_fold$y==0)
    common_class_indices <- which(valid_fold$y==1)
    sampled_indices <- sample(rare_class_indices, size = rare_class_size, replace = FALSE)
    valid_indices <- c(sampled_indices,common_class_indices)
    valid_fold <- valid_fold[valid_indices,]
    outcome_counts_valid <- valid_fold |> count(y)
    outcome_counts_valid$proportion <- outcome_counts_valid$n / sum(outcome_counts_valid$n)
    const_yhat_success <- outcome_counts_valid$proportion[[2]]
  }
  #train_fold <- train_fold[order(rownames(train_fold)),]
  #valid_fold <- valid_fold[order(rownames(valid_fold)),]
  
  y_train_fold <- train_fold[, 1]
  X_train_fold <- as.matrix(train_fold[, -1])
  
  y_valid_fold <- valid_fold[, 1]
  X_valid_fold <- as.matrix(valid_fold[, -1])
  
  return(list(y_train = y_train, X_train=X_train, train_fold = train_fold, valid_fold = valid_fold, y_train_fold = y_train_fold, X_train_fold = X_train_fold, y_valid_fold = y_valid_fold, X_valid_fold=X_valid_fold, const_yhat_success= const_yhat_success, outcome_counts_pre = outcome_counts_pre, outcome_counts_valid = outcome_counts_valid, train_indices=train_indices, valid_indices = valid_indices))
}
```

##### Call the function that creates training and validation sets

```{r}


# Make training and validation sets for the whole data
make_sets_output_1 <- make_sets(df=Heart )
Heart_train_fold <- make_sets_output_1$train_fold
Heart_valid_fold <- make_sets_output_1$valid_fold

# Get matrix versions of the full data
X <- make_sets_output_1$X_train
Y <- make_sets_output_1$y_train

# Make matrix versions of each of these
Heart_X_train_fold <- make_sets_output_1$X_train_fold
Heart_X_valid_fold <- make_sets_output_1$X_valid_fold

Heart_y_train_fold <- make_sets_output_1$y_train_fold
Heart_y_valid_fold <- make_sets_output_1$y_valid_fold

# Extract the indices used for training and validation for future use
train_indices <- make_sets_output_1$train_indices
valid_indices <- make_sets_output_1$valid_indices
```

#### Create k folds for cross-validation 

If we want to use the same folds for cross validation for all of the model fitting procedures and all specifications for all model fitting procedures.


```{r}


k <- 5
folds <- sample(rep(1:k, length.out = nrow(Heart)))

train_input <- list(train_df = Heart, X_train = X, y_train = Y, train_indices=train_indices, valid_indices = valid_indices, folds=folds)

```



#### Create A Dataframe to Store All of Our Model Types and the 'Best' Specifications for Each.

For each model fitting procedure, we create a grid of tuning parameters and model inputs (including a choice of predictors and functional forms where applicable).

We then iteratively fit all of these specifications and calculate the validation/cross-validation set accuracy, choosing the 'Best' model usually as that with the highest accuracy, or the simplest specification with a high enough accuracy.

In our classification setting, this is the misclassification rate.

We then store this specification (all the inputs we need to run this fit) and its accuracy in this aggregated table, repeating this for all model fitting procedures.

In other words, each row corresponds to a model fitting procedure (e.g. GAM, tree, OLS) with each column giving some parameter or information about the specification that achieved the best accuracy given that model procedure.

##### Create a function to make this dataframe

This allows us to create the table above to store any number of tuning parameters and use any form of accuracy measure, in this case, we use the misclassification (misclass) rate,
and 3 tuning parameters.

In this table, sub-models and functional forms and combinations of predictors are also counted as tuning parameters, just for maintainability, although this is not strictly true.


```{r}
gen_models_df <- function(len=10, accuracy_measures = c("min_mse")) {
  df = data.frame(
    model_type = character(len),
    model_name = character(len)
  )
  for (measure in accuracy_measures) {
    df[[measure]] <- numeric(len)
  }
  #tp stands for tuning parameter
  df$tp1_name = character(len)
  df$tp2_name = character(len)
  df$tp3_name = character(len)
  df$tp1_value = numeric(len)
  df$tp2_value = numeric(len)
  df$tp3_value = numeric(len)

  return(df)
}
Heart_models <- gen_models_df(accuracy_measures = c("misclass_rate"))

Heart_models

```


#### Define a set of functions that help to validate or cross-validate a grid of tuning parameters


##### Define a Function that Generates Predictions

Given any model fitting procedure (e.g. tree), a model fitted to that procedure (e.g. tree.fit fit to some training data), and any validation set, this function calculates the predicted outcome.

Calculating predictions varies not only on:

1. Whether we are using classification (in which case we get the conditional probabilities first, and then assign a class based on our thresholds), but also; 

2. what model fitting procedure we are using, since the packages in R use different syntax. For example, some require an X matrix input only (without the outcome), and others have the conditional probabilities in different formats.

```{r}
get_predictions <- function(model, X_valid_fold, y_valid_fold, classify, formula, model_f, matrix, thresh) {
  
  # Handle the matrix vs non-matrix input
  if (matrix) {
    valid_fold <- X_valid_fold
  } else {
    valid_fold <- data.frame(y = y_valid_fold, X_valid_fold)
  }
  
  # Classification predictions
  if (classify) {
    if (model_f == "tree") {
      # For tree models (e.g., decision tree), assuming it's a binary classification
      prob_predictions <- predict(model, valid_fold)[, 2]
    }
    else if (model_f == "naiveBayes") {
      # For Naive Bayes, type="raw" returns probabilities
      prob_predictions <- predict(model, newdata = valid_fold, type = "raw")[, 2]  # Checking column 2 for second class
    }
    else if (model_f == "qda" | model_f == "lda") {
      # For QDA and LDA, we extract posterior probabilities for the second class (usually "1" or "2")
      prob_predictions <- predict(model, newdata = valid_fold, type = "response")$posterior[, 2]  # Assuming the second column is for class "1"
    } 
    else {
      # Default case for other models (e.g., logistic regression)
      prob_predictions <- predict(model, valid_fold, type = "response")
    }
    
    # Apply threshold and convert to factor (binary classification)
    predictions <- as.factor(as.numeric(prob_predictions > thresh))
  } 
  else {
    # If regression, just return predicted values
    predictions <- predict(model, valid_fold)
  }
  
  return(predictions)
}

```

##### Define a Function that Finds Prediction Accuracy

This takes predictions as inputs and calculates the validation set mse or misclass rate accordingly.

```{r}

calculate_error <- function(predictions, true_values, classify) {
  if (classify) {
    return(mean(predictions != (as.numeric(true_values) - 1)))
  } else {
    return(mean((predictions - true_values)^2))
  }
}

```


##### Define a Function that Cross-Validates a Specification

This function takes a model fitting procedure and specification as an input, and then fits the model k times on k folds, using the previously defined functions to generate predictions and prediction accuracy for each fold.

In turn, it stores the accuracy of these folds and calculates the mean of this accuracy across all k folds and stores it as its main output, corresponding to the k-validation-fold accuracy.

```{r}

cross_validate <- function(thresh, model_f, X, Y, k, classify, train_df, folds, model_input, matrix, xname, yname) {
  e_folds <- c()  # Store error metrics for each fold
  fits <- list()   # Store model fits for each fold
  preds <- list()  # Store predictions for each fold
  valids <- list() # Store true y-values for each fold
  
  for (fold in 1:k) {
    # Train/validation split for the fold
    X_train_fold <- X[folds != fold, ]
    y_train_fold <- Y[folds != fold]
    X_valid_fold <- X[folds == fold, ]
    y_valid_fold <- Y[folds == fold]
    
    # Update model_input for this fold
    if (!matrix) {
      model_input$data <- train_df[folds != fold, ]
    } else {
      model_input[[xname]] <- X_train_fold
      model_input[[yname]] <- y_train_fold
    }

    
    # Train model
    model <- do.call(model_f, model_input)
    
    # Get validation predictions
    predictions <- get_predictions(model, X_valid_fold, y_valid_fold, classify, model_input$formula, model_f, matrix, thresh)
    
    # Compute error for this fold
    fold_error <- calculate_error(predictions, y_valid_fold, classify)

    
    # Store results
    fits[[fold]] <- model
    preds[[fold]] <- predictions
    valids[[fold]] <- y_valid_fold
    e_folds <- c(e_folds, fold_error)
  }
  
  # Compute overall CV error
  avg_error <- mean(e_folds)
  
  # Return models, predictions, validation targets, and error
  return(list(fits = fits, preds = preds, valids = valids, error = avg_error))
}


```


##### Define a Function that Cross-Validates a Specification

This function takes a model fitting procedure and specification as an input, and then fits the model once on a single validation set, using the previously defined functions to generate predictions and prediction accuracy on this set.

In turn, it stores this accuracy as its main output, corresponding to the validation set accuracy.

```{r}

validate <- function(thresh, model_f, X, Y, train_indices, valid_indices, classify, train_df, model_input, matrix, xname, yname) {
  
  # Update model_input for this fold
  if (!matrix) {
    model_input$data <- train_df[train_indices,]
  } else {
      model_input[[xname]] <- X[train_indices, ]
      model_input[[yname]] <- y[train_indices]
  }
  
  # Train model using pre-set model_input
  model <- do.call(model_f, model_input)
  # Get validation predictions
  predictions <- get_predictions(model, X[valid_indices, ], Y[valid_indices], classify, model_input$formula, model_f, matrix, thresh)
  
  # Compute validation error
  fold_error <- calculate_error(predictions, Y[valid_indices], classify)

  
  # Return the model, predictions, validation targets, and error
  return(list(fits = model, preds = predictions, valids = Y[valid_indices], error = fold_error))
}

```

##### Define a Function that Validates or Cross Validates a Grid of Specifications

This function takes a model fitting procedure and a grid where each row is a specification as an input, and then finds the validation set or k-fold-validation accuracy, using the previous functions to fit the model, calculate predictions and in turn their accuracy.

In turn, it stores this accuracy for each specification under a new column of this input grid as its main output.

From the model output is also possible to retrieve the model fit, the predictions and the validation sets for each time a model (model fitting procedure and specification/tuning parameters combination) is fit, in case we want to go back to our results.

Depending on interpretability and time, one can choose whether to validate or cross-validate.

It is possible to input the validation set or validation folds from the start, or reshuffle (create these again) each time the function is called.



```{r}



grid_validate <- function(train_input_list = train_input, method = "cv", model_f = "tree", grid_df, k = 5, frac = 0.6, matrix = FALSE, classify = FALSE, test_bal = 0.68, reshuffle = F) {
  train_df <- train_input_list$train_df
  if (reshuffle) {
    sets_output <- make_sets(df = train_df, frac = frac, classify = classify, test_bal = test_bal)
    X <- sets_output$X_train
    Y <- sets_output$y_train
    train_indices <- sets_output$train_indices
    valid_indices <- sets_output$valid_indices
    folds <- sample(rep(1:k, length.out = nrow(train_df))) # makes sure that the same folds are used, just like we make sure that the same validation set is used.
  }
  else {
    X <- train_input_list$X_train
    Y <- train_input_list$y_train
    train_indices <- train_input_list$train_indices
    valid_indices <- train_input_list$valid_indices
    folds <- train_input_list$folds
  }
  # Initialize common variables
  fits <- list()
  preds <- list()
  valids <- list()
  
  # Iterate over each row in grid_df
  for (i in 1:nrow(grid_df)) {
    # Set up model_input
    model_input <- list()
    xname <- ifelse(model_f == "gbart", "x.train", "x")
    yname <- ifelse(model_f == "gbart", "y.train", "y")
    
    if (!matrix) {
      model_input$formula <- as.formula("y ~ .")
    } 
    
    # Apply grid parameters to model_input
    current_model_f <- model_f  # Default model function
    for (tp in names(grid_df)) {
      if (!all(is.na(grid_df[[tp]]))) {
        if (tp == "mindev") {
          model_input$control <- tree.control(nobs = length(train_df$y), mindev = grid_df[i, tp])
        } else if (tp == "preds_used") {
          model_input$formula <- as.formula(paste("y ~", grid_df[i, tp]))
        } else if (tp == "sub_model") {
          current_model_f <- grid_df[i, tp]  # Override model function if 'submodel' is specified
        } else if (tp != "thresh") {
          model_input[[tp]] <- grid_df[i, tp]
        }
      }
    }
    # Get the threshold param if we have it specified and we are classifying. We don't pass this directly into the model fitting function, hence why it is not in the loop above
    thresh <- if ("thresh" %in% names(grid_df) && !is.na(grid_df[i, "thresh"])) grid_df[i, "thresh"] else 0.5
    
    # Call validation function and store results
    if (method == "cv") {
      result <- cross_validate(thresh, current_model_f, X, Y, k, classify, train_df, folds, model_input, matrix, xname, yname)
      error_col <- paste0("cv_", ifelse(classify, "misclass_rate", "mse"))
    } else if (method == "v") {
      result <- validate(thresh, current_model_f, X, Y, train_indices, valid_indices, classify, train_df, model_input, matrix, xname, yname)
      error_col <- paste0("v_", ifelse(classify, "misclass_rate", "mse"))
    } else {
      stop("Invalid method. Choose 'cv' or 'v'.")
    }
    
    # Store results in grid_df and lists
    grid_df[[error_col]][i] <- result$error
    fits[[i]] <- result$fits
    preds[[i]] <- result$preds
    valids[[i]] <- result$valids
  }
  
  return(list(grid_df = grid_df, fits = fits, preds = preds, valids = valids))
}


```

#### Create a Function that can plot how our validation or cross-validation error measures compare based on Combinations of tuning parameters.

This takes a specification grid output from grid_validate() above as an input, plotting how validation accuracy/error changes based on up to two dimensions.

```{r}
plot_grid <- function(grid, val_measure = "v_mse", tp1 = "n_preds_used", tp2 = NA, logx = FALSE) {
  best_model <- grid[which.min(grid[[val_measure]]), ]
  
  plot <- grid |>
    ggplot(aes(x = .data[[tp1]])) +
    geom_point(aes(y = .data[[val_measure]], color = if (!is.na(tp2)) as.factor(.data[[tp2]]) else NULL), size = 2, alpha = 0.5) +
    geom_point(data = best_model, aes(x = .data[[tp1]], y = .data[[val_measure]]), 
               color = "purple", shape = 16, size = 3) +
    labs(
      title = paste(val_measure, "vs.", tp1),
      x = tp1,
      y = val_measure
    ) +
    expand_limits(y = 0.9 * min(grid[[val_measure]])) +
    theme_minimal()
  
  if (!is.na(tp2)) {
    plot <- plot + scale_color_discrete(name = tp2)
  }
  if (logx) {
    plot <- plot + scale_x_log10(breaks = scales::trans_breaks("log10", function(x) 10^x))
  } 
  print(plot)
}
```


#### Create a function that retrieves the best specification 

In other words, this retreives the corresponding sub-model, regressors, functional forms and tuning parameters that give us the best validation set accuracy from a grid (like that output from grid_validate()) where each row is some different specification.

```{r}
get_best_spec <- function(grid_df, grid_validation) {
  e_column <- grep("misclass|mse", colnames(tree_df), value = TRUE)
  best_idx <- which.min(grid_df[[e_column]])
  best_e <- grid_df[best_idx, e_column]
  best_row <- grid_df[best_idx,]
  best_preds <- grid_validation$preds[[best_idx]]
  best_valids <- grid_validation$valids[[best_idx]]
  return(list(preds=best_preds, valids=best_valids, error = best_e, row =best_row))
}


```

#### Create a function that visually compares predicted values to validation values

This is set up to work both for continuous outcomes (preds against valids scatter plot) and a categorical outcome (confusion matrix)

```{r}
graph_preds <- function(preds, valids, cm=T, scatter=F, classify=T) {
  predictions_df <- data.frame(y = valids, yhat=preds)
  if (cm) {
    confusion_matrix <-
    predictions_df |> 
    conf_mat(truth = y, estimate = yhat)
    print(confusion_matrix |> autoplot(type = "heatmap"))
  }
  if (scatter == T) {
    if (classify) {
      predictions_df$yhat <- as.numeric(predictions_df$yhat) - 1
      predictions_df$y <- as.numeric(predictions_df$y) - 1
    }
    print(plot(predictions_df$yhat, predictions_df$y))
    abline(0, 1)
  }
  error_col <- paste0(ifelse(classify, "misclass_rate", "mse"))
  print(paste(error_col, ":", calculate_error(preds, valids, classify)))
}



```



### Baseline Model: Logistic (Few Predictors)
Explain why we used this

#### Validate/Cross-Validate Best Tuning Parameters
Run some code
#### Analyse Results
Look at results from best Model



### Gradient-Descent-Based Model: Logistic (Few Predictors)
Take a decreasing size step, and use a loss function without a constant
Explain why we used this
#### Explain how this works (Stickiest Part of Project)

#### Validate/Cross-Validate Best Tuning Parameters
Run some code
#### Analyse Results
Look at results from best Model


### Relatively Interpretable Model: Logistic (Multiple Predictors and Interactions)

#### Validate/Cross-Validate Best Tuning Parameters
Run some code
#### Analyse Results
Look at results from best Model



### High-Dimensional Model: Regularised Logistic (Few Predictors)

#### Validate/Cross-Validate Best Tuning Parameters
Run some code
#### Analyse Results
Look at results from best Model



### Predictively Accurate Model: Classification Random Forest?




### Classification Tree

#### Validate/Cross-Validate Best Tuning Parameters

##### Create a Grid of Tuning Parameters
```{r}
mindev_values <- c(5,4,3,2,1)*rep(10^seq(-2, -4, by = -1), each=5) # i've done too much matrix stuff in my finance module so this dodgy syntax is now all too natural.

tree_grid <- expand.grid(mindev = mindev_values, thresh = seq(0.4,0.6,0.02))
#grid_results$tree_size <- NA
#tree_grid <- data.frame(dummy = rep(NA,4))
tree_grid
```

##### Compare Prediction accuracy for each of the models
```{r} 
tree_df_output <- grid_validate(grid_df = tree_grid,  method = "v", classify=T)
tree_df <- tree_df_output$grid_df
```
**Look at the Grid**
```{r}
tree_df
```

**Graph the Grid**

```{r}
plot_grid(tree_df, val_measure = "v_misclass_rate", tp1 = "mindev", tp2 = "thresh")
```


#### Analyse the best specification's predictions against the validation set



```{r}
best_spec_output <- get_best_spec(grid_df = tree_df, grid_validation = tree_df_output)
graph_preds(best_spec_output$preds, best_spec_output$valids, cm=T)
```
#### Validate/Cross-Validate Best Tuning Parameters Using In-Built Features

```{r}
# Fit a tree
tree_fit <- tree(y ~ ., Heart_train_fold)
# Fit a glm
heart_2 <- glm(y~ ., Heart_non_fctr, family = "gaussian")
# Store the Probability predictions (the second row corresponds to the postive class probability)
tree_fit_prob_preds <- predict(tree_fit, Heart_valid_fold)[,2]

#GHet the standard bayesian classifier class predictions
preds <- predict(tree_fit, Heart_valid_fold, type = "class")
plot(preds, as.numeric(Heart_valid_fold$y)-1)

```

##### Cross-Validate the Best Pruning Coefficient, K (Penalty for number of terminal nodes) and overall tree size.
```{r}
cv.tree_fit <- cv.tree(tree_fit, FUN = prune.misclass)
par(mfrow = c(1, 2))
plot(cv.tree_fit$size, cv.tree_fit$dev, type = "b")
plot(cv.tree_fit$k, cv.tree_fit$dev, type = "b")
```
#### Analyse the Accuracy


##### Compare confusion matrices for the tree model.
```{r}

tree_fit_predictions <-data.frame(
  y = Heart_valid_fold$y,
  .fitted = tree_fit_prob_preds
) |> 
  mutate(yhat = factor(as.numeric(.fitted > const_yhat_success))) |>
  mutate(misclass = (y!=yhat))
paste("Class Balance:")
tree_fit_predictions |> pull(.fitted) |> mean() # same as mean(tree_fit_predictions$.fitted)
paste("Misclass Rate:")
sum((tree_fit_predictions$y!=tree_fit_predictions$yhat)) / nrow(tree_fit_predictions)
```
##### Look at ROC Curve to guage Trade-Off
```{r, eval=F}
tree_fit_predictions |>
  roc_curve(truth = y, .fitted,
            event_level = "second") |>
  autoplot()
# the 'truth' is just the training y here, not the testing y, which we do not have, nor a validation set.
```
#### Classify at different thresholds.

```{r, eval=F}
higher_cutoff <- const_yhat_success + .15
confusion_matrix_higher <-
  tree_fit_predictions |>
  mutate(yhat = factor(as.numeric(.fitted > higher_cutoff))) |> # this line predicts y = 1 ONLY if the predicted conditional probability is above the higher cutoff
  conf_mat(truth = y, estimate = yhat)
```
```{r, eval=F}
confusion_matrix_medium <-
  tree_fit_predictions |>
  mutate(yhat = factor(as.numeric(.fitted > const_yhat_success))) |> 
  conf_mat(truth = y, estimate = yhat)
```


```{r, eval=F}
lower_cutoff <- const_yhat_success - 0.15
confusion_matrix_lower <-
  tree_fit_predictions |>
  mutate(yhat = factor(as.numeric(.fitted > lower_cutoff))) |>
  conf_mat(truth = y, estimate = yhat)
```

##### 1. Balanced Confusion Matrix
```{r, eval=F}
confusion_matrix_medium |> autoplot(type = "heatmap")
```
##### 3. Low Cut-Off Confusion Matrix
```{r, eval=F}
confusion_matrix_lower |> autoplot(type = "heatmap")
```
##### 3. High Cut-Off Confusion Matrix
```{r, eval=F}
confusion_matrix_higher |> autoplot(type = "heatmap")
```
##### Comparing key sum stats:

```{r, eval=F}
higher_summary <- summary(confusion_matrix_higher) |>
  mutate(higher = .estimate) |>
  dplyr::select(.metric, higher)
medium_summary <- summary(confusion_matrix_medium) |>
  mutate(medium = .estimate) |>
  dplyr::select(medium)
lower_summary <- summary(confusion_matrix_lower) |>
  mutate(lower = .estimate) |>
  dplyr::select(lower)
cbind(higher_summary, medium_summary, lower_summary) |>
  knitr::kable()


```

As seen from the confusion matrices earlier, the threshold does not really seem to matter.

### Generative Models for Classification (GMCs: LDA, QDA and Naive Bayes)**


##### Create a Grid of Tuning Parameters

```{r}

gmc_grid <- expand.grid(sub_model = c("lda","qda", "naiveBayes"), thresh = seq(0.4,0.6,0.02))
gmc_grid$sub_model <- as.character(gmc_grid$sub_model)
gmc_grid
```

##### Compare Prediction accuracy for each of the models

**Look at the Grid**
```{r}
gmc_output <- grid_validate(grid_df = gmc_grid,  method = "v", classify=T)
gmc_df <- gmc_output$grid_df
gmc_df
```
**Graph the Grid**

```{r}
plot_grid(gmc_df, val_measure = "v_misclass_rate", tp1 = "thresh", tp2 = "sub_model")
```




#### Analyse Results

```{r}
best_spec_output <- get_best_spec(grid_df = gmc_df, grid_validation = gmc_output)
graph_preds(best_spec_output$preds, best_spec_output$valids, cm=T)
```


## Conclusion

Did we achieve our objective? Why/Why not?

## Bibliography


## Annex
Only if you get disgusting enough to go really technical. OR, if we tried something first, and it didn't wuite work, we can show that here.