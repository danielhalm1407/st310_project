---
output: html_document
---

# ST 310 Group Project: Predicting Presence of Heart Disease in Patients

**Candidate Numbers:**



## Outline/Contents




## Introduction

### The Dataset
Where from, vars used etc etc

### What We Aim to Achieve
Motivations and objectives

## Set Up

### Packages
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
# General
library(dplyr) # for filtering and sorting to be easier.
library(rstudioapi) # to set the correct working directory



# Fitting Models
#library(tidyverse)
library(ISLR) # to get all the data and models from ISLR to experiment
library(gam) # to fit gams
library(tree) # for tree fitting to guage the functional form of the data
library(randomForest) # for random forest fitting
library(ranger) # for random forest fitting
library(MASS) # for QDA and LDA
library(e1071) # to fit a naive bayes GMC.

# Plotting/ Visualisation
library(ggplot2) # for maintainable plots with a lot of interactability
library(yardstick) # for plotting an ROC curve
library(iml) # To get PDPs from a random forest
library(GGally) # to make various two-way plots
library(rpart.plot) # to display trees generated from the rpart engine

# Easier Analysis
library(tidymodels) # Allows easy model fitting and multi-parameter tuning
library(purrr) # So that I can iterate over custom thresholds to assign class
library(rsample) # to allow parameter tuning using tidymodels

# Resolve package conflicts (basically avoid having to us dplyr::select all the time)
library(conflicted)

conflict_prefer("select", "dplyr")
conflicts_prefer(dplyr::filter)
conflicts_prefer(yardstick::accuracy)
conflicts_prefer(base::as.matrix)
conflicts_prefer(tree::tree)
conflicts_prefer(plyr::id)
conflicts_prefer(dplyr::rename)
conflicts_prefer(e1071::tune)
conflicts_prefer(dplyr::mutate)
conflicts_prefer(dplyr::count)
```

### Directories/Paths
#### Set the current directory to the working directory
```{r}
setPath <- dirname(getSourceEditorContext()$path)

setwd(setPath)
getwd()
```

#### Set the path for the data and predictions
```{r, echo=FALSE}
# Change data_path to where I saved the data
data_path <- paste0(getwd(),"/../data/")
predictions_path <- paste0(getwd(),"/../predictions/")

```

### Load Data
```{r}
Heart <- read.csv(paste0(data_path, "Heart.csv"))
# Rename the outcome column to y
Heart <- Heart %>% rename(y = target)

# Move the last column to the front so that the cross-validation works
Heart <- Heart[, c(ncol(Heart), 1:(ncol(Heart) - 1))]

# Change y to be factor so that  it is recognised as classification
Heart_non_fctr <- Heart
Heart$y <- factor(Heart$y)

# Get number of predictors/features
n_preds <- sum(names(Heart) != "y")

head(Heart)
names(Heart)
```
### Cleaning
Any cleaning we have to do
### Exploratory Data Analysis (EDA)
Any eda and domain-specific research

#### Look at joint distribution of predictors against each other and the outcome

```{r}
Heart[,1:5] |>
  ggpairs(progress = F)
```

```{r}
for (i in seq(2,n_preds,3)) {
  plot <- ggpairs(
    Heart[, c(i:(min(n_preds,i + 2)), 1)], 
    progress = FALSE
  ) + 
    theme_minimal()
  
  print(plot)
}
```

Note in the above that the distribution of most of the integer variables appear to have a discrete gaussian/bell-curve type distribution within each class (with the exception of trestbps (resting blood pressure)). 

Moreover, the distributions for both integer and factor variables can be distinct conditional on each class.Where the distributions are conditionally gaussian, this implies that both the mean and covariance for each class is different for each class, possibly motivating a QDA.


However, this is not precisely a normal (conditional on y), limiting the possible usefulness of QDA as a model fitting procedure.

Naive bayes also appears a dud, as we often observe high correlations between the features/predictors across both classes, and naturally this is likely to extend to high correlations within class as well. Therefore, we cannot make the necessary argument that the predictors are independent within a given class.

## Model Fitting

### Set Up

#### Look at the Class Balance in Our Data


```{r}
# We need the training set to be balanced, yet the validation set to have 68% balance.
outcome_counts_fold <- Heart |> count(y)
outcome_counts_fold$proportion <- outcome_counts_fold$n / sum(outcome_counts_fold$n)
const_yhat_success <- outcome_counts_fold$proportion[[2]]
outcome_counts_fold
```

Nice, it seems rather balanced!

```{r}
const_yhat_success
```


#### Set up separate training and validation sets

We use a built in feature from the Tidyverse package to create a 70-30 train-test split.
```{r}
Heart_split <- initial_split(Heart, prop = 0.7)
Heart_train <- Heart_split %>% training()
Heart_valid <- Heart_split %>% testing()

```
**Inspect the training data**

```{r, eval = FALSE}
Heart_train
```
**Inspect the validation data**
```{r, eval= F}
Heart_valid
```




#### Create A Dataframe to Store All of Our Model Types and the 'Best' Specifications for Each.

For each model fitting procedure, we create a grid of tuning parameters and model inputs (including a choice of predictors and functional forms where applicable).

We then iteratively fit all of these specifications and calculate the validation/cross-validation set accuracy, choosing the 'Best' model usually as that with the highest accuracy, or the simplest specification with a high enough accuracy.

In our classification setting, this is the misclassification rate.

We then store this specification (all the inputs we need to run this fit) and its accuracy in this aggregated table, repeating this for all model fitting procedures.

In other words, each row corresponds to a model fitting procedure (e.g. GAM, tree, OLS) with each column giving some parameter or information about the specification that achieved the best accuracy given that model procedure.

##### Create a function to make this dataframe

This allows us to create the table above to store any number of tuning parameters and use any form of accuracy measure, in this case, we use the misclassification (misclass) rate,
and 3 tuning parameters.

In this table, sub-models and functional forms and combinations of predictors are also counted as tuning parameters, just for maintainability, although this is not strictly true.


```{r}
gen_models_df <- function(len=10, accuracy_measures = c("min_mse")) {
  df = data.frame(
    model_type = character(len),
    model_name = character(len)
  )
  for (measure in accuracy_measures) {
    df[[measure]] <- numeric(len)
  }
  #tp stands for tuning parameter
  df$tp1_name = character(len)
  df$tp2_name = character(len)
  df$tp3_name = character(len)
  df$tp1_value = numeric(len)
  df$tp2_value = numeric(len)
  df$tp3_value = numeric(len)

  return(df)
}
Heart_models <- gen_models_df(accuracy_measures = c("misclass_rate"))

Heart_models

```


#### Functions for Model Tuning

We define a set of functions to select the 'best' tuning parameters for different models, often defined as the simplest model past a threshold level of prediction accuracy.

##### Define a Function that Finds Prediction Accuracy

This takes predictions as inputs and calculates the validation mis-classification rate (or, optionally, MSE) accordingly.

```{r}

calculate_error <- function(predictions, true_values, classify) {
  if (classify) {
    return(mean(predictions != (as.numeric(true_values) - 1)))
  } else {
    return(mean((predictions - true_values)^2))
  }
}

```


##### Define a Function that Calculates Validation Set Accuracy for a Grid of Tuning Parameters.

We use Tidymodels syntax for efficiency and modularity with different fitting procedures.

Uses a helper function to get the model specification (including the fitting procedure and the engine) into tidyverse's desired format first.

```{r}
# Function to get model specification
get_model_spec <- function(model_fitting_procedure, engine, tuning_params) {
  if (model_fitting_procedure == "tree") {
    model_spec_updated <- decision_tree(
      mode = "classification", 
      cost_complexity = tuning_params$cp,  # Cost-complexity pruning
      tree_depth = tuning_params$maxdepth,  # Limit depth
      min_n = tuning_params$min_n  # Controls the number of splits
    ) %>%
      set_engine(engine)  
    
  } else if (model_fitting_procedure == "random_forest") {
    model_spec_updated <- rand_forest(
      mtry = tuning_params$mtry, 
      trees = tuning_params$trees, 
      mode = "classification"
    ) %>%
      set_engine(engine, probability = TRUE)  
    
  } else if (model_fitting_procedure == "boosting") {
    model_spec_updated <- boost_tree(mode = "classification") %>%
      set_engine(engine)  
    
  } else {
    stop("Invalid model fitting procedure. Choose from 'tree', 'random_forest', or 'boosting'.")
  }
  
  return(model_spec_updated)
}

```

```{r}
# Function to get the accuracy for each model specification corresponding to the tuning parameters


grid_validate_tidy <- function(train_data, valid_data, tuning_grid, model_fitting_procedure, engine) {
  # Initialize empty data frames to store results and predictions
  accuracy_df <- tuning_grid
  all_preds_df <- data.frame()
  model_fits <- list()  # Initialize the list to store model fits
  
  # Iterate over each combination of hyperparameters in the tuning grid
  for(i in 1:nrow(tuning_grid)) {
    # Extract current tuning parameters
    tuning_params <- tuning_grid[i, ]
    
    # Get the model specification dynamically
    model_spec_updated <- get_model_spec(model_fitting_procedure, engine, tuning_params)
    
    # Create a workflow
    current_wf <- workflow() %>%
      add_model(model_spec_updated) %>%
      add_formula(as.formula(tuning_params$formula))
    
    # Fit the model on the training data
    model_fit <- fit(current_wf, data = train_data)
    
    # Store the model fit in the list
    model_fits[[i]] <- model_fit  # Index the model fit by i
    
    # Predict probabilities on the validation set
    prob_predictions <- predict(model_fit, valid_data, type = "prob")$.pred_1
    
    # Apply threshold to classify predictions
    predictions <- as.factor(as.numeric(prob_predictions > tuning_params$thresh))
    
    # Calculate misclassification rate
    error <- mean(predictions != valid_data$y)
    
    # Store results
    accuracy_df$misclass_rate[i] = error
    
    # Put the accuracy results first and the formula last
    accuracy_df <- accuracy_df %>%
      select(misclass_rate, everything(), -formula, formula)
    
    # Store predictions
    preds_df <- data.frame(preds = predictions, prob_preds = prob_predictions) %>%
      bind_cols(valid_data %>% select(y)) %>%
      mutate(spec_no = i)
    
    all_preds_df <- rbind(all_preds_df, preds_df)
  }
  
  # Return both results and predictions, including the list of model fits
  return(list(
    accuracy_df = accuracy_df,
    preds = all_preds_df,
    model_fits = model_fits  # Add the list of model fits to the output
  ))
}




```

##### Set Up Formulas to Plug into Tuning Grids

```{r}
predictors <- names(Heart)[-1]
# Set up a set of formulae (subsets of predictors) that we can use
selected_predictors = c(paste(as.character(predictors[1]), collapse = " + "),
                                           paste(as.character(predictors[1:4]), collapse = " + "),
                                           paste(as.character(predictors[1:8]), collapse = " + "),
                                           paste(as.character(predictors[1:12]), collapse = " + "))
formulas <- paste("y ~", selected_predictors)

```



#### Create a Function That Can Plot How Our Validation or Cross-Validation Error Measures Compare Based on Combinations of Tuning Parameters.

This takes a specification grid output from grid_validate_tidy() above as an input, plotting how validation accuracy/error changes based on up to two dimensions.

```{r}
plot_grid <- function(grid, val_measure = "v_mse", tp1 = "n_preds_used", tp2 = NA, logx = FALSE) {
  best_model <- grid[which.min(grid[[val_measure]]), ]
  
  plot <- grid |>
    ggplot(aes(x = .data[[tp1]])) +
    geom_point(aes(y = .data[[val_measure]], color = if (!is.na(tp2)) as.factor(.data[[tp2]]) else NULL), size = 2, alpha = 0.5) +
    geom_point(data = best_model, aes(x = .data[[tp1]], y = .data[[val_measure]]), 
               color = "purple", shape = 16, size = 3) +
    labs(
      title = paste(val_measure, "vs.", tp1),
      x = tp1,
      y = val_measure
    ) +
    expand_limits(y = 0.9 * min(grid[[val_measure]])) +
    theme_minimal()
  
  if (!is.na(tp2)) {
    plot <- plot + scale_color_discrete(name = tp2)
  }
  if (logx) {
    plot <- plot + scale_x_log10(breaks = scales::trans_breaks("log10", function(x) 10^x))
  } 
  print(plot)
}
```


#### Create a function that retrieves the best specification 

In other words, this retreives the corresponding sub-model, regressors, functional forms and tuning parameters that give us the best validation set accuracy from a grid (like that output from grid_validate()) where each row is some different specification.

```{r}
get_best_spec <- function(grid_df, grid_validation) {
  e_column <- grep("misclass|mse", colnames(tree_df), value = TRUE)
  best_idx <- which.min(grid_df[[e_column]])
  best_e <- grid_df[best_idx, e_column]
  best_row <- grid_df[best_idx,]
  best_preds <- grid_validation$preds[[best_idx]]
  best_valids <- grid_validation$valids[[best_idx]]
  best_fits <- grid_validation$fits[[best_idx]]
  return(list(preds=best_preds, valids=best_valids, fits = best_fits, error = best_e, row =best_row))
}


```

#### Create a function that visually compares predicted values to validation values

This is set up to work both for continuous outcomes (preds against valids scatter plot) and a categorical outcome (confusion matrix)

```{r}
graph_preds <- function(preds, valids, cm=T, scatter=F, classify=T) {
  predictions_df <- data.frame(y = valids, yhat=preds)
  if (cm) {
    confusion_matrix <-
    predictions_df |> 
    conf_mat(truth = y, estimate = yhat)
    print(confusion_matrix |> autoplot(type = "heatmap"))
  }
  if (scatter == T) {
    if (classify) {
      predictions_df$yhat <- as.numeric(predictions_df$yhat) - 1
      predictions_df$y <- as.numeric(predictions_df$y) - 1
    }
    print(plot(predictions_df$yhat, predictions_df$y))
    abline(0, 1)
  }
  error_col <- paste0(ifelse(classify, "misclass_rate", "mse"))
  print(paste(error_col, ":", calculate_error(preds, valids, classify)))
}



```




### Baseline Model: Logistic (Few Predictors)
Explain why we used this

#### Validate/Cross-Validate Best Tuning Parameters
Run some code
#### Analyse Results
Look at results from best Model



### Gradient-Descent-Based Model: Logistic (Few Predictors)
Take a decreasing size step, and use a loss function without a constant
Explain why we used this
#### Explain how this works (Stickiest Part of Project)

#### Validate/Cross-Validate Best Tuning Parameters
Run some code
#### Analyse Results
Look at results from best Model


### Relatively Interpretable Model: Depth 5 Classification Tree

### Classification Tree

In this case, we use the rpart engine as opposed to the tree engine.

#### Validate Best Tuning Parameters

##### Set up A Tuning Grid
```{r}
tuning_grid_tree <- expand.grid(
  cp = c(0.001),
  #seq(0.001, 0.0001, length.out = 10),   # Cost complexity pruning (alpha parameter that imposes penalty on tree depth)
  maxdepth = c(1,3,4,5,6,8,10,15,20),  # Limit tree depth
  min_n = c(5),  # Number of obs per split, so if higher, reduces splits
  thresh = c(0.4, 0.5),  
  formula = as.character(formulas[4])
)

tuning_grid_tree$formula <- as.character(tuning_grid_tree$formula)
```

**Look at the Grid**
```{r}
tree_output <- grid_validate_tidy(Heart_train, Heart_valid, tuning_grid_tree, "tree", "rpart")

# View the accuracy results
tree_output$accuracy_df
```

**Look at the Plot**
```{r}
plot_grid(tree_output$accuracy_df, val_measure = "misclass_rate", tp1 = "maxdepth", tp2 = "thresh")
```

It is observable that already once we reach depth 4, validation set accuracy is relatively high, with only an approximate and acceptable 15% misclassification rate. Nonetheless, the short depth retains interpretability.




#### Analyse Results

```{r}
# Check the predictions of the model with the lowest misclass rate
best_spec_idx <- which.min(tree_output$accuracy_df$misclass_rate)
best_spec_idx <- 3 # since this is still relatively interpretable.
tree_fit_predictions <- tree_output$preds %>%
  filter(spec_no == best_spec_idx)
tree_fit <- tree_output$model_fits[[best_spec_idx]]
#tree_fit_predictions
```

##### Prediction Accuracy


**Look at Confusion Matrix**

```{r}
graph_preds(tree_fit_predictions$preds, tree_fit_predictions$y, cm=T, scatter=F)
```



**Look at ROC Curve to guage Cut-Off Trade-Off**

```{r, eval=F}
tree_fit_predictions |>
  roc_curve( truth = y, prob_preds,
            event_level = "second") |>
  autoplot()

```
#### Use Best Model To Learn About Data

**Observe the tuning parameters of this chosen specification**
```{r}

best_params <- tree_output$accuracy_df[best_spec_idx,]
best_params %>% glimpse()

```

##### Look at the splits it is conducting and on what variables:


**Using the rpart engine**

```{r}
# Define the best model specification
best_model_spec <- decision_tree(
  mode = "classification", 
  cost_complexity = best_params$cp,  # Cost-complexity pruning
  tree_depth = best_params$maxdepth,  # Max depth of tree
  min_n = best_params$min_n  # Minimum number of observations per node
) %>%
  set_engine("rpart")

# Create workflow
best_workflow <- workflow() %>%
  add_model(best_model_spec) %>%
  add_formula(as.formula(best_params$formula))

# Fit the model to full training data
best_tree_fit <- fit(best_workflow, data = Heart_train)

# Extract the fitted model
best_rpart_model <- extract_fit_engine(best_tree_fit)  # Gets the rpart model

# Plot the tree
rpart.plot(best_rpart_model)
```
**Using the tree engine**

```{r}
# Fit a full tree with the tree engine from the tree package
full_tree <- tree(y ~ ., data = Heart_train)

# Prune the tree to a depth of 5 (21 terminal nodes)
pruned_tree <- prune.tree(full_tree, best = 12)  # 'best' controls the number of terminal nodes

# Plot the pruned tree
plot(pruned_tree)
text(pruned_tree, pretty = 0)

```

```{r, echo=f, eval = F}
cv_result <- cv.tree(tree_fit_tree, FUN = prune.tree)  # Cross-validation
plot(cv_result$size, cv_result$dev, type = "b")  # Tree size vs. error
plot(cv_result$k, cv_result$dev, type = "b")
```
```{r}

```



#### Validate/Cross-Validate Best Tuning Parameters
Run some code
#### Analyse Results
Look at results from best Model



### High-Dimensional Model: Regularised Logistic (Few Predictors)

#### Validate/Cross-Validate Best Tuning Parameters
Run some code
#### Analyse Results
Look at results from best Model



### Predictively Accurate Model: Classification Random Forest?






### Random Forest

#### Validate Best Tuning Parameters

##### Set up a Grid of Tuning Parameters

```{r}
tuning_grid_rf <- expand.grid(
  mtry = 1:6,
  trees = c(500),
  thresh = c(0.4, 0.5),
  formula = as.character(formulas[3])
)
tuning_grid_rf$formula = as.character(tuning_grid_rf$formula)
tuning_grid_rf

```

##### Compare Accuracy of Different Tuning Parameters

```{r}
# Run the grid validation function
rf_output <- grid_validate_tidy(Heart_train, Heart_valid, tuning_grid_rf, "random_forest", "randomForest")
```


**Look at the Grid**
```{r}

rf_output$accuracy_df
```

**Look at the Plot**
```{r}
plot_grid(rf_output$accuracy_df, val_measure = "misclass_rate", tp1 = "mtry", tp2 = "thresh")
```
#### Analyse Results

```{r}
# Check the predictions of the model with the lowest misclass rate
best_spec_idx <-which.min(rf_output$accuracy_df$misclass_rate)
rf_fit_predictions <- rf_output$preds %>%
  filter(spec_no == best_spec_idx)
#check_preds
```

##### Prediction Accuracy

**Look at Confusion Matrix**

```{r}
graph_preds(rf_fit_predictions$preds, rf_fit_predictions$y, cm=T, scatter=F)
```

**Look at ROC Curve to guage Cut-Off Trade-Off**

```{r, eval=F}
rf_fit_predictions |>
  roc_curve( truth = y, prob_preds,
            event_level = "second") |>
  autoplot()

```

#### Use Best Model To Learn About Data

```{r}

best_spec_row <- rf_output$accuracy_df[best_spec_idx,]
best_spec_row
```


##### Find Variable Importance
```{r}


rf_fit <- randomForest(y ~ ., data = Heart_train, ntree = best_spec_row$trees, mtry = best_spec_row$mtry, importance = TRUE)


varImpPlot(rf_fit)
```



##### Guage Functional Form of Conditional Probability using PDPs
```{r}
pred_rf <- Predictor$new(rf_fit)

pdp_rf <- FeatureEffects$new(pred_rf, features = c("cp","ca", "chol", "trestbps"), method = "pdp+ice")

plot(pdp_rf) 
```







### Subset Selection

```{r}
# Function to get model specification
get_model_spec <- function(model_fitting_procedure, engine, tuning_params) {
  if (model_fitting_procedure == "tree") {
    model_spec_updated <- decision_tree(
      mode = "classification", 
      cost_complexity = tuning_params$cp,  # Cost-complexity pruning
      tree_depth = tuning_params$maxdepth,  # Limit depth
      min_n = tuning_params$min_n  # Controls the number of splits
    ) %>%
      set_engine(engine)  
    
  } else if (model_fitting_procedure == "random_forest") {
    model_spec_updated <- rand_forest(
      mtry = tuning_params$mtry, 
      trees = tuning_params$trees, 
      mode = "classification"
    ) %>%
      set_engine(engine, probability = TRUE)  
    
  } else if (model_fitting_procedure == "boosting") {
    model_spec_updated <- boost_tree(mode = "classification") %>%
      set_engine(engine)  
    
  } else {
    stop("Invalid model fitting procedure. Choose from 'tree', 'random_forest', or 'boosting'.")
  }
  
  return(model_spec_updated)
}

```

```{r}
# Function to get the accuracy for each model specification corresponding to the tuning parameters


grid_validate_tidy <- function(train_data, valid_data, tuning_grid, model_fitting_procedure, engine) {
  # Initialize empty data frames to store results and predictions
  accuracy_df <- tuning_grid
  all_preds_df <- data.frame()
  model_fits <- list()  # Initialize the list to store model fits
  
  # Iterate over each combination of hyperparameters in the tuning grid
  for(i in 1:nrow(tuning_grid)) {
    # Extract current tuning parameters
    tuning_params <- tuning_grid[i, ]
    
    # Get the model specification dynamically
    model_spec_updated <- get_model_spec(model_fitting_procedure, engine, tuning_params)
    
    # Create a workflow
    current_wf <- workflow() %>%
      add_model(model_spec_updated) %>%
      add_formula(as.formula(tuning_params$formula))
    
    # Fit the model on the training data
    model_fit <- fit(current_wf, data = train_data)
    
    # Store the model fit in the list
    model_fits[[i]] <- model_fit  # Index the model fit by i
    
    # Predict probabilities on the validation set
    prob_predictions <- predict(model_fit, valid_data, type = "prob")$.pred_1
    
    # Apply threshold to classify predictions
    predictions <- as.factor(as.numeric(prob_predictions > tuning_params$thresh))
    
    # Calculate misclassification rate
    predictions <- factor(predictions, levels = levels(valid_data$y)) # I had to force level matching because when fbs is used as a single predictor, it throws a level mismatch error, making stepwise selection a problem
    error <- mean(predictions != valid_data$y)
    
    # Store results
    accuracy_df$misclass_rate[i] = error
    
    # Put the accuracy results first and the formula last
    accuracy_df <- accuracy_df %>%
      select(misclass_rate, everything(), -formula, formula)
    
    # Store predictions
    preds_df <- data.frame(preds = predictions, prob_preds = prob_predictions) %>%
      bind_cols(valid_data %>% select(y)) %>%
      mutate(spec_no = i)
    
    all_preds_df <- rbind(all_preds_df, preds_df)
  }
  
  # Return both results and predictions, including the list of model fits
  return(list(
    accuracy_df = accuracy_df,
    preds = all_preds_df,
    model_fits = model_fits  # Add the list of model fits to the output
  ))
}




```


#### Set up a dummy tuning grid

Formula is empty as we start with 0 predictors.

```{r}
tuning_grid_step <- expand.grid(
  mtry = 1:6,
  trees = c(500),
  thresh = c(0.5),
  formula = as.character("")
)
tuning_grid_step$formula = as.character(tuning_grid_step$formula)
tuning_grid_step
```



#### Create a function that conducts one forward or one backward stepwise iteration

##### First Need a Function that Extracts the Predictors from a Formula Column in a Tuning Grid

```{r}
extract_predictors <- function(formula) {
  # Remove "y ~" from the formula
  formula_rhs <- gsub("y ~", "", formula)
  
  # Trim spaces
  formula_rhs <- trimws(formula_rhs)
  
  # Split into vector if not empty, otherwise return empty vector
  if (formula_rhs == "" || formula_rhs == "y ~") {
    return(character(0))  # Return empty vector if no predictors in the formula
  } else {
    return(unlist(strsplit(formula_rhs, " \\+ ")))  # Split by " + "
  }
}

# Test Cases
extract_predictors("y ~ x1 + x2 + x3")  # Returns: c("x1", "x2", "x3")
extract_predictors("y ~")               # Returns: character(0)
extract_predictors("")                  # Returns: character(0)
```




```{r}
fwd_bckwd_step <- function(train_data, valid_data, tuning_params, model_fitting_procedure, engine, direction = "fwd") {
  
  # Get all predictor names except the target variable "y"
  predictors <- setdiff(names(train_data), "y")
  
  # Split current_predictors string into individual variable names, if any
  current_predictors <- extract_predictors(tuning_params$formula)  # Fixed here, using extract_predictors function directly
  
  # Find remaining predictors not in the current predictor set
  predictors_left <- setdiff(predictors, current_predictors)
  
  # Track the number of predictors currently in the model
  n <- length(current_predictors) # Current number of predictors in the model
  
  # Generate formulas based on direction
  if (direction == "fwd") {
    # Forward step: Generate new formulas by adding one predictor at a time
    predictor_combos <- expand.grid(current = paste(current_predictors, collapse = " + "), new = predictors_left, stringsAsFactors = FALSE)
    if (length(current_predictors) == 0) {
      predictor_combos$formula <- paste("y ~", predictor_combos$new)
    }
    else {
      predictor_combos$formula <- paste("y ~", predictor_combos$current, "+", predictor_combos$new)
    }
    
  } else if (direction == "bkwd") {
    # Backward step: Generate new formulas by removing one predictor at a time
    predictor_combos <- expand.grid(current = paste(current_predictors, collapse = " + "), remove = current_predictors, stringsAsFactors = FALSE)
    predictor_combos$formula <- apply(predictor_combos, 1, function(row) {
      remaining_vars <- setdiff(current_predictors, row["remove"])
      paste("y ~", paste(remaining_vars, collapse = " + "))
    })
  }
  
  # Expand tuning_params to match the number of new formulas
  tuning_params <- tuning_params %>% select(-formula)
  fwd_tuning_grid <- tuning_params[rep(1:nrow(tuning_params), each = nrow(predictor_combos)), ]
  
  # Assign new formulas to the expanded tuning grid
  fwd_tuning_grid$formula <- as.character(predictor_combos$formula)
  
  # Calculate the accuracy for each specification in the grid
  accuracy_df <- grid_validate_tidy(train_data, valid_data, fwd_tuning_grid, model_fitting_procedure, engine)$accuracy_df
  
  # Extract the best model (the one with the lowest misclassification rate)
  best_model <- accuracy_df %>% filter(misclass_rate == min(misclass_rate))
  best_model <- best_model[1,]
  
  # Get the final new formula with the new predictors from the best model
  new_formula <- best_model$formula  # Using extract_predictors to split the formula
  
  # Output the best model and the new set of predictors
  return(list(accuracy_df = accuracy_df, best_model = best_model, new_formula = new_formula, n = n))
  
}


fwd_step_output <- fwd_bckwd_step(Heart_train, Heart_valid,  tuning_grid_step[1,], "random_forest", "randomForest", direction = "fwd")

fwd_step_output$accuracy_df
#fwd_step_output$best_model
fwd_step_output$new_formula

```






#### Create a function that conducts mixed stepwise subset selection

This will be based on a rule of conducting b steps backward after every a steps forward, until we get to using the maximum amount of predictors.

This then produces a grid of the best model for each semi iteration, and then in turn selects the best n predictor model out of those.



```{r}
stepwise_selection <- function(train_data, valid_data, tuning_params, model_fitting_procedure, engine, predictors_lim = 4, fwd_steps = 3, bkwd_steps = 2) {
  # Get all predictor names except the target variable "y"
  predictors <- setdiff(names(train_data), "y")
  
  # Get the top number of predictors we can use
  n_predictors <- predictors_lim
  
  # Initialize counters for forward and backward steps
  total_fwd = 0
  total_bkwd = 0
  n = 0  # start with 0 predictors in the model
  
  accuracy_df <- data.frame()  # Initialize an empty dataframe for the accuracy results
  
  # Perform major iterations (each major iteration consists of fwd_steps forward and bkwd_steps backward)
  while (n < n_predictors) {
    # Forward steps (for each major iteration, take fwd_steps forward)
    for (i in 1:fwd_steps) {
      if (n + 1 <= n_predictors) {
        total_fwd = total_fwd + 1
        n = n + 1
        
        # Perform a forward selection step using fwd_bckwd_step
        fwd_step_output <- fwd_bckwd_step(train_data, valid_data, tuning_params, model_fitting_procedure, engine, direction = "fwd")
        
        # Store the best model for this forward step
        best_model <- fwd_step_output$best_model
        
        # Update selected_predictors to the new predictors from the best model
        tuning_params$formula <- as.character(fwd_step_output$new_formula)
        
        # Append a new row to accuracy_df with misclass_rate, total_fwd, etc., and tuning_params
        new_row <- cbind(
          total_steps = total_fwd + total_bkwd,
          total_fwd = total_fwd,
          total_bkwd = total_bkwd,
          n = n,
          misclass_rate = best_model$misclass_rate,
          tuning_params
        )
        accuracy_df <- rbind(accuracy_df, new_row)
      }
    }
    
    # Backward steps (after forward steps, perform bkwd_steps backward)
    # Only start if can make at least the min number of backward steps
    if ( n >= bkwd_steps+1) {
      for (i in 1:bkwd_steps) {
        if (n -1 >= 1) {
          total_bkwd = total_bkwd + 1
          n = n - 1
          
          # Perform a backward selection step using fwd_bckwd_step
          bkwd_step_output <- fwd_bckwd_step(train_data, valid_data, tuning_params, model_fitting_procedure, engine, direction = "bkwd")
          
          # Store the best model for this backward step
          best_model <- bkwd_step_output$best_model
          
          # Update selected_predictors to the new predictors from the best model
          tuning_params$formula <- as.character(bkwd_step_output$new_formula)
          
          # Append a new row to accuracy_df with misclass_rate, total_bkwd, etc., and tuning_params
          new_row <- cbind(
            total_steps = total_fwd + total_bkwd,
            total_fwd = total_fwd,
            total_bkwd = total_bkwd,
            n = n,
            misclass_rate = best_model$misclass_rate,
            tuning_params
          )
          accuracy_df <- rbind(accuracy_df, new_row)
        }
      }
    }
    
    # If we can still take more forward steps and backward steps, continue another major iteration
    if (n + fwd_steps > n_predictors) {
      break  # Stop if adding fwd_steps would exceed the total number of predictors
    }
  }
  
  # Reorder columns to ensure misclass_rate and formula are first
  accuracy_df <- accuracy_df[, c("misclass_rate", "formula", setdiff(names(accuracy_df), c("misclass_rate", "formula")))]
  
  # Return the accuracy_df containing misclassification rates and tuning params for each step
  return(list(accuracy_df = accuracy_df))
}


# Example usage:
stepwise_selection_output <- stepwise_selection(Heart_train, Heart_valid, tuning_grid_step[1,], "random_forest", "randomForest", predictors_lim = 5)
stepwise_df <- stepwise_selection_output$accuracy_df  # Table with steps
stepwise_df
```


#### Analyse Results of Best Model Using Less than 4 Predictors

```{r}
plot_grid(stepwise_df, val_measure = "misclass_rate", tp1 = "total_steps", tp2 = "formula")
```

```{r}
stepwise_selection_output2 <- stepwise_selection(Heart_train, Heart_valid, tuning_grid_step[1,], "random_forest", "randomForest",predictors_lim =2)
stepwise_selection_output2$accuracy_df
```




## Conclusion

Did we achieve our objective? Why/Why not?

## Bibliography


## Annex
Only if you get disgusting enough to go really technical. OR, if we tried something first, and it didn't wuite work, we can show that here.