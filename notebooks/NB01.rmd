---
output: html_document
---

# ST 310 Group Project: Predicting Presence of Heart Disease in Patients

**Candidate Numbers:**



## Outline/Contents




## Introduction

### The Dataset
Where from, vars used etc etc

### What We Aim to Achieve
Motivations and objectives

## Set Up

### Packages
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
# General
library(dplyr) # for filtering and sorting to be easier.
library(rstudioapi) # to set the correct working directory


# Fitting Models
#library(tidyverse)
library(ISLR) # to get all the data and models from ISLR to experiment
library(gam) # to fit gams
library(tree) # for tree fitting to guage the functional form of the data
library(MASS) # for QDA and LDA

# Plotting/ Visualisation
library(ggplot2) # for maintainable plots with a lot of interactability
library(yardstick) # for plotting an ROC curve


```

### Directories/Paths
#### Set the current directory to the working directory
```{r}
setPath <- dirname(getSourceEditorContext()$path)

setwd(setPath)
getwd()
```

#### Set the path for the data and predictions
```{r, echo=FALSE}
# Change data_path to where I saved the data
data_path <- paste0(getwd(),"/../data/")
predictions_path <- paste0(getwd(),"/../predictions/")

```

### Load Data
```{r}
Heart <- read.csv(paste0(data_path, "Heart.csv"))
# Rename the outcome column to y
Heart <- Heart %>% rename(y = target)

# Move the last column to the front so that the cross-validation works
Heart <- Heart[, c(ncol(Heart), 1:(ncol(Heart) - 1))]

# Change y to be factor so that  it is recognised as classification
Heart_non_fctr <- Heart
Heart$y <- factor(Heart$y)

head(Heart)
names(Heart)
```
### Cleaning
Any cleaning we have to do
### Exploratory Data Analysis (EDA)
Any eda and domain-specific research




## Model Fitting

### Set Up

#### Set up separate training and validation sets when k-fold cross-validation takes too long
```{r}

make_sets <- function(df, frac = 0.6, classify = F, test_bal = 0.68) {
  train_indices <- sample(1:nrow(df), size = frac * nrow(df), replace = F)
  
  y_train <- df[, 1]
  X_train <- as.matrix(df[, -1])
  
  if (frac>0) {
    valid_indices <- -train_indices
  }
  else {
    valid_indices = rownames(df)
  }
  train_fold <- df[train_indices,]
  valid_fold <- df[valid_indices,]
  
  const_yhat_success <- NA
  outcome_counts_pre <- NA
  outcome_counts_valid <- NA
  if (classify==T) {
    outcome_counts_pre <- valid_fold |> count(y)
    outcome_counts_pre$proportion <- outcome_counts_pre$n / sum(outcome_counts_pre$n)
    rare_class_size <- round(((1-test_bal) * outcome_counts_pre[outcome_counts_pre$y==1,"n"] * (1/test_bal)),0)
    rare_class_indices <- which(valid_fold$y==0)
    common_class_indices <- which(valid_fold$y==1)
    sampled_indices <- sample(rare_class_indices, size = rare_class_size, replace = FALSE)
    valid_indices <- c(sampled_indices,common_class_indices)
    valid_fold <- valid_fold[valid_indices,]
    outcome_counts_valid <- valid_fold |> count(y)
    outcome_counts_valid$proportion <- outcome_counts_valid$n / sum(outcome_counts_valid$n)
    const_yhat_success <- outcome_counts_valid$proportion[[2]]
  }
  #train_fold <- train_fold[order(rownames(train_fold)),]
  #valid_fold <- valid_fold[order(rownames(valid_fold)),]
  
  y_train_fold <- train_fold[, 1]
  X_train_fold <- as.matrix(train_fold[, -1])
  
  y_valid_fold <- valid_fold[, 1]
  X_valid_fold <- as.matrix(valid_fold[, -1])
  
  return(list(y_train = y_train, X_train=X_train, train_fold = train_fold, valid_fold = valid_fold, y_train_fold = y_train_fold, X_train_fold = X_train_fold, y_valid_fold = y_valid_fold, X_valid_fold=X_valid_fold, const_yhat_success= const_yhat_success, outcome_counts_pre = outcome_counts_pre, outcome_counts_valid = outcome_counts_valid, train_indices=train_indices, valid_indices = valid_indices))
}
```


```{r}
# Get matrix versions of the full data
X <- make_sets_output_1$X_train
Y <- make_sets_output_1$y_train

# Make training and validation sets for the whole data
make_sets_output_1 <- make_sets(df=Heart )
Heart_train_fold <- make_sets_output_1$train_fold
Heart_valid_fold <- make_sets_output_1$valid_fold

# Make matrix versions of each of these
Heart_X_train_fold <- make_sets_output_1$X_train_fold
Heart_X_valid_fold <- make_sets_output_1$X_valid_fold

Heart_y_train_fold <- make_sets_output_1$y_train_fold
Heart_y_valid_fold <- make_sets_output_1$y_valid_fold

# Extract the indices used for training and validation for future use
train_indices <- make_sets_output_1$train_indices
valid_indices <- make_sets_output_1$valid_indices

# Create k folds if we want to use the same folds for cross validation for all of the model fitting procedures and all specifications for all model fitting procedures
k <- 5
folds <- sample(rep(1:k, length.out = nrow(Heart)))

train_input <- list(train_df = Heart, X_train = X, y_train = Y, train_indices=train_indices, valid_indices = valid_indices, folds=folds)

```


```{r}
dim(Heart_X_valid_fold)
length(Heart_y_valid_fold)
data.frame(y = Heart_y_valid_fold, Heart_X_valid_fold)
```


```{r, echo=F}
n_preds <- (dim(Heart)[2]-1)
print(paste("It seems that our data has", n_preds, "predictors"))
```
#### Look at the Class Balance in Our Data
```{r}
# We need the training set to be balanced, yet the validation set to have 68% balance.
outcome_counts_fold <- Heart_valid_fold |> count(y)
outcome_counts_fold$proportion <- outcome_counts_fold$n / sum(outcome_counts_fold$n)
const_yhat_success <- outcome_counts_fold$proportion[[2]]
outcome_counts_fold
```

Nice, it seems rather balanced!
#### Create A Dataframe to Store All of Our Model Types and Misclass Estimates
```{r}
gen_models_df <- function(len=10, accuracy_measures = c("min_mse")) {
  df = data.frame(
    model_type = character(len),
    model_name = character(len)
  )
  for (measure in accuracy_measures) {
    df[[measure]] <- numeric(len)
  }
  #tp stands for tuning parameter
  df$tp1_name = character(len)
  df$tp2_name = character(len)
  df$tp3_name = character(len)
  df$tp1_value = numeric(len)
  df$tp2_value = numeric(len)
  df$tp3_value = numeric(len)

  return(df)
}
Heart_models <- gen_models_df(accuracy_measures = c("cv_misclass_rate"))

Heart_models

```
#### Define a set of functions that help to validate or cross-validate a grid of tuning parameters



```{r}
get_predictions <- function(model, X_valid_fold, y_valid_fold, classify, formula, model_f, matrix, thresh) {
  
  # Handle the matrix vs non-matrix input
  if (matrix) {
    valid_fold <- X_valid_fold
  } else {
    valid_fold <- data.frame(y = y_valid_fold, X_valid_fold)
  }
  
  # Classification predictions
  if (classify) {
    if (model_f == "tree") {
      # For tree models (e.g., decision tree), assuming it's a binary classification
      prob_predictions <- predict(model, valid_fold)[, 2]
    }
    else if (model_f == "naiveBayes") {
      # For Naive Bayes, type="raw" returns probabilities
      prob_predictions <- predict(model, newdata = valid_fold, type = "raw")[, 2]  # Checking column 2 for second class
    }
    else if (model_f == "qda" | model_f == "lda") {
      # For QDA and LDA, we extract posterior probabilities for the second class (usually "1" or "2")
      prob_predictions <- predict(model, newdata = valid_fold, type = "response")$posterior[, 2]  # Assuming the second column is for class "1"
    } 
    else {
      # Default case for other models (e.g., logistic regression)
      prob_predictions <- predict(model, valid_fold, type = "response")
    }
    
    # Apply threshold and convert to factor (binary classification)
    predictions <- as.factor(as.numeric(prob_predictions > thresh))
  } 
  else {
    # If regression, just return predicted values
    predictions <- predict(model, valid_fold)
  }
  
  return(predictions)
}



calculate_error <- function(predictions, true_values, classify) {
  if (classify) {
    return(mean(predictions != (as.numeric(true_values) - 1)))
  } else {
    return(mean((predictions - true_values)^2))
  }
}



cross_validate <- function(thresh, model_f, X, Y, k, classify, train_df, folds, model_input, matrix, xname, yname) {
  e_folds <- c()  # Store error metrics for each fold
  fits <- list()   # Store model fits for each fold
  preds <- list()  # Store predictions for each fold
  valids <- list() # Store true y-values for each fold
  
  for (fold in 1:k) {
    # Train/validation split for the fold
    X_train_fold <- X[folds != fold, ]
    y_train_fold <- Y[folds != fold]
    X_valid_fold <- X[folds == fold, ]
    y_valid_fold <- Y[folds == fold]
    
    # Update model_input for this fold
    if (!matrix) {
      model_input$data <- train_df[folds != fold, ]
    } else {
      model_input[[xname]] <- X_train_fold
      model_input[[yname]] <- y_train_fold
    }

    
    # Train model
    model <- do.call(model_f, model_input)
    
    # Get validation predictions
    predictions <- get_predictions(model, X_valid_fold, y_valid_fold, classify, model_input$formula, model_f, matrix, thresh)
    
    # Compute error for this fold
    fold_error <- calculate_error(predictions, y_valid_fold, classify)

    
    # Store results
    fits[[fold]] <- model
    preds[[fold]] <- predictions
    valids[[fold]] <- y_valid_fold
    e_folds <- c(e_folds, fold_error)
  }
  
  # Compute overall CV error
  avg_error <- mean(e_folds)
  
  # Return models, predictions, validation targets, and error
  return(list(fits = fits, preds = preds, valids = valids, error = avg_error))
}




validate <- function(thresh, model_f, X, Y, train_indices, valid_indices, classify, train_df, model_input, matrix, xname, yname) {
  
  # Update model_input for this fold
  if (!matrix) {
    model_input$data <- train_df[train_indices,]
  } else {
      model_input[[xname]] <- X[train_indices, ]
      model_input[[yname]] <- y[train_indices]
  }
  
  # Train model using pre-set model_input
  model <- do.call(model_f, model_input)
  # Get validation predictions
  predictions <- get_predictions(model, X[valid_indices, ], Y[valid_indices], classify, model_input$formula, model_f, matrix, thresh)
  
  # Compute validation error
  fold_error <- calculate_error(predictions, Y[valid_indices], classify)

  
  # Return the model, predictions, validation targets, and error
  return(list(fits = model, preds = predictions, valids = Y[valid_indices], error = fold_error))
}





grid_validate <- function(train_input_list = train_input, method = "cv", model_f = "tree", grid_df, k = 5, frac = 0.6, matrix = FALSE, classify = FALSE, test_bal = 0.68, reshuffle = F) {
  train_df <- train_input_list$train_df
  if (reshuffle) {
    sets_output <- make_sets(df = train_df, frac = frac, classify = classify, test_bal = test_bal)
    X <- sets_output$X_train
    Y <- sets_output$y_train
    train_indices <- sets_output$train_indices
    valid_indices <- sets_output$valid_indices
    folds <- sample(rep(1:k, length.out = nrow(train_df))) # makes sure that the same folds are used, just like we make sure that the same validation set is used.
  }
  else {
    X <- train_input_list$X_train
    Y <- train_input_list$y_train
    train_indices <- train_input_list$train_indices
    valid_indices <- train_input_list$valid_indices
    folds <- train_input_list$folds
  }
  # Initialize common variables
  fits <- list()
  preds <- list()
  valids <- list()
  
  # Iterate over each row in grid_df
  for (i in 1:nrow(grid_df)) {
    # Set up model_input
    model_input <- list()
    xname <- ifelse(model_f == "gbart", "x.train", "x")
    yname <- ifelse(model_f == "gbart", "y.train", "y")
    
    if (!matrix) {
      model_input$formula <- as.formula("y ~ .")
    } 
    
    # Apply grid parameters to model_input
    current_model_f <- model_f  # Default model function
    for (tp in names(grid_df)) {
      if (!all(is.na(grid_df[[tp]]))) {
        if (tp == "mindev") {
          model_input$control <- tree.control(nobs = length(train_df$y), mindev = grid_df[i, tp])
        } else if (tp == "preds_used") {
          model_input$formula <- as.formula(paste("y ~", grid_df[i, tp]))
        } else if (tp == "sub_model") {
          current_model_f <- grid_df[i, tp]  # Override model function if 'submodel' is specified
        } else if (tp != "thresh") {
          model_input[[tp]] <- grid_df[i, tp]
        }
      }
    }
    # Get the threshold param if we have it specified and we are classifying. We don't pass this directly into the model fitting function, hence why it is not in the loop above
    thresh <- if ("thresh" %in% names(grid_df) && !is.na(grid_df[i, "thresh"])) grid_df[i, "thresh"] else 0.5
    
    # Call validation function and store results
    if (method == "cv") {
      result <- cross_validate(thresh, current_model_f, X, Y, k, classify, train_df, folds, model_input, matrix, xname, yname)
      error_col <- paste0("cv_", ifelse(classify, "misclass_rate", "mse"))
    } else if (method == "v") {
      result <- validate(thresh, current_model_f, X, Y, train_indices, valid_indices, classify, train_df, model_input, matrix, xname, yname)
      error_col <- paste0("v_", ifelse(classify, "misclass_rate", "mse"))
    } else {
      stop("Invalid method. Choose 'cv' or 'v'.")
    }
    
    # Store results in grid_df and lists
    grid_df[[error_col]][i] <- result$error
    fits[[i]] <- result$fits
    preds[[i]] <- result$preds
    valids[[i]] <- result$valids
  }
  
  return(list(grid_df = grid_df, fits = fits, preds = preds, valids = valids))
}


```

#### Create a function that can plot how our validation or cross-validation error measures compare based on pairs of tuning parameters:
```{r}
plot_grid <- function(grid, val_measure = "v_mse", tp1 = "n_preds_used", tp2 = NA, logx = FALSE) {
  best_model <- grid[which.min(grid[[val_measure]]), ]
  
  plot <- grid |>
    ggplot(aes(x = .data[[tp1]])) +
    geom_point(aes(y = .data[[val_measure]], color = if (!is.na(tp2)) as.factor(.data[[tp2]]) else NULL), size = 2, alpha = 0.5) +
    geom_point(data = best_model, aes(x = .data[[tp1]], y = .data[[val_measure]]), 
               color = "purple", shape = 16, size = 3) +
    labs(
      title = paste(val_measure, "vs.", tp1),
      x = tp1,
      y = val_measure
    ) +
    expand_limits(y = 0.9 * min(grid[[val_measure]])) +
    theme_minimal()
  
  if (!is.na(tp2)) {
    plot <- plot + scale_color_discrete(name = tp2)
  }
  if (logx) {
    plot <- plot + scale_x_log10(breaks = scales::trans_breaks("log10", function(x) 10^x))
  } 
  print(plot)
}
```






### Baseline Model: Logistic (Few Predictors)
Explain why we used this

#### Validate/Cross-Validate Best Tuning Parameters
Run some code
#### Analyse Results
Look at results from best Model



### Gradient-Descent-Based Model: Logistic (Few Predictors)
Take a decreasing size step, and use a loss function without a constant
Explain why we used this
#### Explain how this works (Stickiest Part of Project)

#### Validate/Cross-Validate Best Tuning Parameters
Run some code
#### Analyse Results
Look at results from best Model


### Relatively Interpretable Model: Logistic (Multiple Predictors and Interactions)

#### Validate/Cross-Validate Best Tuning Parameters
Run some code
#### Analyse Results
Look at results from best Model



### High-Dimensional Model: Regularised Logistic (Few Predictors)

#### Validate/Cross-Validate Best Tuning Parameters
Run some code
#### Analyse Results
Look at results from best Model



### Predictively Accurate Model: Classification Random Forest

#### Validate/Cross-Validate Best Tuning Parameters


**Start with Classification Tree**
```{r}
mindev_values <- c(5,4,3,2,1)*rep(10^seq(-2, -4, by = -1), each=5) # i've done too much matrix stuff in my finance module so this dodgy syntax is now all too natural.

tree_grid <- expand.grid(mindev = mindev_values, thresh = seq(0.4,0.6,0.05))
#grid_results$tree_size <- NA
#tree_grid <- data.frame(dummy = rep(NA,4))
tree_grid
```
```{r} 
tree_df_output <- grid_validate(grid_df = tree_grid,  method = "v", classify=T)
tree_df <- tree_df_output$grid_df
tree_df

```
#### Analyse the best specification's predictions against the validation set

```{r}
get_best_spec <- function(grid_df = tree_df, grid_validation = tree_df_output) {
  e_column <- grep("misclass|mse", colnames(tree_df), value = TRUE)
  best_idx <- which.min(grid_df[[e_column]])
  best_e <- grid_df[best_idx, e_column]
  best_row <- grid_df[best_idx,]
  best_preds <- grid_validation$preds[[best_idx]]
  best_valids <- grid_validation$valids[[best_idx]]
  return(list(preds=best_preds, valids=best_valids, error = best_e, row =best_row))
}


```



```{r}
graph_preds <- function(preds, valids, cm=T, scatter=F, classify=T) {
  predictions_df <- data.frame(y = valids, yhat=preds)
  if (cm) {
    confusion_matrix <-
    predictions_df |> 
    conf_mat(truth = y, estimate = yhat)
    print(confusion_matrix |> autoplot(type = "heatmap"))
  }
  if (scatter == T) {
    if (classify) {
      predictions_df$yhat <- as.numeric(predictions_df$yhat) - 1
      predictions_df$y <- as.numeric(predictions_df$y) - 1
    }
    print(plot(predictions_df$yhat, predictions_df$y))
    abline(0, 1)
  }
  error_col <- paste0(ifelse(classify, "misclass_rate", "mse"))
  print(paste(error_col, ":", calculate_error(preds, valids, classify)))
}
graph_preds(get_best_spec()$preds, get_best_spec()$valids, cm=T,scatter=T)


```



```{r}
# Fit a tree
tree_fit <- tree(y ~ ., Heart_train_fold)
# Fit a glm
heart_2 <- glm(y~ ., Heart_non_fctr, family = "gaussian")
# Store the Probability predictions (the second row corresponds to the postive class probability)
tree_fit_prob_preds <- predict(tree_fit, Heart_valid_fold)[,2]

#GHet the standard bayesian classifier class predictions
preds <- predict(tree_fit, Heart_valid_fold, type = "class")
plot(preds, as.numeric(Heart_valid_fold$y)-1)

```

#### Cross-Validate the Best Pruning Coefficient, K (Penalty for number of terminal nodes) and overall tree size.
```{r}
cv.tree_fit <- cv.tree(tree_fit, FUN = prune.misclass)
par(mfrow = c(1, 2))
plot(cv.tree_fit$size, cv.tree_fit$dev, type = "b")
plot(cv.tree_fit$k, cv.tree_fit$dev, type = "b")
```
#### Analyse the confusion matrices for the tree model.
```{r}

tree_fit_predictions <-data.frame(
  y = Heart_valid_fold$y,
  .fitted = tree_fit_prob_preds
) |> 
  mutate(yhat = factor(as.numeric(.fitted > const_yhat_success))) |>
  mutate(misclass = (y!=yhat))
paste("Class Balance:")
tree_fit_predictions |> pull(.fitted) |> mean() # same as mean(tree_fit_predictions$.fitted)
paste("Misclass Rate:")
sum((tree_fit_predictions$y!=tree_fit_predictions$yhat)) / nrow(tree_fit_predictions)
```
**Look at ROC Curve to guage Trade-Off**
```{r, eval=F}
tree_fit_predictions |>
  roc_curve(truth = y, .fitted,
            event_level = "second") |>
  autoplot()
# the 'truth' is just the training y here, not the testing y, which we do not have, nor a validation set.
```
**Classify at different thresholds.**

```{r, eval=F}
higher_cutoff <- const_yhat_success + .15
confusion_matrix_higher <-
  tree_fit_predictions |>
  mutate(yhat = factor(as.numeric(.fitted > higher_cutoff))) |> # this line predicts y = 1 ONLY if the predicted conditional probability is above the higher cutoff
  conf_mat(truth = y, estimate = yhat)
```
```{r, eval=F}
confusion_matrix_medium <-
  tree_fit_predictions |>
  mutate(yhat = factor(as.numeric(.fitted > const_yhat_success))) |> 
  conf_mat(truth = y, estimate = yhat)
```


```{r, eval=F}
lower_cutoff <- const_yhat_success - 0.15
confusion_matrix_lower <-
  tree_fit_predictions |>
  mutate(yhat = factor(as.numeric(.fitted > lower_cutoff))) |>
  conf_mat(truth = y, estimate = yhat)
```

**1. Balanced Confusion Matrix**
```{r, eval=F}
confusion_matrix_medium |> autoplot(type = "heatmap")
```
**3. Low Cut-Off Confusion Matrix**
```{r, eval=F}
confusion_matrix_lower |> autoplot(type = "heatmap")
```
**3. High Cut-Off Confusion Matrix**
```{r, eval=F}
confusion_matrix_higher |> autoplot(type = "heatmap")
```
**Comparing key sum stats:**

```{r, eval=F}
higher_summary <- summary(confusion_matrix_higher) |>
  mutate(higher = .estimate) |>
  dplyr::select(.metric, higher)
medium_summary <- summary(confusion_matrix_medium) |>
  mutate(medium = .estimate) |>
  dplyr::select(medium)
lower_summary <- summary(confusion_matrix_lower) |>
  mutate(lower = .estimate) |>
  dplyr::select(lower)
cbind(higher_summary, medium_summary, lower_summary) |>
  knitr::kable()


```

As seen from the confusion matrices earlier, the threshold does not really seem to matter.

**Move to GMCs (LDA, QDA and Naive Bayes)**
```{r}
library(e1071) # to fit a naive bayes GMC. Putting it earlier caused some problems, so I just put it here
```

```{r}

gmc_grid <- expand.grid(sub_model = c("lda","qda", "naiveBayes"), thresh = seq(0.4,0.6,0.05))
gmc_grid$sub_model <- as.character(gmc_grid$sub_model)
gmc_grid
```
```{r}
gmc_output <- grid_validate(grid_df = gmc_grid,  method = "v", classify=T)
gmc_df <- gmc_output$grid_df
gmc_df
```

```{r}
plot_grid(gmc_df, val_measure = "v_misclass_rate", tp1 = "thresh", tp2 = "sub_model")
```


#### Analyse Results
Look at results from best Model


## Conclusion

Did we achieve our objective? Why/Why not?

## Bibliography


## Annex
Only if you get disgusting enough to go really technical. OR, if we tried something first, and it didn't wuite work, we can show that here.